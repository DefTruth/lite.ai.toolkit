
<!---
## <p align="center"> ğŸ…ğŸ…Lite.AI.ToolKit: ä¸€ä¸ªå¼€ç®±å³ç”¨çš„C++ AIæ¨¡å‹å·¥å…·ç®±</p>
--->

<div id="lite.ai.toolkit-Introduction"></div>  


![logo-v3](docs/resources/logo-v3.png)

<div align='center'>
  <img src=https://img.shields.io/badge/mac|linux|win-pass-brightgreen.svg >
  <img src=https://img.shields.io/badge/device-GPU/CPU-yellow.svg >
  <img src=https://img.shields.io/badge/license-GPLv3-blue.svg >
  <img src=https://img.shields.io/badge/onnxruntime-1.10.0-turquoise.svg >
  <img src=https://img.shields.io/badge/mnn-1.2.0-hotpink.svg >
  <img src=https://img.shields.io/badge/ncnn-1.0.21-orange.svg >
  <img src=https://img.shields.io/badge/tnn-0.3.0-blue.svg >
</div>

ğŸ› **Lite.Ai.ToolKit**: ä¸€ä¸ªè½»é‡çº§çš„`C++` AIæ¨¡å‹å·¥å…·ç®±ï¼Œç”¨æˆ·å‹å¥½ï¼ˆè¿˜è¡Œå§ï¼‰ï¼Œå¼€ç®±å³ç”¨ã€‚å·²ç»åŒ…æ‹¬ **[80+](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md)** æµè¡Œçš„å¼€æºæ¨¡å‹ã€‚è¿™æ˜¯ä¸€ä¸ªæ ¹æ®ä¸ªäººå…´è¶£æ•´ç†çš„C++å·¥å…·ç®±ï¼Œ, æ¶µç›–[ç›®æ ‡æ£€æµ‹](#lite.ai.toolkit-object-detection)ã€[äººè„¸æ£€æµ‹](#lite.ai.toolkit-face-detection)ã€[äººè„¸è¯†åˆ«](#lite.ai.toolkit-face-recognition)ã€[è¯­ä¹‰åˆ†å‰²](#lite.ai.toolkit-segmentation)ã€[æŠ å›¾](#lite.ai.toolkit-matting)ç­‰é¢†åŸŸã€‚è¯¦è§ [Model Zoo](#lite.ai.toolkit-Model-Zoo) å’Œ [ONNX Hub](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md) ã€[MNN Hub](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.mnn.md) ã€[TNN Hub](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.tnn.md) ã€[NCNN Hub](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.ncnn.md). [è‹¥æ˜¯æœ‰ç”¨ï¼Œâ¤ï¸ä¸å¦¨ç»™ä¸ªâ­ï¸ğŸŒŸæ”¯æŒä¸€ä¸‹å§ï¼Œæ„Ÿè°¢æ”¯æŒ~]

<div align='center'>
  <img src='logs/test_lite_yolov5_1.jpg' height="90px" width="90px">
  <img src='docs/resources/efficientdet_d0.jpg' height="90px" width="90px">
  <img src='docs/resources/street.jpg' height="90px" width="90px">
  <img src='logs/test_lite_ultraface.jpg' height="90px" width="90px">
  <img src='logs/test_lite_face_landmarks_1000.jpg' height="90px" width="90px">
  <img src='logs/test_lite_fsanet.jpg' height="90px" width="90px">
  <img src='logs/test_lite_deeplabv3_resnet101.jpg' height="90px" width="90px">
  <img src='logs/test_lite_fast_style_transfer_mosaic.jpg' height="90px" width="90px"> 
  <br>
  <img src='docs/resources/teslai.gif' height="90px" width="90px">
  <img src='docs/resources/tesla.gif' height="90px" width="90px">
  <img src='docs/resources/dance3i.gif' height="90px" width="90px">
  <img src='docs/resources/dance3.gif' height="90px" width="90px">  
  <img src='docs/resources/yolop1.png' height="90px" width="90px">
  <img src='docs/resources/yolop1.gif' height="90px" width="90px">
  <img src='docs/resources/yolop2.png' height="90px" width="90px">
  <img src='docs/resources/yolop2.gif' height="90px" width="90px">
</div>    

<p align="center"><a href="README.md">English</a> | ä¸­æ–‡æ–‡æ¡£ | <a href=#lite.ai.toolkit-Build-MacOS>MacOS</a> | <a href=#lite.ai.toolkit-Build-Linux>Linux</a> | <a href=#lite.ai.toolkit-Build-Windows>Windows</a> </p>


## æ ¸å¿ƒç‰¹å¾ğŸ‘ğŸ‘‹
<div id="lite.ai.toolkit-Core-Features"></div>

* **ç”¨æˆ·å‹å¥½ï¼Œå¼€ç®±å³ç”¨ã€‚** ä½¿ç”¨ç®€å•ä¸€è‡´çš„è°ƒç”¨è¯­æ³•ï¼Œå¦‚**lite::cv::Type::Class**ï¼Œè¯¦è§[examples](#lite.ai.toolkit-Examples-for-Lite.AI.ToolKit).
* **å°‘é‡ä¾èµ–ï¼Œæ„å»ºå®¹æ˜“ã€‚** ç›®å‰, é»˜è®¤åªä¾èµ– **OpenCV** å’Œ **ONNXRuntime**ï¼Œè¯¦è§[build](#lite.ai.toolkit-Build-Lite.AI.ToolKit)ã€‚
* **ä¼—å¤šçš„ç®—æ³•æ¨¡å—ï¼Œä¸”æŒç»­æ›´æ–°ã€‚** ç›®å‰ï¼ŒåŒ…æ‹¬ 10+ ç®—æ³•æ¨¡å—ã€**[80+](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md)** æµè¡Œçš„å¼€æºæ¨¡å‹ä»¥åŠ **[500+](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md)** æƒé‡æ–‡ä»¶

## å¼•ç”¨ ğŸ‰ğŸ‰

å¦‚æœæ‚¨åœ¨è‡ªå·±çš„é¡¹ç›®ä¸­ä½¿ç”¨äº†**Lite.Ai.ToolKit**ï¼Œå¯è€ƒè™‘æŒ‰ä»¥ä¸‹æ–¹å¼è¿›è¡Œå¼•ç”¨ã€‚
```BibTeX
@misc{lite.ai.toolkit2021,
  title={lite.ai.toolkit: A lite C++ toolkit of awesome AI models.},
  url={https://github.com/DefTruth/lite.ai.toolkit},
  note={Open-source software available at https://github.com/DefTruth/lite.ai.toolkit},
  author={Yan Jun},
  year={2021}
}
```  

## å…³äºè®­ç»ƒ ğŸ¤“ğŸ‘€  
ä¸€ä¸ªç”¨äºäººè„¸å…³é”®ç‚¹æ£€æµ‹çš„è®­ç»ƒå’Œè¯„ä¼°çš„å·¥å…·ç®±å·²ç»å¼€æºï¼Œå¯é€šè¿‡pipä¸€é”®å®‰è£…ï¼Œåœ°å€åœ¨[torchlm](https://github.com/DefTruth/torchlm).

## é¢„ç¼–è¯‘åº“ å’Œ æŠ€æœ¯è§„åˆ’ âœ…
![](docs/resources/lite.ai.toolkit-roadmap-v0.1.png)

ç›®å‰ï¼Œæœ‰ä¸€äº›é¢„ç¼–è¯‘çš„MacOS(x64)å’ŒLinux(x64)ä¸‹çš„lite.ai.toolkitåŠ¨æ€åº“ï¼Œå¯ä»¥ç›´æ¥ä»ä»¥ä¸‹é“¾æ¥è¿›è¡Œä¸‹è½½ã€‚Windows(x64)å’ŒAndroidä¸‹çš„é¢„ç¼–è¯‘åº“ï¼Œä¹Ÿä¼šåœ¨æœ€è¿‘å‘å¸ƒå‡ºæ¥ã€‚æ›´å¤šè¯¦æƒ…è¯·å‚è€ƒ[issues#48](https://github.com/DefTruth/lite.ai.toolkit/issues/48) . æ›´å¤šå¯ä¸‹è½½çš„çš„é¢„ç¼–è¯‘åº“ï¼Œè¯·è·³è½¬åˆ°[releases](https://github.com/DefTruth/lite.ai.toolkit/releases) æŸ¥çœ‹ã€‚

* [x]  [lite0.1.1-osx10.15.x-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.8.1.zip](https://github.com/DefTruth/lite.ai.toolkit/releases/download/v0.1.1/lite0.1.1-osx10.15.x-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.8.1.zip)
* [x]  [lite0.1.1-osx10.15.x-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.9.0.zip](https://github.com/DefTruth/lite.ai.toolkit/releases/download/v0.1.1/lite0.1.1-osx10.15.x-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.9.0.zip)
* [x]  [lite0.1.1-osx10.15.x-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.10.0.zip](https://github.com/DefTruth/lite.ai.toolkit/releases/download/v0.1.1/lite0.1.1-osx10.15.x-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.10.0.zip)
* [x]  [lite0.1.1-ubuntu18.04-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.8.1.zip](https://github.com/DefTruth/lite.ai.toolkit/releases/download/v0.1.1/lite0.1.1-ubuntu18.04-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.8.1.zip)
* [x]  [lite0.1.1-ubuntu18.04-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.9.0.zip](https://github.com/DefTruth/lite.ai.toolkit/releases/download/v0.1.1/lite0.1.1-ubuntu18.04-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.9.0.zip)
* [x]  [lite0.1.1-ubuntu18.04-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.10.0.zip](https://github.com/DefTruth/lite.ai.toolkit/releases/download/v0.1.1/lite0.1.1-ubuntu18.04-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.10.0.zip)

åœ¨Linuxä¸‹ï¼Œä¸ºäº†é“¾æ¥åˆ°é¢„ç¼–è¯‘åº“ï¼Œä½¿ç”¨å‰ï¼Œéœ€è¦å…ˆå°†`lite.ai.toolkit/lib`çš„è·¯å¾„æ·»åŠ åˆ°LD_LIBRARY_PATH.
```shell
export LD_LIBRARY_PATH=YOUR-PATH-TO/lite.ai.toolkit/lib:$LD_LIBRARY_PATH
```
## å¿«é€Ÿé…ç½® ğŸ‘€

å¯ä»¥å‚è€ƒä»¥ä¸‹çš„CMakeLists.txtï¼Œå¿«é€Ÿé…ç½®lite.ai.toolkitã€‚ğŸ‘‡ğŸ‘€

```cmake
set(LITE_AI_DIR ${CMAKE_SOURCE_DIR}/lite.ai.toolkit)
include_directories(${LITE_AI_DIR}/include)
link_directories(${LITE_AI_DIR}/lib})
set(TOOLKIT_LIBS lite.ai.toolkit onnxruntime)
set(OpenCV_LIBS opencv_core opencv_imgcodecs opencv_imgproc opencv_video opencv_videoio)

add_executable(lite_yolov5 examples/test_lite_yolov5.cpp)
target_link_libraries(lite_yolov5 ${TOOLKIT_LIBS} ${OpenCV_LIBS})
```

## ç›®å½• ğŸ“–ğŸ’¡
* [æ ¸å¿ƒç‰¹å¾](#lite.ai.toolkit-Core-Features)
* [å¿«é€Ÿå¼€å§‹](#lite.ai.toolkit-Quick-Start)
* [æŠ€æœ¯è§„åˆ’](#lite.ai.toolkit-RoadMap)
* [é‡è¦æ›´æ–°](#lite.ai.toolkit-Important-Updates)
* [æ¨¡å‹æ”¯æŒçŸ©é˜µ](#lite.ai.toolkit-Supported-Models-Matrix)
* [ç¼–è¯‘æ–‡æ¡£](#lite.ai.toolkit-Build-Lite.AI.ToolKit)
* [æ¨¡å‹ä¸‹è½½](#lite.ai.toolkit-Model-Zoo)
* [åº”ç”¨æ¡ˆä¾‹](#lite.ai.toolkit-Examples-for-Lite.AI.ToolKit)
* [å¼€æºåè®®](#lite.ai.toolkit-License)
* [å¼•ç”¨å‚è€ƒ](#lite.ai.toolkit-References)
* [å¦‚ä½•æ·»åŠ æ‚¨çš„æ¨¡å‹](#lite.ai.toolkit-Contribute)

## 1. å¿«é€Ÿå¼€å§‹ ğŸŒŸğŸŒŸ
<div id="lite.ai.toolkit-Quick-Start"></div>

#### æ¡ˆä¾‹0: ä½¿ç”¨[YOLOv5](https://github.com/ultralytics/yolov5) è¿›è¡Œç›®æ ‡æ£€æµ‹ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/yolov5s.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_yolov5_1.jpg";
  std::string save_img_path = "../../../logs/test_lite_yolov5_1.jpg";

  auto *yolov5 = new lite::cv::detection::YoloV5(onnx_path); 
  std::vector<lite::types::Boxf> detected_boxes;
  cv::Mat img_bgr = cv::imread(test_img_path);
  yolov5->detect(img_bgr, detected_boxes);
  
  lite::utils::draw_boxes_inplace(img_bgr, detected_boxes);
  cv::imwrite(save_img_path, img_bgr);  
  
  delete yolov5;
}
```


## 2. é‡è¦æ›´æ–° ğŸ†•
<div id="lite.ai.toolkit-Important-Updates"></div>

<details> 
<summary> Click here to see details of Important Updates! </summary>

|     Date     |                                        Model                                         |                                                    C++                                                    |                         Paper                         |                                      Code                                       |                                        Awesome                                        |     Type     | 
|:------------:|:------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------:|:-------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------:|:------------:|  
| ã€2022/04/03ã€‘ |                      [MODNet](https://github.com/ZHKKKe/MODNet)                      |    [link](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_modnet.cpp)    |   [AAAI 2022](https://arxiv.org/pdf/2011.11961.pdf)   |                    [code](https://github.com/ZHKKKe/MODNet)                     |        ![](https://img.shields.io/github/stars/ZHKKKe/MODNet.svg?style=social)        |   matting    |
| ã€2022/03/23ã€‘ |                   [PIPNtet](https://github.com/jhb86253817/PIPNet)                   |   [link](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_pipnet98.cpp)   |     [CVPR 2021](https://arxiv.org/abs/2003.03771)     |                  [code](https://github.com/jhb86253817/PIPNet)                  |     ![](https://img.shields.io/github/stars/jhb86253817/PIPNet.svg?style=social)      | face::align  |
| ã€2022/01/19ã€‘ |                [YOLO5Face](https://github.com/deepcam-cn/yolov5-face)                |  [link](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolo5face.cpp)   |    [arXiv 2021](https://arxiv.org/abs/2105.12931)     |                [code](https://github.com/deepcam-cn/yolov5-face)                |   ![](https://img.shields.io/github/stars/deepcam-cn/yolov5-face.svg?style=social)    | face::detect |
| ã€2022/01/07ã€‘ |   [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd/)   |    [link](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_scrfd.cpp)     |     [CVPR 2021](https://arxiv.org/abs/2105.04714)     | [code](https://github.com/deepinsight/insightface/blob/master/detection/scrfd/) |   ![](https://img.shields.io/github/stars/deepinsight/insightface.svg?style=social)   | face::detect |
| ã€2021/12/27ã€‘ |                  [NanoDetPlus](https://github.com/RangiLyu/nanodet)                  | [link](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_nanodet_plus.cpp) |    [blog](https://zhuanlan.zhihu.com/p/449912627)     |                   [code](https://github.com/RangiLyu/nanodet)                   |      ![](https://img.shields.io/github/stars/RangiLyu/nanodet.svg?style=social)       |  detection   |
| ã€2021/12/08ã€‘ |                 [MGMatting](https://github.com/yucornetto/MGMatting)                 |  [link](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_mg_matting.cpp)  |     [CVPR 2021](https://arxiv.org/abs/2012.06722)     |                 [code](https://github.com/yucornetto/MGMatting)                 |    ![](https://img.shields.io/github/stars/yucornetto/MGMatting.svg?style=social)     |   matting    |
| ã€2021/11/11ã€‘ |       [YoloV5_V_6_0](https://github.com/ultralytics/yolov5/releases/tag/v6.0)        | [link](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolov5_v6.0.cpp)  | [doi](https://zenodo.org/record/5563715#.YbXffH1Bzfs) |         [code](https://github.com/ultralytics/yolov5/releases/tag/v6.0)         |     ![](https://img.shields.io/github/stars/ultralytics/yolov5.svg?style=social)      |  detection   |
| ã€2021/10/26ã€‘ | [YoloX_V_0_1_1](https://github.com/Megvii-BaseDetection/YOLOX/releases/tag/0.1.1rc0) | [link](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolox_v0.1.1.cpp) |    [arXiv 2021](https://arxiv.org/abs/2107.08430)     |              [code](https://github.com/Megvii-BaseDetection/YOLOX)              | ![](https://img.shields.io/github/stars/Megvii-BaseDetection/YOLOX.svg?style=social)  |  detection   |
| ã€2021/10/02ã€‘ |                    [NanoDet](https://github.com/RangiLyu/nanodet)                    |   [link](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_nanodet.cpp)    |    [blog](https://zhuanlan.zhihu.com/p/306530300)     |                   [code](https://github.com/RangiLyu/nanodet)                   |      ![](https://img.shields.io/github/stars/RangiLyu/nanodet.svg?style=social)       |  detection   |
| ã€2021/09/20ã€‘ |         [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting)         |     [link](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_rvm.cpp)      |     [WACV 2022](https://arxiv.org/abs/2108.11515)     |             [code](https://github.com/PeterL1n/RobustVideoMatting)              | ![](https://img.shields.io/github/stars/PeterL1n/RobustVideoMatting.svg?style=social) |   matting    |
| ã€2021/09/02ã€‘ |                       [YOLOP](https://github.com/hustvl/YOLOP)                       |    [link](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolop.cpp)     |    [arXiv 2021](https://arxiv.org/abs/2108.11250)     |                     [code](https://github.com/hustvl/YOLOP)                     |        ![](https://img.shields.io/github/stars/hustvl/YOLOP.svg?style=social)         |  detection   |

<!---
![](docs/resources/scrfd-mgmatting-nanodetplus.jpg)
--->

</details>

## 3. æ¨¡å‹æ”¯æŒçŸ©é˜µ
<div id="lite.ai.toolkit-Supported-Models-Matrix"></div>

* / = æš‚ä¸æ”¯æŒ.
* âœ… = å¯ä»¥è¿è¡Œï¼Œä¸”å®˜æ–¹æ”¯æŒ.
* âœ”ï¸ = å¯ä»¥è¿è¡Œï¼Œä½†éå®˜æ–¹æ”¯æŒ.
* â” = è®¡åˆ’ä¸­ï¼Œä½†ä¸ä¼šå¾ˆå¿«å®ç°ï¼Œä¹Ÿè®¸å‡ ä¸ªæœˆå.

|                                                       Class                                                       | Size  |       Type       |                                                          Demo                                                          | ONNXRuntime | MNN | NCNN | TNN | MacOS | Linux | Windows | Android |
|:-----------------------------------------------------------------------------------------------------------------:|:-----:|:----------------:|:----------------------------------------------------------------------------------------------------------------------:|:-----------:|:---:|:----:|:---:|:-----:|:-----:|:-------:|:-------:|
|                                  [YoloV5](https://github.com/ultralytics/yolov5)                                  |  28M  |   *detection*    |          [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolov5.cpp)           |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|         [YoloV3](https://github.com/onnx/models/blob/master/vision/object_detection_segmentation/yolov3)          | 236M  |   *detection*    |          [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolov3.cpp)           |      âœ…      |  /  |  /   |  /  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|     [TinyYoloV3](https://github.com/onnx/models/blob/master/vision/object_detection_segmentation/tiny-yolov3)     |  33M  |   *detection*    |        [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_tiny_yolov3.cpp)        |      âœ…      |  /  |  /   |  /  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                              [YoloV4](https://github.com/argusswift/YOLOv4-pytorch)                               | 176M  |   *detection*    |          [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolov4.cpp)           |      âœ…      |  /  |  /   |  /  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|            [SSD](https://github.com/onnx/models/blob/master/vision/object_detection_segmentation/ssd)             |  76M  |   *detection*    |            [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_ssd.cpp)            |      âœ…      |  /  |  /   |  /  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
| [SSDMobileNetV1](https://github.com/onnx/models/blob/master/vision/object_detection_segmentation/ssd-mobilenetv1) |  27M  |   *detection*    |      [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_ssd_mobilenetv1.cpp)      |      âœ…      |  /  |  /   |  /  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                              [YoloX](https://github.com/Megvii-BaseDetection/YOLOX)                               | 3.5M  |   *detection*    |           [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolox.cpp)           |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                        [TinyYoloV4VOC](https://github.com/bubbliiiing/yolov4-tiny-pytorch)                        |  22M  |   *detection*    |      [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_tiny_yolov4_voc.cpp)      |      âœ…      |  /  |  /   |  /  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                       [TinyYoloV4COCO](https://github.com/bubbliiiing/yolov4-tiny-pytorch)                        |  22M  |   *detection*    |     [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_tiny_yolov4_coco.cpp)      |      âœ…      |  /  |  /   |  /  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                                   [YoloR](https://github.com/WongKinYiu/yolor)                                    |  39M  |   *detection*    |           [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolor.cpp)           |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                            [ScaledYoloV4](https://github.com/WongKinYiu/ScaledYOLOv4)                             | 270M  |   *detection*    |       [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_scaled_yolov4.cpp)       |      âœ…      |  /  |  /   |  /  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                    [EfficientDet](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch)                    |  15M  |   *detection*    |       [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_efficientdet.cpp)        |      âœ…      |  /  |  /   |  /  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                   [EfficientDetD7](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch)                   | 220M  |   *detection*    |      [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_efficientdet_d7.cpp)      |      âœ…      |  /  |  /   |  /  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                   [EfficientDetD8](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch)                   | 322M  |   *detection*    |      [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_efficientdet_d8.cpp)      |      âœ…      |  /  |  /   |  /  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                                     [YOLOP](https://github.com/hustvl/YOLOP)                                      |  30M  |   *detection*    |           [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolop.cpp)           |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                                  [NanoDet](https://github.com/RangiLyu/nanodet)                                   | 1.1M  |   *detection*    |          [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_nanodet.cpp)          |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                                [NanoDetPlus](https://github.com/RangiLyu/nanodet)                                 | 4.5M  |   *detection*    |       [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_nanodet_plus.cpp)        |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                               [NanoDetEffi...](https://github.com/RangiLyu/nanodet)                               |  12M  |   *detection*    | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_nanodet_efficientnet_lite.cpp) |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                          [YoloX_V_0_1_1](https://github.com/Megvii-BaseDetection/YOLOX)                           | 3.5M  |   *detection*    |       [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolox_v0.1.1.cpp)        |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                               [YoloV5_V_6_0](https://github.com/ultralytics/yolov5)                               | 7.5M  |   *detection*    |        [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolov5_v6.0.cpp)        |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|         [GlintArcFace](https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch)          |  92M  |     *faceid*     |       [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_glint_arcface.cpp)       |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|         [GlintCosFace](https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch)          |  92M  |     *faceid*     |       [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_glint_cosface.cpp)       |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|          [GlintPartialFC](https://github.com/deepinsight/insightface/tree/master/recognition/partial_fc)          | 170M  |     *faceid*     |     [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_glint_partial_fc.cpp)      |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                              [FaceNet](https://github.com/timesler/facenet-pytorch)                               |  89M  |     *faceid*     |          [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_facenet.cpp)          |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                         [FocalArcFace](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch)                          | 166M  |     *faceid*     |       [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_focal_arcface.cpp)       |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                       [FocalAsiaArcFace](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch)                        | 166M  |     *faceid*     |    [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_focal_asia_arcface.cpp)     |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                [TencentCurricularFace](https://github.com/Tencent/TFace/tree/master/tasks/distfc)                 | 249M  |     *faceid*     |  [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_tencent_curricular_face.cpp)  |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                    [TencentCifpFace](https://github.com/Tencent/TFace/tree/master/tasks/cifp)                     | 130M  |     *faceid*     |     [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_tencent_cifp_face.cpp)     |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                        [CenterLossFace](https://github.com/louis-she/center-loss.pytorch)                         | 280M  |     *faceid*     |     [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_center_loss_face.cpp)      |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                           [SphereFace](https://github.com/clcarwin/sphereface_pytorch)                            |  80M  |     *faceid*     |        [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_sphere_face.cpp)        |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                              [PoseRobustFace](https://github.com/penincillin/DREAM)                               |  92M  |     *faceid*     |     [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_pose_robust_face.cpp)      |      âœ…      |  /  |  /   |  /  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                            [NaivePoseRobustFace](https://github.com/penincillin/DREAM)                            |  43M  |     *faceid*     |  [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_naive_pose_robust_face.cpp)   |      âœ…      |  /  |  /   |  /  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                        [MobileFaceNet](https://github.com/Xiaoccer/MobileFaceNet_Pytorch)                         | 3.8M  |     *faceid*     |      [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_mobile_facenet.cpp)       |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                        [CavaGhostArcFace](https://github.com/cavalleria/cavaface.pytorch)                         |  15M  |     *faceid*     |    [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_cava_ghost_arcface.cpp)     |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                        [CavaCombinedFace](https://github.com/cavalleria/cavaface.pytorch)                         | 250M  |     *faceid*     |    [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_cava_combined_face.cpp)     |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                    [MobileSEFocalFace](https://github.com/grib0ed0v/face_recognition.pytorch)                     | 4.5M  |     *faceid*     |    [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_mobilese_focal_face.cpp)    |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                       [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting)                        |  14M  |    *matting*     |            [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_rvm.cpp)            |      âœ…      |  âœ…  |  /   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                               [MGMatting](https://github.com/yucornetto/MGMatting)                                | 113M  |    *matting*     |        [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_mg_matting.cpp)         |      âœ…      |  âœ…  |  /   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                                    [MODNet](https://github.com/ZHKKKe/MODNet)                                     |  24M  |    *matting*     |          [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_modnet.cpp)           |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                                   [MODNetDyn](https://github.com/ZHKKKe/MODNet)                                   |  24M  |    *matting*     |        [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_modnet_dyn.cpp)         |      âœ…      |  /  |  /   |  /  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                      [BackgroundMattingV2](https://github.com/PeterL1n/BackgroundMattingV2)                       |  20M  |    *matting*     |    [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_backgroundmattingv2.cpp)    |      âœ…      |  âœ…  |  /   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                     [BackgroundMattingV2Dyn](https://github.com/PeterL1n/BackgroundMattingV2)                     |  20M  |    *matting*     |  [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_backgroundmattingv2_dyn.cpp)  |      âœ…      |  /  |  /   |  /  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                [UltraFace](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB)                 | 1.1M  |  *face::detect*  |         [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_ultraface.cpp)         |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                            [RetinaFace](https://github.com/biubug6/Pytorch_Retinaface)                            | 1.6M  |  *face::detect*  |        [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_retinaface.cpp)         |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                             [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)                             | 3.8M  |  *face::detect*  |         [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_faceboxes.cpp)         |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                             [FaceBoxesV2](https://github.com/jhb86253817/FaceBoxesV2)                             | 3.8M  |  *face::detect*  |        [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_faceboxesv2.cpp)        |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                 [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd/)                  | 2.5M  |  *face::detect*  |           [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_scrfd.cpp)           |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |  
|                              [YOLO5Face](https://github.com/deepcam-cn/yolov5-face)                               | 4.8M  |  *face::detect*  |         [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolo5face.cpp)         |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |  
|                            [PFLD](https://github.com/Hsintao/pfld_106_face_landmarks)                             | 1.0M  |  *face::align*   |           [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_pfld.cpp)            |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                               [PFLD98](https://github.com/polarisZhao/PFLD-pytorch)                               | 4.8M  |  *face::align*   |          [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_pfld98.cpp)           |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                         [MobileNetV268](https://github.com/cunjian/pytorch_face_landmark)                         | 9.4M  |  *face::align*   |      [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_mobilenetv2_68.cpp)       |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                        [MobileNetV2SE68](https://github.com/cunjian/pytorch_face_landmark)                        |  11M  |  *face::align*   |     [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_mobilenetv2_se_68.cpp)     |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                            [PFLD68](https://github.com/cunjian/pytorch_face_landmark)                             | 2.8M  |  *face::align*   |          [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_pfld68.cpp)           |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                         [FaceLandmark1000](https://github.com/Single430/FaceLandmark1000)                         | 2.0M  |  *face::align*   |    [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_face_landmarks_1000.cpp)    |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                                 [PIPNet98](https://github.com/jhb86253817/PIPNet)                                 | 44.0M |  *face::align*   |         [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_pipnet98.cpp)          |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                                 [PIPNet68](https://github.com/jhb86253817/PIPNet)                                 | 44.0M |  *face::align*   |         [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_pipnet68.cpp)          |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                                 [PIPNet29](https://github.com/jhb86253817/PIPNet)                                 | 44.0M |  *face::align*   |         [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_pipnet29.cpp)          |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                                 [PIPNet19](https://github.com/jhb86253817/PIPNet)                                 | 44.0M |  *face::align*   |         [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_pipnet19.cpp)          |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                           [FSANet](https://github.com/omasaht/headpose-fsanet-pytorch)                            | 1.2M  |   *face::pose*   |          [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_fsanet.cpp)           |      âœ…      |  âœ…  |  /   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|            [AgeGoogleNet](https://github.com/onnx/models/tree/master/vision/body_analysis/age_gender)             |  23M  |   *face::attr*   |       [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_age_googlenet.cpp)       |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|           [GenderGoogleNet](https://github.com/onnx/models/tree/master/vision/body_analysis/age_gender)           |  23M  |   *face::attr*   |     [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_gender_googlenet.cpp)      |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|         [EmotionFerPlus](https://github.com/onnx/models/blob/master/vision/body_analysis/emotion_ferplus)         |  33M  |   *face::attr*   |      [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_emotion_ferplus.cpp)      |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|              [VGG16Age](https://github.com/onnx/models/tree/master/vision/body_analysis/age_gender)               | 514M  |   *face::attr*   |         [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_vgg16_age.cpp)         |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|             [VGG16Gender](https://github.com/onnx/models/tree/master/vision/body_analysis/age_gender)             | 512M  |   *face::attr*   |       [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_vgg16_gender.cpp)        |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                               [SSRNet](https://github.com/oukohou/SSR_Net_Pytorch)                                | 190K  |   *face::attr*   |          [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_ssrnet.cpp)           |      âœ…      |  âœ…  |  /   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                  [EfficientEmotion7](https://github.com/HSE-asavchenko/face-emotion-recognition)                  |  15M  |   *face::attr*   |    [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_efficient_emotion7.cpp)     |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                  [EfficientEmotion8](https://github.com/HSE-asavchenko/face-emotion-recognition)                  |  15M  |   *face::attr*   |    [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_efficient_emotion8.cpp)     |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                   [MobileEmotion7](https://github.com/HSE-asavchenko/face-emotion-recognition)                    |  13M  |   *face::attr*   |      [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_mobile_emotion7.cpp)      |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                   [ReXNetEmotion7](https://github.com/HSE-asavchenko/face-emotion-recognition)                    |  30M  |   *face::attr*   |      [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_rexnet_emotion7.cpp)      |      âœ…      |  âœ…  |  /   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|     [EfficientNetLite4](https://github.com/onnx/models/blob/master/vision/classification/efficientnet-lite4)      |  49M  | *classification* |    [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_efficientnet_lite4.cpp)     |      âœ…      |  âœ…  |  /   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|            [ShuffleNetV2](https://github.com/onnx/models/blob/master/vision/classification/shufflenet)            | 8.7M  | *classification* |       [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_shufflenetv2.cpp)        |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                          [DenseNet121](https://pytorch.org/hub/pytorch_vision_densenet/)                          | 30.7M | *classification* |         [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_densenet.cpp)          |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                           [GhostNet](https://pytorch.org/hub/pytorch_vision_ghostnet/)                            |  20M  | *classification* |         [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_ghostnet.cpp)          |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                            [HdrDNet](https://pytorch.org/hub/pytorch_vision_hardnet//)                            |  13M  | *classification* |          [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_hardnet.cpp)          |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                             [IBNNet](https://pytorch.org/hub/pytorch_vision_ibnnet/)                              |  97M  | *classification* |          [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_ibnnet.cpp)           |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                        [MobileNetV2](https://pytorch.org/hub/pytorch_vision_mobilenet_v2/)                        |  13M  | *classification* |        [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_mobilenetv2.cpp)        |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                             [ResNet](https://pytorch.org/hub/pytorch_vision_resnet/)                              |  44M  | *classification* |          [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_resnet.cpp)           |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                            [ResNeXt](https://pytorch.org/hub/pytorch_vision_resnext/)                             |  95M  | *classification* |          [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_resnext.cpp)          |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                 [DeepLabV3ResNet101](https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/)                 | 232M  |  *segmentation*  |    [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_deeplabv3_resnet101.cpp)    |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                       [FCNResNet101](https://pytorch.org/hub/pytorch_vision_fcn_resnet101/)                       | 207M  |  *segmentation*  |       [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_fcn_resnet101.cpp)       |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|      [FastStyleTransfer](https://github.com/onnx/models/blob/master/vision/style_transfer/fast_neural_style)      | 6.4M  |     *style*      |    [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_fast_style_transfer.cpp)    |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                              [Colorizer](https://github.com/richzhang/colorization)                               | 123M  |  *colorization*  |         [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_colorizer.cpp)         |      âœ…      |  âœ…  |  /   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    /    |
|                             [SubPixelCNN](https://github.com/niazwazir/SUB_PIXEL_CNN)                             | 234K  |   *resolution*   |       [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_subpixel_cnn.cpp)        |      âœ…      |  âœ…  |  /   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                             [SubPixelCNN](https://github.com/niazwazir/SUB_PIXEL_CNN)                             | 234K  |   *resolution*   |       [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_subpixel_cnn.cpp)        |      âœ…      |  âœ…  |  /   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                           [InsectDet](https://github.com/quarrying/quarrying-insect-id)                           |  27M  |   *detection*    |         [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_insectdet.cpp)         |      âœ…      |  âœ…  |  /   |  âœ…  |   âœ…   |  âœ”ï¸   |   âœ”ï¸    |    â”    |
|                           [InsectID](https://github.com/quarrying/quarrying-insect-id)                            |  22M  | *classification* |         [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_insectid.cpp)          |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |   âœ…   |   âœ”ï¸    |   âœ”ï¸    |    â”    |
|                            [PlantID](https://github.com/quarrying/quarrying-plant-id)                             |  30M  | *classification* |          [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_plantid.cpp)          |      âœ…      |  âœ…  |  âœ…   |  âœ…  |   âœ…   |   âœ…   |   âœ”ï¸    |   âœ”ï¸    |    â”    |


## 4. ç¼–è¯‘æ–‡æ¡£
<div id="lite.ai.toolkit-Build-MacOS"></div>
<div id="lite.ai.toolkit-Build-Lite.AI.ToolKit"></div>

* MacOS: ä»**Lite.Ai.ToolKit** æºç ç¼–è¯‘**MacOS**ä¸‹çš„åŠ¨æ€åº“ã€‚éœ€è¦æ³¨æ„çš„æ˜¯**Lite.Ai.ToolKit** ä½¿ç”¨**onnxruntime**ä½œä¸ºé»˜è®¤çš„åç«¯ï¼Œå› ä¸ºonnxruntimeæ”¯æŒå¤§éƒ¨åˆ†onnxçš„åŸç”Ÿç®—å­ï¼Œå…·æœ‰æ›´é«˜çš„æ˜“ç”¨æ€§ã€‚å¦‚ä½•ç¼–è¯‘Linuxå’ŒWindowsç‰ˆæœ¬ï¼Ÿç‚¹å‡» â–¶ï¸ æŸ¥çœ‹ã€‚
```shell
    git clone --depth=1 https://github.com/DefTruth/lite.ai.toolkit.git  # æœ€æ–°æºç 
    cd lite.ai.toolkit && sh ./build.sh  # å¯¹äºMacOS, ä½ å¯ä»¥ç›´æ¥åˆ©ç”¨æœ¬é¡¹ç›®åŒ…å«çš„OpenCV, ONNXRuntime, MNN, NCNN and TNNä¾èµ–åº“ï¼Œæ— éœ€é‡æ–°ç¼–è¯‘
```

<div id="lite.ai.toolkit-Build-Linux"></div>
<div id="lite.ai.toolkit-Build-Windows"></div>

<details>
<summary>ğŸ’¡ï¸ Linux å’Œ Windows </summary>  

### Linux å’Œ Windows

âš ï¸ **Lite.Ai.ToolKit** çš„å‘è¡Œç‰ˆæœ¬ç›®å‰ä¸ç›´æ¥æ”¯æŒLinuxå’ŒWindowsï¼Œä½ éœ€è¦ä»ä¸‹è½½**Lite.Ai.ToolKit**çš„æºç è¿›è¡Œæ„å»ºã€‚é¦–å…ˆï¼Œä½ éœ€è¦ä¸‹è½½(å¦‚æœæœ‰å®˜æ–¹ç¼–è¯‘å¥½çš„å‘è¡Œç‰ˆæœ¬çš„è¯)æˆ–ç¼–è¯‘**OpenCV** ã€**ONNXRuntime** å’Œå…¶ä»–ä½ éœ€è¦çš„æ¨ç†å¼•æ“ï¼Œå¦‚MNNã€NCNNã€TNNï¼Œç„¶åæŠŠå®ƒä»¬çš„å¤´æ–‡ä»¶åˆ†åˆ«æ”¾å…¥å„è‡ªå¯¹åº”çš„æ–‡ä»¶å¤¹ï¼Œæˆ–è€…ç›´æ¥ä½¿ç”¨æœ¬é¡¹ç›®æä¾›çš„å¤´æ–‡ä»¶ã€‚æœ¬é¡¹ç›®çš„ä¾èµ–åº“å¤´æ–‡ä»¶æ˜¯ç›´æ¥ä»ç›¸åº”çš„å®˜æ–¹åº“æ‹·è´è€Œæ¥çš„ï¼Œä½†ä¸åŒæ“ä½œç³»ç»Ÿä¸‹çš„åŠ¨æ€åº“éœ€è¦é‡æ–°ç¼–è¯‘æˆ–ä¸‹è½½ï¼ŒMacOSç”¨æˆ·å¯ä»¥ç›´æ¥ä½¿ç”¨æœ¬é¡¹ç›®æä¾›çš„å„ä¸ªä¾èµ–åº“çš„åŠ¨æ€åº“ã€‚
* **lite.ai.toolkit/opencv2**
  ```shell
    cp -r you-path-to-downloaded-or-built-opencv/include/opencv4/opencv2 lite.ai.toolkit/opencv2
  ```
* **lite.ai.toolkit/onnxruntime**
  ```shell
    cp -r you-path-to-downloaded-or-built-onnxruntime/include/onnxruntime lite.ai.toolkit/onnxruntime
  ```
* **lite.ai.toolkit/MNN**
  ```shell
    cp -r you-path-to-downloaded-or-built-MNN/include/MNN lite.ai.toolkit/MNN
  ```
* **lite.ai.toolkit/ncnn**
  ```shell
    cp -r you-path-to-downloaded-or-built-ncnn/include/ncnn lite.ai.toolkit/ncnn
  ```
* **lite.ai.toolkit/tnn**
  ```shell
    cp -r you-path-to-downloaded-or-built-TNN/include/tnn lite.ai.toolkit/tnn
  ```

ç„¶åæŠŠå„ä¸ªä¾èµ–åº“æ‹·è´åˆ°**lite.ai.toolkit/lib/(linux|windows)** æ–‡ä»¶å¤¹ã€‚ è¯·å‚è€ƒä¾èµ–åº“çš„ç¼–è¯‘æ–‡æ¡£[<sup>1</sup>](#lite.ai.toolkit-1)ã€‚
* **lite.ai.toolkit/lib/(linux|windows)**
  ```shell
    cp you-path-to-downloaded-or-built-opencv/lib/*opencv* lite.ai.toolkit/lib/(linux|windows)/
    cp you-path-to-downloaded-or-built-onnxruntime/lib/*onnxruntime* lite.ai.toolkit/lib/(linux|windows)/
    cp you-path-to-downloaded-or-built-MNN/lib/*MNN* lite.ai.toolkit/lib/(linux|windows)/
    cp you-path-to-downloaded-or-built-ncnn/lib/*ncnn* lite.ai.toolkit/lib/(linux|windows)/
    cp you-path-to-downloaded-or-built-TNN/lib/*TNN* lite.ai.toolkit/lib/(linux|windows)/
  ```
æ³¨æ„ï¼Œä½ è¿˜éœ€è¦å®‰è£…ffmpeg(<=4.2.2)ï¼Œå› ä¸ºopencvçš„videoioæ¨¡å—ä¾èµ–ffmpegè¿›è¡Œmp4çš„ç¼–è§£ç ã€‚å‚è€ƒ[issue#203](https://github.com/DefTruth/lite.ai.toolkit/issues/6) . åœ¨MacOSä¸‹ï¼Œffmpeg4.2.2å·²ç»ä½œä¸ºä¸€ä¸ªè‡ªå®šä¹‰ä¾èµ–åº“è¢«æˆ‘æ‰“åŒ…è¿›lite.ai.toolkitï¼Œä¸éœ€è¦å†ä»HomeBrewå®‰è£…ä¸ºç³»ç»Ÿåº“ï¼Œå› æ­¤lite.ai.toolkitæ˜¯å•ä½“çš„ï¼Œä½ å¯ä»¥æŠŠå®ƒç§»æ¤åˆ°appé‡Œé¢ï¼Œä¸ç”¨å¿ƒå¦ä¸€å°è¿è¡Œappçš„æœºå™¨æ²¡æœ‰ffmpegï¼ŒMacOSç‰ˆæœ¬çš„lite.ai.toolkitå·²ç»åŒ…å«ffmpeg. åœ¨Windowsä¸‹ï¼Œopencvå®˜æ–¹å›¢é˜Ÿå·²ç»æä¾›äº†ç”¨äºopencvçš„ffmpegé¢„ç¼–è¯‘åº“ã€‚åœ¨Linuxä¸‹ç¼–è¯‘opencvæ—¶ï¼Œè¯·ç¡®ä¿-DWITH_FFMPEG=ONï¼Œå¹¶æ£€æŸ¥æ˜¯å¦é“¾æ¥åˆ°ffmpeg. 
* å…ˆç¼–è¯‘ffmpegï¼Œæ³¨æ„å¿…é¡»æ˜¯ä½ç‰ˆæœ¬çš„ï¼Œé«˜äº4.4çš„ï¼Œopencvä¼šä¸å…¼å®¹ã€‚
```shell
git clone --depth=1 https://git.ffmpeg.org/ffmpeg.git -b n4.2.2
cd ffmpeg
./configure --enable-shared --disable-x86asm --prefix=/usr/local/opt/ffmpeg --disable-static
make -j8
make install
```
* ç„¶åï¼Œç¼–è¯‘å¸¦ffmpegæ”¯æŒçš„OpenCVï¼ŒæŒ‡å®š-DWITH_FFMPEG=ON
```shell
#!/bin/bash

mkdir build
cd build

cmake .. \
  -D CMAKE_BUILD_TYPE=Release \
  -D CMAKE_INSTALL_PREFIX=your-path-to-custom-dir \
  -D BUILD_TESTS=OFF \
  -D BUILD_PERF_TESTS=OFF \
  -D BUILD_opencv_python3=OFF \
  -D BUILD_opencv_python2=OFF \
  -D BUILD_SHARED_LIBS=ON \
  -D BUILD_opencv_apps=OFF \
  -D WITH_FFMPEG=ON 
  
make -j8
make install
cd ..
```
ç¼–è¯‘å®Œopencvåï¼Œä½ å°±å¯ä»¥æŒ‰ç…§ä¸Šè¿°çš„æ­¥éª¤ï¼Œç»§ç»­ç¼–è¯‘lite.ai.toolkit.

* Windows: ä½ å¯ä»¥å‚è€ƒ[issue#6](https://github.com/DefTruth/lite.ai.toolkit/issues/6) ï¼Œè®¨è®ºäº†å¸¸è§çš„ç¼–è¯‘é—®é¢˜ã€‚
* Linux: å‚è€ƒMacOSä¸‹çš„ç¼–è¯‘ï¼Œæ›¿æ¢Linuxç‰ˆæœ¬çš„ä¾èµ–åº“å³å¯ã€‚Linuxä¸‹çš„å‘è¡Œç‰ˆæœ¬å°†ä¼šåœ¨è¿‘æœŸæ·»åŠ  ~ [issue#2](https://github.com/DefTruth/lite.ai.toolkit/issues/2)
* ä»¤äººå¼€å¿ƒçš„æ¶ˆæ¯!!! : ğŸš€ ä½ å¯ä»¥ç›´æ¥ä¸‹è½½æœ€æ–°çš„**ONNXRuntime**å®˜æ–¹æ„å»ºçš„åŠ¨æ€åº“ï¼ŒåŒ…å«Windows, Linux, MacOS and Armçš„ç‰ˆæœ¬!!! CPUå’ŒGPUçš„ç‰ˆæœ¬å‡å¯è·å¾—ã€‚ä¸éœ€è¦å†ä»æºç è¿›è¡Œç¼–è¯‘äº†ï¼Œniceã€‚å¯ä»¥ä»[v1.8.1](https://github.com/microsoft/onnxruntime/releases) ä¸‹è½½æœ€æ–°çš„åŠ¨æ€åº“. æˆ‘ç›®å‰åœ¨**Lite.Ai.ToolKit**ä¸­ç”¨çš„æ˜¯1.7.0ï¼Œä½ å¯ä»¥ä»[v1.7.0](https://github.com/microsoft/onnxruntime/releases/tag/v1.7.0) ä¸‹è½½, ä½†1.8.1åº”è¯¥ä¹Ÿæ˜¯å¯è¡Œçš„ã€‚å¯¹äº**OpenCV**ï¼Œè¯·å°è¯•ä»æºç æ„å»º(Linux) æˆ–è€… ç›´æ¥ä»[OpenCV 4.5.3](https://github.com/opencv/opencv/releases) ä¸‹è½½å®˜æ–¹ç¼–è¯‘å¥½çš„åŠ¨æ€åº“(Windows). ç„¶åæŠŠå¤´æ–‡ä»¶å’Œä¾èµ–åº“æ”¾å…¥ä¸Šè¿°çš„æ–‡ä»¶å¤¹ä¸­.

* Windows GPU å…¼å®¹æ€§: è¯¦è§[issue#10](https://github.com/DefTruth/lite.ai.toolkit/issues/10).
* Linux GPU å…¼å®¹æ€§: è¯¦è§[issue#97](https://github.com/DefTruth/lite.ai.toolkit/issues/97).

</details>  

<details>
<summary>ğŸ”‘ï¸ å¦‚ä½•é“¾æ¥Lite.Ai.ToolKitåŠ¨æ€åº“?</summary>  

* ä½ å¯å‚è€ƒä»¥ä¸‹çš„CMakeLists.txtè®¾ç½®æ¥é“¾æ¥åŠ¨æ€åº“.

```cmake
cmake_minimum_required(VERSION 3.17)
project(lite.ai.toolkit.demo)

set(CMAKE_CXX_STANDARD 11)

# setting up lite.ai.toolkit
set(LITE_AI_DIR ${CMAKE_SOURCE_DIR}/lite.ai.toolkit)
set(LITE_AI_INCLUDE_DIR ${LITE_AI_DIR}/include)
set(LITE_AI_LIBRARY_DIR ${LITE_AI_DIR}/lib)
include_directories(${LITE_AI_INCLUDE_DIR})
link_directories(${LITE_AI_LIBRARY_DIR})

set(OpenCV_LIBS
        opencv_highgui
        opencv_core
        opencv_imgcodecs
        opencv_imgproc
        opencv_video
        opencv_videoio
        )
# add your executable
set(EXECUTABLE_OUTPUT_PATH ${CMAKE_SOURCE_DIR}/examples/build)

add_executable(lite_rvm examples/test_lite_rvm.cpp)
target_link_libraries(lite_rvm
        lite.ai.toolkit
        onnxruntime
        MNN  # need, if built lite.ai.toolkit with ENABLE_MNN=ON,  default OFF
        ncnn # need, if built lite.ai.toolkit with ENABLE_NCNN=ON, default OFF 
        TNN  # need, if built lite.ai.toolkit with ENABLE_TNN=ON,  default OFF 
        ${OpenCV_LIBS})  # link lite.ai.toolkit & other libs.
```

```shell
cd ./build/lite.ai.toolkit/lib && otool -L liblite.ai.toolkit.0.0.1.dylib 
liblite.ai.toolkit.0.0.1.dylib:
        @rpath/liblite.ai.toolkit.0.0.1.dylib (compatibility version 0.0.1, current version 0.0.1)
        @rpath/libopencv_highgui.4.5.dylib (compatibility version 4.5.0, current version 4.5.2)
        @rpath/libonnxruntime.1.7.0.dylib (compatibility version 0.0.0, current version 1.7.0)
        ...
```


```shell
cd ../ && tree .
â”œâ”€â”€ bin
â”œâ”€â”€ include
â”‚Â Â  â”œâ”€â”€ lite
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ backend.h
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ config.h
â”‚Â Â  â”‚Â Â  â””â”€â”€ lite.h
â”‚Â Â  â””â”€â”€ ort
â””â”€â”€ lib
    â””â”€â”€ liblite.ai.toolkit.0.0.1.dylib
```
* è¿è¡Œå·²ç»ç¼–è¯‘å¥½çš„examples:
```shell
cd ./build/lite.ai.toolkit/bin && ls -lh | grep lite
-rwxr-xr-x  1 root  staff   301K Jun 26 23:10 liblite.ai.toolkit.0.0.1.dylib
...
-rwxr-xr-x  1 root  staff   196K Jun 26 23:10 lite_yolov4
-rwxr-xr-x  1 root  staff   196K Jun 26 23:10 lite_yolov5
...
```

```shell
./lite_yolov5
LITEORT_DEBUG LogId: ../../../hub/onnx/cv/yolov5s.onnx
=============== Input-Dims ==============
...
detected num_anchors: 25200
generate_bboxes num: 66
Default Version Detected Boxes Num: 5
```

ä¸ºäº†é“¾æ¥`lite.ai.toolkit`åŠ¨æ€åº“ï¼Œä½ éœ€è¦ç¡®ä¿`OpenCV` and `onnxruntime`ä¹Ÿè¢«æ­£ç¡®åœ°é“¾æ¥ã€‚ä½ å¯ä»¥åœ¨[CMakeLists.txt](https://github.com/DefTruth/RobustVideoMatting-ncnn-mnn-tnn-onnxruntime/blob/main/CMakeLists.txt) ä¸­æ‰¾åˆ°ä¸€ä¸ªç®€å•ä¸”å®Œæ•´çš„ï¼Œå…³äºå¦‚ä½•æ­£ç¡®åœ°é“¾æ¥Lite.AI.ToolKitåŠ¨æ€åº“çš„åº”ç”¨æ¡ˆä¾‹ã€‚

</details>


## 5. æ¨¡å‹ä¸‹è½½  
<div id="lite.ai.toolkit-2"></div>
<div id="lite.ai.toolkit-Model-Zoo"></div>

**Lite.Ai.ToolKit** ç›®å‰åŒ…æ‹¬ **[80+](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md)** æµè¡Œçš„å¼€æºæ¨¡å‹ä»¥åŠ **[500+](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md)** æ–‡ä»¶ï¼Œå¤§éƒ¨åˆ†æ–‡ä»¶æ˜¯æˆ‘è‡ªå·±è½¬æ¢çš„ã€‚ä½ å¯ä»¥é€šè¿‡**lite::cv::Type::Class** è¯­æ³•è¿›è¡Œè°ƒç”¨ï¼Œå¦‚ **[lite::cv::detection::YoloV5](#lite.ai.toolkit-object-detection)**ã€‚æ›´å¤šçš„ç»†èŠ‚è§[Examples for Lite.Ai.ToolKit](#lite.ai.toolkit-Examples-for-Lite.AI.ToolKit)ã€‚æ³¨æ„ï¼Œç”±äºGoogle Driver(15G)çš„å­˜å‚¨é™åˆ¶ï¼Œæˆ‘æ— æ³•ä¸Šä¼ æ‰€æœ‰çš„æ¨¡å‹æ–‡ä»¶ï¼Œå›½å†…çš„å°ä¼™ä¼´è¯·ä½¿ç”¨ç™¾åº¦äº‘ç›˜ã€‚

| File |                                Baidu Drive                                |                                             Google Drive                                             |                                                          Docker Hub                                                          |                                               Hub (Docs)                                               |  
|:----:|:-------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------:|:------------------------------------------------------------------------------------------------------:|
| ONNX | [Baidu Drive](https://pan.baidu.com/s/1elUGcx7CZkkjEoYhTMwTRQ) code: 8gin | [Google Drive](https://drive.google.com/drive/folders/1p6uBcxGeyS1exc-T61vL8YRhwjYL4iD2?usp=sharing) | [ONNX Docker v0.1.22.01.08 (28G), v0.1.22.02.02 (400M)](https://hub.docker.com/r/qyjdefdocker/lite.ai.toolkit-onnx-hub/tags) | [ONNX Hub](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md) |    
| MNN  | [Baidu Drive](https://pan.baidu.com/s/1KyO-bCYUv6qPq2M8BH_Okg) code: 9v63 |                                                  â”                                                   |  [MNN Docker v0.1.22.01.08 (11G), v0.1.22.02.02 (213M)](https://hub.docker.com/r/qyjdefdocker/lite.ai.toolkit-mnn-hub/tags)  |  [MNN Hub](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.mnn.md)  |  
| NCNN | [Baidu Drive](https://pan.baidu.com/s/1hlnqyNsFbMseGFWscgVhgQ) code: sc7f |                                                  â”                                                   | [NCNN Docker v0.1.22.01.08 (9G), v0.1.22.02.02 (197M)](https://hub.docker.com/r/qyjdefdocker/lite.ai.toolkit-ncnn-hub/tags)  | [NCNN Hub](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.ncnn.md) |
| TNN  | [Baidu Drive](https://pan.baidu.com/s/1lvM2YKyUbEc5HKVtqITpcw) code: 6o6k |                                                  â”                                                   |  [TNN Docker v0.1.22.01.08 (11G), v0.1.22.02.02 (217M)](https://hub.docker.com/r/qyjdefdocker/lite.ai.toolkit-tnn-hub/tags)  |  [TNN Hub](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.tnn.md)  |

```shell
  docker pull qyjdefdocker/lite.ai.toolkit-onnx-hub:v0.1.22.01.08  # (28G)
  docker pull qyjdefdocker/lite.ai.toolkit-mnn-hub:v0.1.22.01.08   # (11G)
  docker pull qyjdefdocker/lite.ai.toolkit-ncnn-hub:v0.1.22.01.08  # (9G)
  docker pull qyjdefdocker/lite.ai.toolkit-tnn-hub:v0.1.22.01.08   # (11G)
  docker pull qyjdefdocker/lite.ai.toolkit-onnx-hub:v0.1.22.02.02  # (400M) + YOLO5Face
  docker pull qyjdefdocker/lite.ai.toolkit-mnn-hub:v0.1.22.02.02   # (213M) + YOLO5Face
  docker pull qyjdefdocker/lite.ai.toolkit-ncnn-hub:v0.1.22.02.02  # (197M) + YOLO5Face
  docker pull qyjdefdocker/lite.ai.toolkit-tnn-hub:v0.1.22.02.02   # (217M) + YOLO5Face
```
<details>
<summary> â‡ï¸ å‘½åç©ºé—´å’ŒLite.Ai.ToolKitç®—æ³•æ¨¡å—çš„å¯¹åº”å…³ç³» </summary>  

### å‘½åç©ºé—´å’ŒLite.Ai.ToolKitç®—æ³•æ¨¡å—çš„å¯¹åº”å…³ç³»

| Namepace                   | Details                                                                            |
|:---------------------------|:-----------------------------------------------------------------------------------|
| *lite::cv::detection*      | Object Detection. one-stage and anchor-free detectors, YoloV5, YoloV4, SSD, etc. âœ… |
| *lite::cv::classification* | Image Classification. DensNet, ShuffleNet, ResNet, IBNNet, GhostNet, etc. âœ…        |
| *lite::cv::faceid*         | Face Recognition. ArcFace, CosFace, CurricularFace, etc. â‡ï¸                        |
| *lite::cv::face*           | Face Analysis. *detect*, *align*, *pose*, *attr*, etc. â‡ï¸                          |
| *lite::cv::face::detect*   | Face Detection. UltraFace, RetinaFace, FaceBoxes, PyramidBox, etc. â‡ï¸              |
| *lite::cv::face::align*    | Face Alignment. PFLD(106), FaceLandmark1000(1000 landmarks), PRNet, etc. â‡ï¸        |
| *lite::cv::face::pose*     | Head Pose Estimation.  FSANet, etc. â‡ï¸                                             |
| *lite::cv::face::attr*     | Face Attributes. Emotion, Age, Gender. EmotionFerPlus, VGG16Age, etc. â‡ï¸           |
| *lite::cv::segmentation*   | Object Segmentation. Such as FCN, DeepLabV3, etc.  â‡ï¸ ï¸                            |
| *lite::cv::style*          | Style Transfer. Contains neural style transfer now, such as FastStyleTransfer.  âš ï¸ |
| *lite::cv::matting*        | Image Matting. Object and Human matting.   â‡ï¸ ï¸                                    |
| *lite::cv::colorization*   | Colorization. Make Gray image become RGB. âš ï¸                                       |
| *lite::cv::resolution*     | Super Resolution.  âš ï¸                                                              |


### Lite.Ai.ToolKitçš„ç±»ä¸æƒé‡æ–‡ä»¶å¯¹åº”å…³ç³»è¯´æ˜

Lite.AI.ToolKitçš„ç±»ä¸æƒé‡æ–‡ä»¶å¯¹åº”å…³ç³»è¯´æ˜ï¼Œå¯ä»¥åœ¨[lite.ai.toolkit.hub.onnx.md](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md) ä¸­æ‰¾åˆ°ã€‚æ¯”å¦‚, *lite::cv::detection::YoloV5* å’Œ *lite::cv::detection::YoloX* çš„æƒé‡æ–‡ä»¶ä¸ºï¼š


|             Class             | Pretrained ONNX Files |                 Rename or Converted From (Repo)                  | Size  |
|:-----------------------------:|:---------------------:|:----------------------------------------------------------------:|:-----:|
| *lite::cv::detection::YoloV5* |     yolov5l.onnx      |    [yolov5](https://github.com/ultralytics/yolov5) (ğŸ”¥ğŸ”¥ğŸ’¥â†‘)     | 188Mb |
| *lite::cv::detection::YoloV5* |     yolov5m.onnx      |    [yolov5](https://github.com/ultralytics/yolov5) (ğŸ”¥ğŸ”¥ğŸ’¥â†‘)     | 85Mb  |
| *lite::cv::detection::YoloV5* |     yolov5s.onnx      |    [yolov5](https://github.com/ultralytics/yolov5) (ğŸ”¥ğŸ”¥ğŸ’¥â†‘)     | 29Mb  |
| *lite::cv::detection::YoloV5* |     yolov5x.onnx      |    [yolov5](https://github.com/ultralytics/yolov5) (ğŸ”¥ğŸ”¥ğŸ’¥â†‘)     | 351Mb |
| *lite::cv::detection::YoloX*  |     yolox_x.onnx      | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) (ğŸ”¥ğŸ”¥!!â†‘) | 378Mb |
| *lite::cv::detection::YoloX*  |     yolox_l.onnx      | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) (ğŸ”¥ğŸ”¥!!â†‘) | 207Mb |
| *lite::cv::detection::YoloX*  |     yolox_m.onnx      | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) (ğŸ”¥ğŸ”¥!!â†‘) | 97Mb  |
| *lite::cv::detection::YoloX*  |     yolox_s.onnx      | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) (ğŸ”¥ğŸ”¥!!â†‘) | 34Mb  |
| *lite::cv::detection::YoloX*  |    yolox_tiny.onnx    | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) (ğŸ”¥ğŸ”¥!!â†‘) | 19Mb  |
| *lite::cv::detection::YoloX*  |    yolox_nano.onnx    | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) (ğŸ”¥ğŸ”¥!!â†‘) | 3.5Mb |

è¿™æ„å‘³ç€ï¼Œä½ å¯ä»¥é€šè¿‡Lite.Ai.ToolKitä¸­çš„åŒä¸€ä¸ªç±»ï¼Œæ ¹æ®ä½ çš„ä½¿ç”¨æƒ…å†µï¼ŒåŠ è½½ä»»æ„ä¸€ä¸ª`yolov5*.onnx`æˆ–`yolox_*.onnx`ï¼Œå¦‚ *YoloV5*, *YoloX*ç­‰.

```c++
auto *yolov5 = new lite::cv::detection::YoloV5("yolov5x.onnx");  // for server
auto *yolov5 = new lite::cv::detection::YoloV5("yolov5l.onnx"); 
auto *yolov5 = new lite::cv::detection::YoloV5("yolov5m.onnx");  
auto *yolov5 = new lite::cv::detection::YoloV5("yolov5s.onnx");  // for mobile device 
auto *yolox = new lite::cv::detection::YoloX("yolox_x.onnx");  
auto *yolox = new lite::cv::detection::YoloX("yolox_l.onnx");  
auto *yolox = new lite::cv::detection::YoloX("yolox_m.onnx");  
auto *yolox = new lite::cv::detection::YoloX("yolox_s.onnx");  
auto *yolox = new lite::cv::detection::YoloX("yolox_tiny.onnx");  
auto *yolox = new lite::cv::detection::YoloX("yolox_nano.onnx");  // 3.5Mb only !
```

</details>  

<details>
<summary> ğŸ”‘ï¸ å¦‚ä½•ä»é€šè¿‡Docker Hubä¸‹è½½Model Zooï¼Ÿ</summary>    

* Firstly, pull the image from docker hub.
  ```shell
  docker pull qyjdefdocker/lite.ai.toolkit-mnn-hub:v0.1.22.01.08 # (11G)
  docker pull qyjdefdocker/lite.ai.toolkit-ncnn-hub:v0.1.22.01.08 # (9G)
  docker pull qyjdefdocker/lite.ai.toolkit-tnn-hub:v0.1.22.01.08 # (11G)
  docker pull qyjdefdocker/lite.ai.toolkit-onnx-hub:v0.1.22.01.08 # (28G)
  ```
* Secondly, run the container with local `share` dir using `docker run -idt xxx`. A minimum example will show you as follows.
  * make a `share` dir in your local device.
  ```shell
  mkdir share # any name is ok.
  ```
  * write `run_mnn_docker_hub.sh` script like:
  ```shell
  #!/bin/bash  
  PORT1=6072
  PORT2=6084
  SERVICE_DIR=/Users/xxx/Desktop/your-path-to/share
  CONRAINER_DIR=/home/hub/share
  CONRAINER_NAME=mnn_docker_hub_d
  
  docker run -idt -p ${PORT2}:${PORT1} -v ${SERVICE_DIR}:${CONRAINER_DIR} --shm-size=16gb --name ${CONRAINER_NAME} qyjdefdocker/lite.ai.toolkit-mnn-hub:v0.1.22.01.08

  ```
* Finally, copy the model weights from `/home/hub/mnn/cv` to your local `share` dir.
  ```shell
  # activate mnn docker.
  sh ./run_mnn_docker_hub.sh
  docker exec -it mnn_docker_hub_d /bin/bash
  # copy the models to the share dir.
  cd /home/hub 
  cp -rf mnn/cv share/
  ```

</details>

### Model Hubs
lite.ai.toolkitæä¾›å¤§é‡çš„é¢„è®­ç»ƒæ¨¡å‹çš„ONNXæ–‡ä»¶. åŒæ—¶, æ›´å¤šçš„æ¨¡å‹æƒé‡æ–‡ä»¶è¯¦è§ [Model Zoo](#lite.ai.toolkit-Model-Zoo) and [ONNX Hub](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md), [MNN Hub](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.mnn.md), [TNN Hub](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.tnn.md), [NCNN Hub](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.ncnn.md).
<details>
<summary> ğŸ”‘ï¸ ONNX Model Hub </summary>
#### Object Detection.

<div id="lite.ai.toolkit.hub.onnx-object-detection"></div>

|                     Class                      |         Pretrained ONNX Files          |                          Rename or Converted From (Repo)                          | Size  |
|:----------------------------------------------:|:--------------------------------------:|:---------------------------------------------------------------------------------:|:-----:|
|         *lite::cv::detection::YoloV5*          |              yolov5l.onnx              |                  [yolov5](https://github.com/ultralytics/yolov5)                  | 188Mb |
|         *lite::cv::detection::YoloV5*          |              yolov5m.onnx              |                  [yolov5](https://github.com/ultralytics/yolov5)                  | 85Mb  |
|         *lite::cv::detection::YoloV5*          |              yolov5s.onnx              |                  [yolov5](https://github.com/ultralytics/yolov5)                  | 29Mb  |
|         *lite::cv::detection::YoloV5*          |              yolov5x.onnx              |                  [yolov5](https://github.com/ultralytics/yolov5)                  | 351Mb |
|          *lite::cv::detection::YoloX*          |              yolox_x.onnx              |              [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)               | 378Mb |
|          *lite::cv::detection::YoloX*          |              yolox_l.onnx              |              [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)               | 207Mb |
|          *lite::cv::detection::YoloX*          |              yolox_m.onnx              |              [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)               | 97Mb  |
|          *lite::cv::detection::YoloX*          |              yolox_s.onnx              |              [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)               | 34Mb  |
|          *lite::cv::detection::YoloX*          |            yolox_tiny.onnx             |              [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)               | 19Mb  |
|          *lite::cv::detection::YoloX*          |            yolox_nano.onnx             |              [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)               | 3.5Mb |
|         *lite::cv::detection::YoloV3*          |             yolov3-10.onnx             |                   [onnx-models](https://github.com/onnx/models)                   | 236Mb |
|       *lite::cv::detection::TinyYoloV3*        |          tiny-yolov3-11.onnx           |                   [onnx-models](https://github.com/onnx/models)                   | 33Mb  |
|         *lite::cv::detection::YoloV4*          |    voc-mobilenetv2-yolov4-640.onnx     |             [YOLOv4...](https://github.com/argusswift/YOLOv4-pytorch)             | 176Mb |
|         *lite::cv::detection::YoloV4*          |    voc-mobilenetv2-yolov4-416.onnx     |             [YOLOv4...](https://github.com/argusswift/YOLOv4-pytorch)             | 176Mb |
|           *lite::cv::detection::SSD*           |              ssd-10.onnx               |                   [onnx-models](https://github.com/onnx/models)                   | 76Mb  |
|          *lite::cv::detection::YoloR*          |        yolor-d6-1280-1280.onnx         |                   [yolor](https://github.com/WongKinYiu/yolor)                    | 667Mb |
|          *lite::cv::detection::YoloR*          |         yolor-d6-640-640.onnx          |                   [yolor](https://github.com/WongKinYiu/yolor)                    | 601Mb |
|          *lite::cv::detection::YoloR*          |         yolor-d6-320-320.onnx          |                   [yolor](https://github.com/WongKinYiu/yolor)                    | 584Mb |
|          *lite::cv::detection::YoloR*          |        yolor-e6-1280-1280.onnx         |                   [yolor](https://github.com/WongKinYiu/yolor)                    | 530Mb |
|          *lite::cv::detection::YoloR*          |         yolor-e6-640-640.onnx          |                   [yolor](https://github.com/WongKinYiu/yolor)                    | 464Mb |
|          *lite::cv::detection::YoloR*          |         yolor-e6-320-320.onnx          |                   [yolor](https://github.com/WongKinYiu/yolor)                    | 448Mb |
|          *lite::cv::detection::YoloR*          |        yolor-p6-1280-1280.onnx         |                   [yolor](https://github.com/WongKinYiu/yolor)                    | 214Mb |
|          *lite::cv::detection::YoloR*          |         yolor-p6-640-640.onnx          |                   [yolor](https://github.com/WongKinYiu/yolor)                    | 160Mb |
|          *lite::cv::detection::YoloR*          |         yolor-p6-320-320.onnx          |                   [yolor](https://github.com/WongKinYiu/yolor)                    | 147Mb |
|          *lite::cv::detection::YoloR*          |        yolor-w6-1280-1280.onnx         |                   [yolor](https://github.com/WongKinYiu/yolor)                    | 382Mb |
|          *lite::cv::detection::YoloR*          |         yolor-w6-640-640.onnx          |                   [yolor](https://github.com/WongKinYiu/yolor)                    | 324Mb |
|          *lite::cv::detection::YoloR*          |         yolor-w6-320-320.onnx          |                   [yolor](https://github.com/WongKinYiu/yolor)                    | 309Mb |
|          *lite::cv::detection::YoloR*          |     yolor-ssss-s2d-1280-1280.onnx      |                   [yolor](https://github.com/WongKinYiu/yolor)                    | 90Mb  |
|          *lite::cv::detection::YoloR*          |      yolor-ssss-s2d-640-640.onnx       |                   [yolor](https://github.com/WongKinYiu/yolor)                    | 49Mb  |
|          *lite::cv::detection::YoloR*          |      yolor-ssss-s2d-320-320.onnx       |                   [yolor](https://github.com/WongKinYiu/yolor)                    | 39Mb  |
|      *lite::cv::detection::TinyYoloV4VOC*      |      yolov4_tiny_weights_voc.onnx      |       [yolov4-tiny...](https://github.com/bubbliiiing/yolov4-tiny-pytorch)        | 23Mb  |
|      *lite::cv::detection::TinyYoloV4VOC*      |    yolov4_tiny_weights_voc_SE.onnx     |       [yolov4-tiny...](https://github.com/bubbliiiing/yolov4-tiny-pytorch)        | 23Mb  |
|      *lite::cv::detection::TinyYoloV4VOC*      |   yolov4_tiny_weights_voc_CBAM.onnx    |       [yolov4-tiny...](https://github.com/bubbliiiing/yolov4-tiny-pytorch)        | 23Mb  |
|      *lite::cv::detection::TinyYoloV4VOC*      |    yolov4_tiny_weights_voc_ECA.onnx    |       [yolov4-tiny...](https://github.com/bubbliiiing/yolov4-tiny-pytorch)        | 23Mb  |
|     *lite::cv::detection::TinyYoloV4COCO*      |     yolov4_tiny_weights_coco.onnx      |       [yolov4-tiny...](https://github.com/bubbliiiing/yolov4-tiny-pytorch)        | 23Mb  |
|      *lite::cv::detection::ScaledYoloV4*       | ScaledYoloV4_yolov4-p5-1280-1280.onnx  |            [ScaledYOLOv4](https://github.com/WongKinYiu/ScaledYOLOv4)             | 270Mb |
|      *lite::cv::detection::ScaledYoloV4*       |  ScaledYoloV4_yolov4-p5-640-640.onnx   |            [ScaledYOLOv4](https://github.com/WongKinYiu/ScaledYOLOv4)             | 270Mb |
|      *lite::cv::detection::ScaledYoloV4*       |  ScaledYoloV4_yolov4-p5-320-320.onnx   |            [ScaledYOLOv4](https://github.com/WongKinYiu/ScaledYOLOv4)             | 270Mb |
|      *lite::cv::detection::ScaledYoloV4*       | ScaledYoloV4_yolov4-p6-1280-1280.onnx  |            [ScaledYOLOv4](https://github.com/WongKinYiu/ScaledYOLOv4)             | 487Mb |
|      *lite::cv::detection::ScaledYoloV4*       |  ScaledYoloV4_yolov4-p6-640-640.onnx   |            [ScaledYOLOv4](https://github.com/WongKinYiu/ScaledYOLOv4)             | 487Mb |
|      *lite::cv::detection::ScaledYoloV4*       |  ScaledYoloV4_yolov4-p6-320-320.onnx   |            [ScaledYOLOv4](https://github.com/WongKinYiu/ScaledYOLOv4)             | 487Mb |
|      *lite::cv::detection::ScaledYoloV4*       | ScaledYoloV4_yolov4-p7-1280-1280.onnx  |            [ScaledYOLOv4](https://github.com/WongKinYiu/ScaledYOLOv4)             | 1.1Gb |
|      *lite::cv::detection::ScaledYoloV4*       |  ScaledYoloV4_yolov4-p7-640-640.onnx   |            [ScaledYOLOv4](https://github.com/WongKinYiu/ScaledYOLOv4)             | 1.1Gb |
|      *lite::cv::detection::ScaledYoloV4*       | ScaledYoloV4_yolov4-p5_-1280-1280.onnx |            [ScaledYOLOv4](https://github.com/WongKinYiu/ScaledYOLOv4)             | 270Mb |
|      *lite::cv::detection::ScaledYoloV4*       |  ScaledYoloV4_yolov4-p5_-640-640.onnx  |            [ScaledYOLOv4](https://github.com/WongKinYiu/ScaledYOLOv4)             | 270Mb |
|      *lite::cv::detection::ScaledYoloV4*       |  ScaledYoloV4_yolov4-p5_-320-320.onnx  |            [ScaledYOLOv4](https://github.com/WongKinYiu/ScaledYOLOv4)             | 270Mb |
|      *lite::cv::detection::ScaledYoloV4*       | ScaledYoloV4_yolov4-p6_-1280-1280.onnx |            [ScaledYOLOv4](https://github.com/WongKinYiu/ScaledYOLOv4)             | 487Mb |
|      *lite::cv::detection::ScaledYoloV4*       |  ScaledYoloV4_yolov4-p6_-640-640.onnx  |            [ScaledYOLOv4](https://github.com/WongKinYiu/ScaledYOLOv4)             | 487Mb |
|      *lite::cv::detection::ScaledYoloV4*       |  ScaledYoloV4_yolov4-p6_-320-320.onnx  |            [ScaledYOLOv4](https://github.com/WongKinYiu/ScaledYOLOv4)             | 487Mb |
|      *lite::cv::detection::EfficientDet*       |          efficientdet-d0.onnx          | [...EfficientDet...](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch) | 15Mb  |
|      *lite::cv::detection::EfficientDet*       |          efficientdet-d1.onnx          | [...EfficientDet...](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch) | 26Mb  |
|      *lite::cv::detection::EfficientDet*       |          efficientdet-d2.onnx          | [...EfficientDet...](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch) | 32Mb  |
|      *lite::cv::detection::EfficientDet*       |          efficientdet-d3.onnx          | [...EfficientDet...](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch) | 49Mb  |
|      *lite::cv::detection::EfficientDet*       |          efficientdet-d4.onnx          | [...EfficientDet...](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch) | 85Mb  |
|      *lite::cv::detection::EfficientDet*       |          efficientdet-d5.onnx          | [...EfficientDet...](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch) | 138Mb |
|      *lite::cv::detection::EfficientDet*       |          efficientdet-d6.onnx          | [...EfficientDet...](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch) | 220Mb |
|     *lite::cv::detection::EfficientDetD7*      |          efficientdet-d7.onnx          | [...EfficientDet...](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch) | 220Mb |
|     *lite::cv::detection::EfficientDetD8*      |          efficientdet-d8.onnx          | [...EfficientDet...](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch) | 322Mb |
|          *lite::cv::detection::YOLOP*          |          yolop-1280-1280.onnx          |                     [YOLOP](https://github.com/hustvl/YOLOP)                      | 30Mb  |
|          *lite::cv::detection::YOLOP*          |           yolop-640-640.onnx           |                     [YOLOP](https://github.com/hustvl/YOLOP)                      | 30Mb  |
|          *lite::cv::detection::YOLOP*          |           yolop-320-320.onnx           |                     [YOLOP](https://github.com/hustvl/YOLOP)                      | 30Mb  |
|         *lite::cv::detection::NanoDet*         |          nanodet_m_0.5x.onnx           |                  [nanodet](https://github.com/RangiLyu/nanodet)                   | 1.1Mb |
|         *lite::cv::detection::NanoDet*         |             nanodet_m.onnx             |                  [nanodet](https://github.com/RangiLyu/nanodet)                   | 3.6Mb |
|         *lite::cv::detection::NanoDet*         |          nanodet_m_1.5x.onnx           |                  [nanodet](https://github.com/RangiLyu/nanodet)                   | 7.9Mb |
|         *lite::cv::detection::NanoDet*         |        nanodet_m_1.5x_416.onnx         |                  [nanodet](https://github.com/RangiLyu/nanodet)                   | 7.9Mb |
|         *lite::cv::detection::NanoDet*         |           nanodet_m_416.onnx           |                  [nanodet](https://github.com/RangiLyu/nanodet)                   | 3.6Mb |
|         *lite::cv::detection::NanoDet*         |             nanodet_g.onnx             |                  [nanodet](https://github.com/RangiLyu/nanodet)                   | 14Mb  |
|         *lite::cv::detection::NanoDet*         |             nanodet_t.onnx             |                  [nanodet](https://github.com/RangiLyu/nanodet)                   | 5.1Mb |
|         *lite::cv::detection::NanoDet*         |       nanodet-RepVGG-A0_416.onnx       |                  [nanodet](https://github.com/RangiLyu/nanodet)                   | 26Mb  |
| *lite::cv::detection::NanoDetEfficientNetLite* |  nanodet-EfficientNet-Lite0_320.onnx   |                  [nanodet](https://github.com/RangiLyu/nanodet)                   | 12Mb  |
| *lite::cv::detection::NanoDetEfficientNetLite* |  nanodet-EfficientNet-Lite1_416.onnx   |                  [nanodet](https://github.com/RangiLyu/nanodet)                   | 15Mb  |
| *lite::cv::detection::NanoDetEfficientNetLite* |  nanodet-EfficientNet-Lite2_512.onnx   |                  [nanodet](https://github.com/RangiLyu/nanodet)                   | 18Mb  |
|      *lite::cv::detection::YoloX_V_0_1_1*      |          yolox_x_v0.1.1.onnx           |              [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)               | 378Mb |
|      *lite::cv::detection::YoloX_V_0_1_1*      |          yolox_l_v0.1.1.onnx           |              [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)               | 207Mb |
|      *lite::cv::detection::YoloX_V_0_1_1*      |          yolox_m_v0.1.1.onnx           |              [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)               | 97Mb  |
|      *lite::cv::detection::YoloX_V_0_1_1*      |          yolox_s_v0.1.1.onnx           |              [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)               | 34Mb  |
|      *lite::cv::detection::YoloX_V_0_1_1*      |         yolox_tiny_v0.1.1.onnx         |              [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)               | 19Mb  |
|      *lite::cv::detection::YoloX_V_0_1_1*      |         yolox_nano_v0.1.1.onnx         |              [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)               | 3.5Mb |
|      *lite::cv::detection::YoloV5_V_6_0*       |       yolov5l.640-640.v.6.0.onnx       |                  [yolov5](https://github.com/ultralytics/yolov5)                  | 178Mb |
|      *lite::cv::detection::YoloV5_V_6_0*       |       yolov5m.640-640.v.6.0.onnx       |                  [yolov5](https://github.com/ultralytics/yolov5)                  | 81Mb  |
|      *lite::cv::detection::YoloV5_V_6_0*       |       yolov5s.640-640.v.6.0.onnx       |                  [yolov5](https://github.com/ultralytics/yolov5)                  | 28Mb  |
|      *lite::cv::detection::YoloV5_V_6_0*       |       yolov5x.640-640.v.6.0.onnx       |                  [yolov5](https://github.com/ultralytics/yolov5)                  | 331Mb |
|      *lite::cv::detection::YoloV5_V_6_0*       |       yolov5n.640-640.v.6.0.onnx       |                  [yolov5](https://github.com/ultralytics/yolov5)                  | 7.5Mb |
|      *lite::cv::detection::YoloV5_V_6_0*       |      yolov5l6.640-640.v.6.0.onnx       |                  [yolov5](https://github.com/ultralytics/yolov5)                  | 294Mb |
|      *lite::cv::detection::YoloV5_V_6_0*       |      yolov5m6.640-640.v.6.0.onnx       |                  [yolov5](https://github.com/ultralytics/yolov5)                  | 128Mb |
|      *lite::cv::detection::YoloV5_V_6_0*       |      yolov5s6.640-640.v.6.0.onnx       |                  [yolov5](https://github.com/ultralytics/yolov5)                  | 50Mb  |
|      *lite::cv::detection::YoloV5_V_6_0*       |      yolov5x6.640-640.v.6.0.onnx       |                  [yolov5](https://github.com/ultralytics/yolov5)                  | 538Mb |
|      *lite::cv::detection::YoloV5_V_6_0*       |      yolov5n6.640-640.v.6.0.onnx       |                  [yolov5](https://github.com/ultralytics/yolov5)                  | 14Mb  |
|      *lite::cv::detection::YoloV5_V_6_0*       |     yolov5l6.1280-1280.v.6.0.onnx      |                  [yolov5](https://github.com/ultralytics/yolov5)                  | 294Mb |
|      *lite::cv::detection::YoloV5_V_6_0*       |     yolov5m6.1280-1280.v.6.0.onnx      |                  [yolov5](https://github.com/ultralytics/yolov5)                  | 128Mb |
|      *lite::cv::detection::YoloV5_V_6_0*       |     yolov5s6.1280-1280.v.6.0.onnx      |                  [yolov5](https://github.com/ultralytics/yolov5)                  | 50Mb  |
|      *lite::cv::detection::YoloV5_V_6_0*       |     yolov5x6.1280-1280.v.6.0.onnx      |                  [yolov5](https://github.com/ultralytics/yolov5)                  | 538Mb |
|      *lite::cv::detection::YoloV5_V_6_0*       |     yolov5n6.1280-1280.v.6.0.onnx      |                  [yolov5](https://github.com/ultralytics/yolov5)                  | 14Mb  |
|       *lite::cv::detection::NanoDetPlus*       |        nanodet-plus-m_320.onnx         |                  [nanodet](https://github.com/RangiLyu/nanodet)                   | 4.5Mb |
|       *lite::cv::detection::NanoDetPlus*       |        nanodet-plus-m_416.onnx         |                  [nanodet](https://github.com/RangiLyu/nanodet)                   | 4.5Mb |
|       *lite::cv::detection::NanoDetPlus*       |      nanodet-plus-m-1.5x_320.onnx      |                  [nanodet](https://github.com/RangiLyu/nanodet)                   | 9.4Mb |
|       *lite::cv::detection::NanoDetPlus*       |      nanodet-plus-m-1.5x_416.onnx      |                  [nanodet](https://github.com/RangiLyu/nanodet)                   | 9.4Mb |
|        *lite::cv::detection::InsectDet*        |     quarrying_insect_detector.onnx     |           [InsectID](https://github.com/quarrying/quarrying-insect-id)            | 22Mb  |

#### Classification.

<div id="lite.ai.toolkit.hub.onnx-classification"></div>


|                    Class                     |      Pretrained ONNX Files       |               Rename or Converted From (Repo)                | Size  |
|:--------------------------------------------:|:--------------------------------:|:------------------------------------------------------------:|:-----:|
| *lite::cv::classification:EfficientNetLite4* |    efficientnet-lite4-11.onnx    |        [onnx-models](https://github.com/onnx/models)         | 49Mb  |
|   *lite::cv::classification::ShuffleNetV2*   |      shufflenet-v2-10.onnx       |        [onnx-models](https://github.com/onnx/models)         | 8.7Mb |
|   *lite::cv::classification::DenseNet121*    |         densenet121.onnx         |       [torchvision](https://github.com/pytorch/vision)       | 30Mb  |
|     *lite::cv::classification::GhostNet*     |          ghostnet.onnx           |       [torchvision](https://github.com/pytorch/vision)       | 20Mb  |
|     *lite::cv::classification::HdrDNet*      |           hardnet.onnx           |       [torchvision](https://github.com/pytorch/vision)       | 13Mb  |
|      *lite::cv::classification::IBNNet*      |          ibnnet18.onnx           |       [torchvision](https://github.com/pytorch/vision)       | 97Mb  |
|   *lite::cv::classification::MobileNetV2*    |         mobilenetv2.onnx         |       [torchvision](https://github.com/pytorch/vision)       | 13Mb  |
|      *lite::cv::classification::ResNet*      |          resnet18.onnx           |       [torchvision](https://github.com/pytorch/vision)       | 44Mb  |
|     *lite::cv::classification::ResNeXt*      |           resnext.onnx           |       [torchvision](https://github.com/pytorch/vision)       | 95Mb  |
|     *lite::cv::classification::InsectID*     | quarrying_insect_identifier.onnx | [InsectID](https://github.com/quarrying/quarrying-insect-id) | 27Mb  |
|      *lite::cv::classification:PlantID*      |   quarrying_plantid_model.onnx   |  [PlantID](https://github.com/quarrying/quarrying-plant-id)  | 30Mb  |


#### Face Detection.

<div id="lite.ai.toolkit.hub.onnx-face-detection"></div>  

|                 Class                 |            Pretrained ONNX Files            |                             Rename or Converted From (Repo)                             |  Size  |
|:-------------------------------------:|:-------------------------------------------:|:---------------------------------------------------------------------------------------:|:------:|
|  *lite::cv::face::detect::UltraFace*  |           ultraface-rfb-640.onnx            | [Ultra-Light...](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) | 1.5Mb  |
|  *lite::cv::face::detect::UltraFace*  |           ultraface-rfb-320.onnx            | [Ultra-Light...](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) | 1.2Mb  |
| *lite::cv::face::detect::RetinaFace*  |      Pytorch_RetinaFace_resnet50.onnx       |             [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface)              | 104Mb  |
| *lite::cv::face::detect::RetinaFace*  |  Pytorch_RetinaFace_resnet50-640-640.onnx   |             [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface)              | 104Mb  |
| *lite::cv::face::detect::RetinaFace*  |  Pytorch_RetinaFace_resnet50-320-320.onnx   |             [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface)              | 104Mb  |
| *lite::cv::face::detect::RetinaFace*  |  Pytorch_RetinaFace_resnet50-720-1080.onnx  |             [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface)              | 104Mb  |
| *lite::cv::face::detect::RetinaFace*  |     Pytorch_RetinaFace_mobile0.25.onnx      |             [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface)              | 1.6Mb  |
| *lite::cv::face::detect::RetinaFace*  | Pytorch_RetinaFace_mobile0.25-640-640.onnx  |             [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface)              | 1.6Mb  |
| *lite::cv::face::detect::RetinaFace*  | Pytorch_RetinaFace_mobile0.25-320-320.onnx  |             [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface)              | 1.6Mb  |
| *lite::cv::face::detect::RetinaFace*  | Pytorch_RetinaFace_mobile0.25-720-1080.onnx |             [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface)              | 1.6Mb  |
|  *lite::cv::face::detect::FaceBoxes*  |               FaceBoxes.onnx                |                [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)                | 3.8Mb  |
|  *lite::cv::face::detect::FaceBoxes*  |           FaceBoxes-640-640.onnx            |                [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)                | 3.8Mb  |
|  *lite::cv::face::detect::FaceBoxes*  |           FaceBoxes-320-320.onnx            |                [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)                | 3.8Mb  |
|  *lite::cv::face::detect::FaceBoxes*  |           FaceBoxes-720-1080.onnx           |                [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)                | 3.8Mb  |
|    *lite::cv::face::detect::SCRFD*    |        scrfd_500m_shape160x160.onnx         |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.5Mb  |
|    *lite::cv::face::detect::SCRFD*    |        scrfd_500m_shape320x320.onnx         |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.5Mb  |
|    *lite::cv::face::detect::SCRFD*    |        scrfd_500m_shape640x640.onnx         |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.5Mb  |
|    *lite::cv::face::detect::SCRFD*    |     scrfd_500m_bnkps_shape160x160.onnx      |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.5Mb  |  
|    *lite::cv::face::detect::SCRFD*    |     scrfd_500m_bnkps_shape320x320.onnx      |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.5Mb  |  
|    *lite::cv::face::detect::SCRFD*    |     scrfd_500m_bnkps_shape640x640.onnx      |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.5Mb  |  
|    *lite::cv::face::detect::SCRFD*    |         scrfd_1g_shape160x160.onnx          |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.7Mb  |
|    *lite::cv::face::detect::SCRFD*    |         scrfd_1g_shape320x320.onnx          |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.7Mb  |
|    *lite::cv::face::detect::SCRFD*    |         scrfd_1g_shape640x640.onnx          |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.7Mb  |
|    *lite::cv::face::detect::SCRFD*    |        scrfd_2.5g_shape160x160.onnx         |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 3.3Mb  |
|    *lite::cv::face::detect::SCRFD*    |        scrfd_2.5g_shape320x320.onnx         |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 3.3Mb  |
|    *lite::cv::face::detect::SCRFD*    |        scrfd_2.5g_shape640x640.onnx         |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 3.3Mb  |
|    *lite::cv::face::detect::SCRFD*    |     scrfd_2.5g_bnkps_shape160x160.onnx      |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 3.3Mb  |  
|    *lite::cv::face::detect::SCRFD*    |     scrfd_2.5g_bnkps_shape320x320.onnx      |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 3.3Mb  |  
|    *lite::cv::face::detect::SCRFD*    |     scrfd_2.5g_bnkps_shape640x640.onnx      |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 3.3Mb  |  
|    *lite::cv::face::detect::SCRFD*    |         scrfd_10g_shape640x640.onnx         |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 16.9Mb |
|    *lite::cv::face::detect::SCRFD*    |        scrfd_10g_shape1280x1280.onnx        |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 16.9Mb |
|    *lite::cv::face::detect::SCRFD*    |      scrfd_10g_bnkps_shape640x640.onnx      |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 16.9Mb |  
|    *lite::cv::face::detect::SCRFD*    |     scrfd_10g_bnkps_shape1280x1280.onnx     |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 16.9Mb |  
|  *lite::cv::face::detect::YOLO5Face*  |      yolov5face-blazeface-640x640.onnx      |                 [YOLO5Face](https://github.com/deepcam-cn/yolov5-face)                  | 3.4Mb  |
|  *lite::cv::face::detect::YOLO5Face*  |          yolov5face-l-640x640.onnx          |                 [YOLO5Face](https://github.com/deepcam-cn/yolov5-face)                  | 181Mb  |
|  *lite::cv::face::detect::YOLO5Face*  |          yolov5face-m-640x640.onnx          |                 [YOLO5Face](https://github.com/deepcam-cn/yolov5-face)                  |  83Mb  |
|  *lite::cv::face::detect::YOLO5Face*  |        yolov5face-n-0.5-320x320.onnx        |                 [YOLO5Face](https://github.com/deepcam-cn/yolov5-face)                  | 2.5Mb  |
|  *lite::cv::face::detect::YOLO5Face*  |        yolov5face-n-0.5-640x640.onnx        |                 [YOLO5Face](https://github.com/deepcam-cn/yolov5-face)                  | 4.6Mb  |
|  *lite::cv::face::detect::YOLO5Face*  |          yolov5face-n-640x640.onnx          |                 [YOLO5Face](https://github.com/deepcam-cn/yolov5-face)                  | 9.5Mb  |
|  *lite::cv::face::detect::YOLO5Face*  |          yolov5face-s-640x640.onnx          |                 [YOLO5Face](https://github.com/deepcam-cn/yolov5-face)                  |  30Mb  |
| *lite::cv::face::detect::FaceBoxesV2* |          faceboxesv2-640x640.onnx           |                [FaceBoxesV2](https://github.com/jhb86253817/FaceBoxesV2)                | 4.0Mb  |

#### Face Alignment.

<div id="lite.ai.toolkit.hub.onnx-face-alignment"></div>  


|                   Class                    |                     Pretrained ONNX Files                     |                  Rename or Converted From (Repo)                   |  Size   |
|:------------------------------------------:|:-------------------------------------------------------------:|:------------------------------------------------------------------:|:-------:|
|       *lite::cv::face::align::PFLD*        |                      pfld-106-lite.onnx                       | [pfld_106_...](https://github.com/Hsintao/pfld_106_face_landmarks) |  1.0Mb  |
|       *lite::cv::face::align::PFLD*        |                       pfld-106-v3.onnx                        | [pfld_106_...](https://github.com/Hsintao/pfld_106_face_landmarks) |  5.5Mb  |
|       *lite::cv::face::align::PFLD*        |                       pfld-106-v2.onnx                        | [pfld_106_...](https://github.com/Hsintao/pfld_106_face_landmarks) |  5.0Mb  |
|      *lite::cv::face::align::PFLD98*       |                    PFLD-pytorch-pfld.onnx                     |       [PFLD...](https://github.com/polarisZhao/PFLD-pytorch)       |  4.8Mb  |
|   *lite::cv::face::align::MobileNetV268*   |       pytorch_face_landmarks_landmark_detection_56.onnx       |  [...landmark](https://github.com/cunjian/pytorch_face_landmark)   |  9.4Mb  |
|  *lite::cv::face::align::MobileNetV2SE68*  | pytorch_face_landmarks_landmark_detection_56_se_external.onnx |  [...landmark](https://github.com/cunjian/pytorch_face_landmark)   |  11Mb   |
|      *lite::cv::face::align::PFLD68*       |               pytorch_face_landmarks_pfld.onnx                |  [...landmark](https://github.com/cunjian/pytorch_face_landmark)   |  2.8Mb  |
| *lite::cv::face::align::FaceLandmarks1000* |                     FaceLandmark1000.onnx                     |   [FaceLandm...](https://github.com/Single430/FaceLandmark1000)    |  2.0Mb  |
|     *lite::cv::face::align::PIPNet98*      |            pipnet_resnet18_10x19x32x256_aflw.onnx             |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 44.0Mb  |
|     *lite::cv::face::align::PIPNet68*      |            pipnet_resnet18_10x29x32x256_cofw.onnx             |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 44.0Mb  |
|     *lite::cv::face::align::PIPNet29*      |            pipnet_resnet18_10x68x32x256_300w.onnx             |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 44.0Mb  |
|     *lite::cv::face::align::PIPNet19*      |            pipnet_resnet18_10x98x32x256_wflw.onnx             |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 44.0Mb  |
|     *lite::cv::face::align::PIPNet98*      |            pipnet_resnet101_10x19x32x256_aflw.onnx            |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 150.0Mb |
|     *lite::cv::face::align::PIPNet68*      |            pipnet_resnet101_10x29x32x256_cofw.onnx            |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 150.0Mb |
|     *lite::cv::face::align::PIPNet29*      |            pipnet_resnet101_10x68x32x256_300w.onnx            |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 150.0Mb |
|     *lite::cv::face::align::PIPNet19*      |            pipnet_resnet101_10x98x32x256_wflw.onnx            |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 150.0Mb |



#### Face Attributes.

<div id="lite.ai.toolkit.hub.onnx-face-attributes"></div>  


|                   Class                   |                    Pretrained ONNX Files                     |                      Rename or Converted From (Repo)                      | Size  |
|:-----------------------------------------:|:------------------------------------------------------------:|:-------------------------------------------------------------------------:|:-----:|
|   *lite::cv::face::attr::AgeGoogleNet*    |                      age_googlenet.onnx                      |               [onnx-models](https://github.com/onnx/models)               | 23Mb  |
|  *lite::cv::face::attr::GenderGoogleNet*  |                    gender_googlenet.onnx                     |               [onnx-models](https://github.com/onnx/models)               | 23Mb  |
|  *lite::cv::face::attr::EmotionFerPlus*   |                    emotion-ferplus-7.onnx                    |               [onnx-models](https://github.com/onnx/models)               | 33Mb  |
|  *lite::cv::face::attr::EmotionFerPlus*   |                    emotion-ferplus-8.onnx                    |               [onnx-models](https://github.com/onnx/models)               | 33Mb  |
|     *lite::cv::face::attr::VGG16Age*      |               vgg_ilsvrc_16_age_imdb_wiki.onnx               |               [onnx-models](https://github.com/onnx/models)               | 514Mb |
|     *lite::cv::face::attr::VGG16Age*      |           vgg_ilsvrc_16_age_chalearn_iccv2015.onnx           |               [onnx-models](https://github.com/onnx/models)               | 514Mb |
|    *lite::cv::face::attr::VGG16Gender*    |             vgg_ilsvrc_16_gender_imdb_wiki.onnx              |               [onnx-models](https://github.com/onnx/models)               | 512Mb |
|      *lite::cv::face::attr::SSRNet*       |                         ssrnet.onnx                          |         [SSR_Net...](https://github.com/oukohou/SSR_Net_Pytorch)          | 190Kb |
| *lite::cv::face::attr::EfficientEmotion7* |           face-emotion-recognition-enet_b0_7.onnx            | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 15Mb  |
| *lite::cv::face::attr::EfficientEmotion8* |      face-emotion-recognition-enet_b0_8_best_afew.onnx       | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 15Mb  |
| *lite::cv::face::attr::EfficientEmotion8* |      face-emotion-recognition-enet_b0_8_best_vgaf.onnx       | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 15Mb  |
|  *lite::cv::face::attr::MobileEmotion7*   |          face-emotion-recognition-mobilenet_7.onnx           | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 13Mb  |
|  *lite::cv::face::attr::ReXNetEmotion7*   | face-emotion-recognition-affectnet_7_vggface2_rexnet150.onnx | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 30Mb  |


#### Face Recognition.

<div id="lite.ai.toolkit.hub.onnx-face-recognition"></div>  


|                   Class                   |                  Pretrained ONNX Files                  |                    Rename or Converted From (Repo)                     | Size  |
|:-----------------------------------------:|:-------------------------------------------------------:|:----------------------------------------------------------------------:|:-----:|
|     *lite::cv::faceid::GlintArcFace*      |                ms1mv3_arcface_r100.onnx                 |       [insightface](https://github.com/deepinsight/insightface)        | 248Mb |
|     *lite::cv::faceid::GlintArcFace*      |                 ms1mv3_arcface_r50.onnx                 |       [insightface](https://github.com/deepinsight/insightface)        | 166Mb |
|     *lite::cv::faceid::GlintArcFace*      |                 ms1mv3_arcface_r34.onnx                 |       [insightface](https://github.com/deepinsight/insightface)        | 130Mb |
|     *lite::cv::faceid::GlintArcFace*      |                 ms1mv3_arcface_r18.onnx                 |       [insightface](https://github.com/deepinsight/insightface)        | 91Mb  |
|     *lite::cv::faceid::GlintCosFace*      |               glint360k_cosface_r100.onnx               |       [insightface](https://github.com/deepinsight/insightface)        | 248Mb |
|     *lite::cv::faceid::GlintCosFace*      |               glint360k_cosface_r50.onnx                |       [insightface](https://github.com/deepinsight/insightface)        | 166Mb |
|     *lite::cv::faceid::GlintCosFace*      |               glint360k_cosface_r34.onnx                |       [insightface](https://github.com/deepinsight/insightface)        | 130Mb |
|     *lite::cv::faceid::GlintCosFace*      |               glint360k_cosface_r18.onnx                |       [insightface](https://github.com/deepinsight/insightface)        | 91Mb  |
|    *lite::cv::faceid::GlintPartialFC*     |             partial_fc_glint360k_r100.onnx              |       [insightface](https://github.com/deepinsight/insightface)        | 248Mb |
|    *lite::cv::faceid::GlintPartialFC*     |              partial_fc_glint360k_r50.onnx              |       [insightface](https://github.com/deepinsight/insightface)        | 91Mb  |
|        *lite::cv::faceid::FaceNet*        |              facenet_vggface2_resnet.onnx               |       [facenet...](https://github.com/timesler/facenet-pytorch)        | 89Mb  |
|        *lite::cv::faceid::FaceNet*        |            facenet_casia-webface_resnet.onnx            |       [facenet...](https://github.com/timesler/facenet-pytorch)        | 89Mb  |
|     *lite::cv::faceid::FocalArcFace*      |              focal-arcface-ms1m-ir152.onnx              |   [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch)   | 269Mb |
|     *lite::cv::faceid::FocalArcFace*      |          focal-arcface-ms1m-ir50-epoch120.onnx          |   [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch)   | 166Mb |
|     *lite::cv::faceid::FocalArcFace*      |          focal-arcface-ms1m-ir50-epoch63.onnx           |   [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch)   | 166Mb |
|   *lite::cv::faceid::FocalAsiaArcFace*    |             focal-arcface-bh-ir50-asia.onnx             |   [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch)   | 166Mb |
| *lite::cv::faceid::TencentCurricularFace* |          Tencent_CurricularFace_Backbone.onnx           |               [TFace](https://github.com/Tencent/TFace)                | 249Mb |
|    *lite::cv::faceid::TencentCifpFace*    |        Tencent_Cifp_BUPT_Balancedface_IR_34.onnx        |               [TFace](https://github.com/Tencent/TFace)                | 130Mb |
|    *lite::cv::faceid::CenterLossFace*     |              CenterLossFace_epoch_100.onnx              |   [center-loss...](https://github.com/louis-she/center-loss.pytorch)   | 280Mb |
|      *lite::cv::faceid::SphereFace*       |                 sphere20a_20171020.onnx                 |      [sphere...](https://github.com/clcarwin/sphereface_pytorch)       | 86Mb  |
|    *lite::cv::faceid::PoseRobustFace*     |              dream_cfp_res50_end2end.onnx               |             [DREAM](https://github.com/penincillin/DREAM)              | 92Mb  |
|    *lite::cv::faceid::PoseRobustFace*     |              dream_ijba_res18_end2end.onnx              |             [DREAM](https://github.com/penincillin/DREAM)              | 43Mb  |
|  *lite::cv::faceid:NaivePoseRobustFace*   |               dream_cfp_res50_naive.onnx                |             [DREAM](https://github.com/penincillin/DREAM)              | 91Mb  |
|  *lite::cv::faceid:NaivePoseRobustFace*   |               dream_ijba_res18_naive.onnx               |             [DREAM](https://github.com/penincillin/DREAM)              | 43Mb  |
|     *lite::cv::faceid:MobileFaceNet*      |             MobileFaceNet_Pytorch_068.onnx              |   [MobileFace...](https://github.com/Xiaoccer/MobileFaceNet_Pytorch)   | 3.8Mb |
|    *lite::cv::faceid:CavaGhostArcFace*    |      cavaface_GhostNet_x1.3_Arcface_Epoch_24.onnx       |     [cavaface...](https://github.com/cavalleria/cavaface.pytorch)      | 15Mb  |
|    *lite::cv::faceid:CavaCombinedFace*    |        cavaface_IR_SE_100_Combined_Epoch_24.onnx        |     [cavaface...](https://github.com/cavalleria/cavaface.pytorch)      | 250Mb |
|   *lite::cv::faceid:MobileSEFocalFace*    | face_recognition.pytorch_Mobilenet_se_focal_121000.onnx | [face_recog...](https://github.com/grib0ed0v/face_recognition.pytorch) | 4.5Mb |


#### Head Pose Estimation.

<div id="lite.ai.toolkit.hub.onnx-head-pose-estimation"></div>  


|             Class              | Pretrained ONNX Files |                  Rename or Converted From (Repo)                   | Size  |
|:------------------------------:|:---------------------:|:------------------------------------------------------------------:|:-----:|
| *lite::cv::face::pose::FSANet* |    fsanet-var.onnx    | [...fsanet...](https://github.com/omasaht/headpose-fsanet-pytorch) | 1.2Mb |
| *lite::cv::face::pose::FSANet* |    fsanet-1x1.onnx    | [...fsanet...](https://github.com/omasaht/headpose-fsanet-pytorch) | 1.2Mb |


#### Segmentation.

<div id="lite.ai.toolkit.hub.onnx-segmentation"></div>  


|                    Class                     |     Pretrained ONNX Files     |         Rename or Converted From (Repo)          | Size  |
|:--------------------------------------------:|:-----------------------------:|:------------------------------------------------:|:-----:|
| *lite::cv::segmentation::DeepLabV3ResNet101* | deeplabv3_resnet101_coco.onnx | [torchvision](https://github.com/pytorch/vision) | 232Mb |
|    *lite::cv::segmentation::FCNResNet101*    |      fcn_resnet101.onnx       | [torchvision](https://github.com/pytorch/vision) | 207Mb |


#### Style Transfer.

<div id="lite.ai.toolkit.hub.onnx-style-transfer"></div>  

|                Class                 |   Pretrained ONNX Files    |        Rename or Converted From (Repo)        | Size  |
|:------------------------------------:|:--------------------------:|:---------------------------------------------:|:-----:|
| *lite::cv::style::FastStyleTransfer* |    style-mosaic-8.onnx     | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::cv::style::FastStyleTransfer* |     style-candy-9.onnx     | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::cv::style::FastStyleTransfer* |     style-udnie-8.onnx     | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::cv::style::FastStyleTransfer* |     style-udnie-9.onnx     | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::cv::style::FastStyleTransfer* |  style-pointilism-8.onnx   | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::cv::style::FastStyleTransfer* |  style-pointilism-9.onnx   | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::cv::style::FastStyleTransfer* | style-rain-princess-9.onnx | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::cv::style::FastStyleTransfer* | style-rain-princess-8.onnx | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::cv::style::FastStyleTransfer* |     style-candy-8.onnx     | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::cv::style::FastStyleTransfer* |    style-mosaic-9.onnx     | [onnx-models](https://github.com/onnx/models) | 6.4Mb |


#### Colorization.

<div id="lite.ai.toolkit.hub.onnx-colorization"></div>

|                Class                |   Pretrained ONNX Files   |              Rename or Converted From (Repo)              | Size  |
|:-----------------------------------:|:-------------------------:|:---------------------------------------------------------:|:-----:|
| *lite::cv::colorization::Colorizer* |   eccv16-colorizer.onnx   | [colorization](https://github.com/richzhang/colorization) | 123Mb |
| *lite::cv::colorization::Colorizer* | siggraph17-colorizer.onnx | [colorization](https://github.com/richzhang/colorization) | 129Mb |


#### Super Resolution.

<div id="lite.ai.toolkit.hub.onnx-super-resolution"></div>

|                Class                | Pretrained ONNX Files |              Rename or Converted From (Repo)              | Size  |
|:-----------------------------------:|:---------------------:|:---------------------------------------------------------:|:-----:|
| *lite::cv::resolution::SubPixelCNN* |   subpixel-cnn.onnx   | [...PIXEL...](https://github.com/niazwazir/SUB_PIXEL_CNN) | 234Kb |


#### Matting.

<div id="lite.ai.toolkit.hub.onnx-matting"></div>

|                    Class                    |                Pretrained ONNX Files                |                    Rename or Converted From (Repo)                     | Size  |
|:-------------------------------------------:|:---------------------------------------------------:|:----------------------------------------------------------------------:|:-----:|
|   *lite::cv::matting::RobustVideoMatting*   |              rvm_mobilenetv3_fp32.onnx              |  [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting)  | 14Mb  |
|   *lite::cv::matting::RobustVideoMatting*   |              rvm_mobilenetv3_fp16.onnx              |  [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting)  | 7.2Mb |
|   *lite::cv::matting::RobustVideoMatting*   |               rvm_resnet50_fp32.onnx                |  [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting)  | 50Mb  |
|   *lite::cv::matting::RobustVideoMatting*   |               rvm_resnet50_fp16.onnx                |  [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting)  | 100Mb |
|       *lite::cv::matting::MGMatting*        |               MGMatting-DIM-100k.onnx               |          [MGMatting](https://github.com/yucornetto/MGMatting)          | 113Mb |
|       *lite::cv::matting::MGMatting*        |               MGMatting-RWP-100k.onnx               |          [MGMatting](https://github.com/yucornetto/MGMatting)          | 113Mb |
|         *lite::cv::matting::MODNet*         | modnet_photographic_portrait_matting-1024x1024.onnx |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|         *lite::cv::matting::MODNet*         | modnet_photographic_portrait_matting-1024x512.onnx  |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|         *lite::cv::matting::MODNet*         |  modnet_photographic_portrait_matting-256x256.onnx  |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|         *lite::cv::matting::MODNet*         |  modnet_photographic_portrait_matting-256x512.onnx  |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|         *lite::cv::matting::MODNet*         | modnet_photographic_portrait_matting-512x1024.onnx  |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|         *lite::cv::matting::MODNet*         |  modnet_photographic_portrait_matting-512x256.onnx  |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|         *lite::cv::matting::MODNet*         |  modnet_photographic_portrait_matting-512x512.onnx  |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|         *lite::cv::matting::MODNet*         |    modnet_webcam_portrait_matting-1024x1024.onnx    |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|         *lite::cv::matting::MODNet*         |    modnet_webcam_portrait_matting-1024x512.onnx     |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|         *lite::cv::matting::MODNet*         |     modnet_webcam_portrait_matting-256x256.onnx     |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|         *lite::cv::matting::MODNet*         |     modnet_webcam_portrait_matting-256x512.onnx     |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|         *lite::cv::matting::MODNet*         |    modnet_webcam_portrait_matting-512x1024.onnx     |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|         *lite::cv::matting::MODNet*         |     modnet_webcam_portrait_matting-512x256.onnx     |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|         *lite::cv::matting::MODNet*         |     modnet_webcam_portrait_matting-512x512.onnx     |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|       *lite::cv::matting::MODNetDyn*        |      modnet_photographic_portrait_matting.onnx      |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|       *lite::cv::matting::MODNetDyn*        |         modnet_webcam_portrait_matting.onnx         |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|  *lite::cv::matting::BackgroundMattingV2*   |         BGMv2_mobilenetv2-256x256-full.onnx         | [BackgroundMattingV2](https://github.com/PeterL1n/BackgroundMattingV2) | 20Mb  |
|  *lite::cv::matting::BackgroundMattingV2*   |         BGMv2_mobilenetv2-512x512-full.onnx         | [BackgroundMattingV2](https://github.com/PeterL1n/BackgroundMattingV2) | 20Mb  |
|  *lite::cv::matting::BackgroundMattingV2*   |        BGMv2_mobilenetv2-1080x1920-full.onnx        | [BackgroundMattingV2](https://github.com/PeterL1n/BackgroundMattingV2) | 20Mb  |
|  *lite::cv::matting::BackgroundMattingV2*   |        BGMv2_mobilenetv2-2160x3840-full.onnx        | [BackgroundMattingV2](https://github.com/PeterL1n/BackgroundMattingV2) | 20Mb  |
|  *lite::cv::matting::BackgroundMattingV2*   |         BGMv2_resnet50-1080x1920-full.onnx          | [BackgroundMattingV2](https://github.com/PeterL1n/BackgroundMattingV2) | 20Mb  |
|  *lite::cv::matting::BackgroundMattingV2*   |         BGMv2_resnet50-2160x3840-full.onnx          | [BackgroundMattingV2](https://github.com/PeterL1n/BackgroundMattingV2) | 20Mb  |
|  *lite::cv::matting::BackgroundMattingV2*   |         BGMv2_resnet101-2160x3840-full.onnx         | [BackgroundMattingV2](https://github.com/PeterL1n/BackgroundMattingV2) | 154Mb |
| *lite::cv::matting::BackgroundMattingV2Dyn* |          BGMv2_mobilenetv2_4k_dynamic.onnx          | [BackgroundMattingV2](https://github.com/PeterL1n/BackgroundMattingV2) | 157Mb |
| *lite::cv::matting::BackgroundMattingV2Dyn* |          BGMv2_mobilenetv2_hd_dynamic.onnx          | [BackgroundMattingV2](https://github.com/PeterL1n/BackgroundMattingV2) | 230Mb |

</details>

## 6. åº”ç”¨æ¡ˆä¾‹

<div id="lite.ai.toolkit-Examples-for-Lite.AI.ToolKit"></div>

æ›´å¤šçš„åº”ç”¨æ¡ˆä¾‹è¯¦è§[examples](https://github.com/DefTruth/lite.ai.toolkit/tree/main/examples/lite/cv) ã€‚ç‚¹å‡» â–¶ï¸ å¯ä»¥çœ‹åˆ°è¯¥ä¸»é¢˜ä¸‹æ›´å¤šçš„æ¡ˆä¾‹ã€‚

<div id="lite.ai.toolkit-object-detection"></div>

#### æ¡ˆä¾‹0: ä½¿ç”¨[YOLOv5](https://github.com/ultralytics/yolov5) è¿›è¡Œç›®æ ‡æ£€æµ‹ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/yolov5s.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_yolov5_1.jpg";
  std::string save_img_path = "../../../logs/test_lite_yolov5_1.jpg";

  auto *yolov5 = new lite::cv::detection::YoloV5(onnx_path); 
  std::vector<lite::types::Boxf> detected_boxes;
  cv::Mat img_bgr = cv::imread(test_img_path);
  yolov5->detect(img_bgr, detected_boxes);
  
  lite::utils::draw_boxes_inplace(img_bgr, detected_boxes);
  cv::imwrite(save_img_path, img_bgr);  
  
  delete yolov5;
}
```

è¾“å‡ºçš„ç»“æœæ˜¯:
<div align='center'>
  <img src='logs/test_lite_yolov5_1.jpg' height="256px">
  <img src='logs/test_lite_yolov5_2.jpg' height="256px">
</div>

æˆ–è€…ä½ å¯ä»¥ä½¿ç”¨æœ€æ–°çš„ ğŸ”¥ğŸ”¥ ! YOLO ç³»åˆ—æ£€æµ‹å™¨[YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) æˆ– [YoloR](https://github.com/WongKinYiu/yolor) ï¼Œå®ƒä»¬ä¼šè·å¾—æ¥è¿‘çš„ç»“æœã€‚

æ›´å¤šå¯ç”¨çš„é€šç”¨ç›®æ ‡æ£€æµ‹å™¨ï¼ˆ80ç±»ã€COCOï¼‰:  
```c++
auto *detector = new lite::cv::detection::YoloX(onnx_path);  // Newest YOLO detector !!! 2021-07
auto *detector = new lite::cv::detection::YoloV4(onnx_path); 
auto *detector = new lite::cv::detection::YoloV3(onnx_path); 
auto *detector = new lite::cv::detection::TinyYoloV3(onnx_path); 
auto *detector = new lite::cv::detection::SSD(onnx_path); 
auto *detector = new lite::cv::detection::YoloV5(onnx_path); 
auto *detector = new lite::cv::detection::YoloR(onnx_path);  // Newest YOLO detector !!! 2021-05
auto *detector = new lite::cv::detection::TinyYoloV4VOC(onnx_path); 
auto *detector = new lite::cv::detection::TinyYoloV4COCO(onnx_path); 
auto *detector = new lite::cv::detection::ScaledYoloV4(onnx_path); 
auto *detector = new lite::cv::detection::EfficientDet(onnx_path); 
auto *detector = new lite::cv::detection::EfficientDetD7(onnx_path); 
auto *detector = new lite::cv::detection::EfficientDetD8(onnx_path); 
auto *detector = new lite::cv::detection::YOLOP(onnx_path);
auto *detector = new lite::cv::detection::NanoDet(onnx_path); // Super fast and tiny!
auto *detector = new lite::cv::detection::NanoDetPlus(onnx_path); // Super fast and tiny! 2021/12/25
auto *detector = new lite::cv::detection::NanoDetEfficientNetLite(onnx_path); // Super fast and tiny!
```

****

<div id="lite.ai.toolkit-matting"></div>  

#### æ¡ˆä¾‹1: ä½¿ç”¨[RobustVideoMatting2021ğŸ”¥ğŸ”¥ğŸ”¥](https://github.com/PeterL1n/RobustVideoMatting) è¿›è¡Œè§†é¢‘æŠ å›¾ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚

```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/rvm_mobilenetv3_fp32.onnx";
  std::string video_path = "../../../examples/lite/resources/test_lite_rvm_0.mp4";
  std::string output_path = "../../../logs/test_lite_rvm_0.mp4";
  
  auto *rvm = new lite::cv::matting::RobustVideoMatting(onnx_path, 16); // 16 threads
  std::vector<lite::types::MattingContent> contents;
  
  // 1. video matting.
  rvm->detect_video(video_path, output_path, contents, false, 0.4f);
  
  delete rvm;
}
```
è¾“å‡ºçš„ç»“æœæ˜¯:

<div align='center'>
  <img src='docs/resources/interviewi.gif' height="80px" width="150px">
  <img src='docs/resources/interview.gif' height="80px" width="150px">  
  <img src='docs/resources/dance3i.gif' height="80px" width="150px">
  <img src='docs/resources/dance3.gif' height="80px" width="150px">
  <br>
  <img src='docs/resources/teslai.gif' height="80px" width="150px">
  <img src='docs/resources/tesla.gif' height="80px" width="150px">  
  <img src='docs/resources/b5i.gif' height="80px" width="150px">
  <img src='docs/resources/b5.gif' height="80px" width="150px">
</div>

æ›´å¤šå¯ç”¨çš„æŠ å›¾æ¨¡å‹ï¼ˆå›¾ç‰‡æŠ å›¾ã€è§†é¢‘æŠ å›¾ã€trimap/mask-freeã€trimap/mask-basedï¼‰:
```c++
auto *matting = new lite::cv::matting::RobustVideoMatting:(onnx_path);  //  WACV 2022.
auto *matting = new lite::cv::matting::MGMatting(onnx_path); // CVPR 2021
auto *matting = new lite::cv::matting::MODNet(onnx_path); // AAAI 2022
auto *matting = new lite::cv::matting::MODNetDyn(onnx_path); // AAAI 2022 Dynamic Shape Inference.
auto *matting = new lite::cv::matting::BackgroundMattingV2(onnx_path); // CVPR 2020 
auto *matting = new lite::cv::matting::BackgroundMattingV2Dyn(onnx_path); // CVPR 2020 Dynamic Shape Inference.
```

****

<div id="lite.ai.toolkit-face-alignment"></div>

#### æ¡ˆä¾‹2: ä½¿ç”¨[FaceLandmarks1000](https://github.com/Single430/FaceLandmark1000) è¿›è¡Œäººè„¸1000å…³é”®ç‚¹æ£€æµ‹ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/FaceLandmark1000.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_face_landmarks_0.png";
  std::string save_img_path = "../../../logs/test_lite_face_landmarks_1000.jpg";
    
  auto *face_landmarks_1000 = new lite::cv::face::align::FaceLandmark1000(onnx_path);

  lite::types::Landmarks landmarks;
  cv::Mat img_bgr = cv::imread(test_img_path);
  face_landmarks_1000->detect(img_bgr, landmarks);
  lite::utils::draw_landmarks_inplace(img_bgr, landmarks);
  cv::imwrite(save_img_path, img_bgr);
  
  delete face_landmarks_1000;
}
```
è¾“å‡ºçš„ç»“æœæ˜¯:
<div align='center'>
  <img src='logs/test_lite_face_landmarks_1000.jpg' height="224px" width="224px">
  <img src='logs/test_lite_face_landmarks_1000_2.jpg' height="224px" width="224px">
  <img src='logs/test_lite_face_landmarks_1000_0.jpg' height="224px" width="224px">
</div>    

æ›´å¤šå¯ç”¨çš„äººè„¸å…³é”®ç‚¹æ£€æµ‹å™¨ï¼ˆ68ç‚¹ã€98ç‚¹ã€106ç‚¹ã€1000ç‚¹ï¼‰:
```c++
auto *align = new lite::cv::face::align::PFLD(onnx_path);  // 106 landmarks, 1.0Mb only!
auto *align = new lite::cv::face::align::PFLD98(onnx_path);  // 98 landmarks, 4.8Mb only!
auto *align = new lite::cv::face::align::PFLD68(onnx_path);  // 68 landmarks, 2.8Mb only!
auto *align = new lite::cv::face::align::MobileNetV268(onnx_path);  // 68 landmarks, 9.4Mb only!
auto *align = new lite::cv::face::align::MobileNetV2SE68(onnx_path);  // 68 landmarks, 11Mb only!
auto *align = new lite::cv::face::align::FaceLandmark1000(onnx_path);  // 1000 landmarks, 2.0Mb only!
auto *align = new lite::cv::face::align::PIPNet98(onnx_path);  // 98 landmarks, CVPR2021!
auto *align = new lite::cv::face::align::PIPNet68(onnx_path);  // 68 landmarks, CVPR2021!
auto *align = new lite::cv::face::align::PIPNet29(onnx_path);  // 29 landmarks, CVPR2021!
auto *align = new lite::cv::face::align::PIPNet19(onnx_path);  // 19 landmarks, CVPR2021!
```

****  

<div id="lite.ai.toolkit-colorization"></div>

#### æ¡ˆä¾‹3: ä½¿ç”¨[colorization](https://github.com/richzhang/colorization) è¿›è¡Œå›¾åƒç€è‰²ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/eccv16-colorizer.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_colorizer_1.jpg";
  std::string save_img_path = "../../../logs/test_lite_eccv16_colorizer_1.jpg";
  
  auto *colorizer = new lite::cv::colorization::Colorizer(onnx_path);
  
  cv::Mat img_bgr = cv::imread(test_img_path);
  lite::types::ColorizeContent colorize_content;
  colorizer->detect(img_bgr, colorize_content);
  
  if (colorize_content.flag) cv::imwrite(save_img_path, colorize_content.mat);
  delete colorizer;
}
```
è¾“å‡ºçš„ç»“æœæ˜¯:

<div align='center'>
  <img src='examples/lite/resources/test_lite_colorizer_1.jpg' height="224px" width="224px">
  <img src='examples/lite/resources/test_lite_colorizer_2.jpg' height="224px" width="224px">
  <img src='examples/lite/resources/test_lite_colorizer_3.jpg' height="224px" width="224px">  
  <br> 
  <img src='logs/test_lite_siggraph17_colorizer_1.jpg' height="224px" width="224px">
  <img src='logs/test_lite_siggraph17_colorizer_2.jpg' height="224px" width="224px">
  <img src='logs/test_lite_siggraph17_colorizer_3.jpg' height="224px" width="224px">
</div>    

æ›´å¤šå¯ç”¨çš„ç€è‰²å™¨æ¨¡å‹ï¼ˆç°åº¦å›¾è½¬å½©è‰²å›¾ï¼‰:
```c++
auto *colorizer = new lite::cv::colorization::Colorizer(onnx_path);
```

****

<div id="lite.ai.toolkit-face-recognition"></div>  

#### æ¡ˆä¾‹4: ä½¿ç”¨[ArcFace](https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch) è¿›è¡Œäººè„¸è¯†åˆ«ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚

```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/ms1mv3_arcface_r100.onnx";
  std::string test_img_path0 = "../../../examples/lite/resources/test_lite_faceid_0.png";
  std::string test_img_path1 = "../../../examples/lite/resources/test_lite_faceid_1.png";
  std::string test_img_path2 = "../../../examples/lite/resources/test_lite_faceid_2.png";

  auto *glint_arcface = new lite::cv::faceid::GlintArcFace(onnx_path);

  lite::types::FaceContent face_content0, face_content1, face_content2;
  cv::Mat img_bgr0 = cv::imread(test_img_path0);
  cv::Mat img_bgr1 = cv::imread(test_img_path1);
  cv::Mat img_bgr2 = cv::imread(test_img_path2);
  glint_arcface->detect(img_bgr0, face_content0);
  glint_arcface->detect(img_bgr1, face_content1);
  glint_arcface->detect(img_bgr2, face_content2);

  if (face_content0.flag && face_content1.flag && face_content2.flag)
  {
    float sim01 = lite::utils::math::cosine_similarity<float>(
        face_content0.embedding, face_content1.embedding);
    float sim02 = lite::utils::math::cosine_similarity<float>(
        face_content0.embedding, face_content2.embedding);
    std::cout << "Detected Sim01: " << sim  << " Sim02: " << sim02 << std::endl;
  }

  delete glint_arcface;
}
```

è¾“å‡ºçš„ç»“æœæ˜¯:
<div align='center'>
  <img src='examples/lite/resources/test_lite_arcface_resnet_0.png' height="224px" width="224px">
  <img src='examples/lite/resources/test_lite_arcface_resnet_1.png' height="224px" width="224px">
  <img src='examples/lite/resources/test_lite_arcface_resnet_2.png' height="224px" width="224px">
</div>  

> Detected Sim01: 0.721159  Sim02: -0.0626267

æ›´å¤šå¯ç”¨çš„äººè„¸è¯†åˆ«æ¨¡å‹ï¼ˆäººè„¸ç‰¹å¾æå–ï¼‰:
```c++
auto *recognition = new lite::cv::faceid::GlintCosFace(onnx_path);  // DeepGlint(insightface)
auto *recognition = new lite::cv::faceid::GlintArcFace(onnx_path);  // DeepGlint(insightface)
auto *recognition = new lite::cv::faceid::GlintPartialFC(onnx_path); // DeepGlint(insightface)
auto *recognition = new lite::cv::faceid::FaceNet(onnx_path);
auto *recognition = new lite::cv::faceid::FocalArcFace(onnx_path);
auto *recognition = new lite::cv::faceid::FocalAsiaArcFace(onnx_path);
auto *recognition = new lite::cv::faceid::TencentCurricularFace(onnx_path); // Tencent(TFace)
auto *recognition = new lite::cv::faceid::TencentCifpFace(onnx_path); // Tencent(TFace)
auto *recognition = new lite::cv::faceid::CenterLossFace(onnx_path);
auto *recognition = new lite::cv::faceid::SphereFace(onnx_path);
auto *recognition = new lite::cv::faceid::PoseRobustFace(onnx_path);
auto *recognition = new lite::cv::faceid::NaivePoseRobustFace(onnx_path);
auto *recognition = new lite::cv::faceid::MobileFaceNet(onnx_path); // 3.8Mb only !
auto *recognition = new lite::cv::faceid::CavaGhostArcFace(onnx_path);
auto *recognition = new lite::cv::faceid::CavaCombinedFace(onnx_path);
auto *recognition = new lite::cv::faceid::MobileSEFocalFace(onnx_path); // 4.5Mb only !
```

****

<div id="lite.ai.toolkit-face-detection"></div>

#### æ¡ˆä¾‹5: ä½¿ç”¨[SCRFD 2021](https://github.com/deepinsight/insightface/blob/master/detection/scrfd/) è¿›è¡Œäººè„¸æ£€æµ‹ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/scrfd_2.5g_bnkps_shape640x640.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_face_detector.jpg";
  std::string save_img_path = "../../../logs/test_lite_scrfd.jpg";
  
  auto *scrfd = new lite::cv::face::detect::SCRFD(onnx_path);
  
  std::vector<lite::types::BoxfWithLandmarks> detected_boxes;
  cv::Mat img_bgr = cv::imread(test_img_path);
  scrfd->detect(img_bgr, detected_boxes);
  
  lite::utils::draw_boxes_with_landmarks_inplace(img_bgr, detected_boxes);
  cv::imwrite(save_img_path, img_bgr);
  
  std::cout << "Default Version Done! Detected Face Num: " << detected_boxes.size() << std::endl;
  
  delete scrfd;
}
```
è¾“å‡ºçš„ç»“æœæ˜¯:
<div align='center'>
  <img src='docs/resources/scrfd.jpg' height="224px" width="224px">
  <img src='docs/resources/scrfd_2.jpg' height="224px" width="224px">
  <img src='docs/resources/scrfd_3.jpg' height="224px" width="224px">
</div>  

æ›´å¤šå¯ç”¨çš„äººè„¸æ£€æµ‹å™¨ï¼ˆè½»é‡çº§äººè„¸æ£€æµ‹å™¨ï¼‰:
```c++
auto *detector = new lite::face::detect::UltraFace(onnx_path);  // 1.1Mb only !
auto *detector = new lite::face::detect::FaceBoxes(onnx_path);  // 3.8Mb only ! 
auto *detector = new lite::face::detect::FaceBoxesv2(onnx_path);  // 4.0Mb only ! 
auto *detector = new lite::face::detect::RetinaFace(onnx_path);  // 1.6Mb only ! CVPR2020
auto *detector = new lite::face::detect::SCRFD(onnx_path);  // 2.5Mb only ! CVPR2021, Super fast and accurate!!
auto *detector = new lite::face::detect::YOLO5Face(onnx_path);  // 2021, Super fast and accurate!!
```

<div id="lite.ai.toolkit-segmentation"></div>  

#### æ¡ˆä¾‹6: ä½¿ç”¨ [DeepLabV3ResNet101](https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/) è¿›è¡Œè¯­ä¹‰åˆ†å‰². è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/deeplabv3_resnet101_coco.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_deeplabv3_resnet101.png";
  std::string save_img_path = "../../../logs/test_lite_deeplabv3_resnet101.jpg";

  auto *deeplabv3_resnet101 = new lite::cv::segmentation::DeepLabV3ResNet101(onnx_path, 16); // 16 threads

  lite::types::SegmentContent content;
  cv::Mat img_bgr = cv::imread(test_img_path);
  deeplabv3_resnet101->detect(img_bgr, content);

  if (content.flag)
  {
    cv::Mat out_img;
    cv::addWeighted(img_bgr, 0.2, content.color_mat, 0.8, 0., out_img);
    cv::imwrite(save_img_path, out_img);
    if (!content.names_map.empty())
    {
      for (auto it = content.names_map.begin(); it != content.names_map.end(); ++it)
      {
        std::cout << it->first << " Name: " << it->second << std::endl;
      }
    }
  }
  delete deeplabv3_resnet101;
}
```

è¾“å‡ºçš„ç»“æœæ˜¯:  
<div align='center'>
  <img src='examples/lite/resources/test_lite_deeplabv3_resnet101.png' height="256px">
  <img src='logs/test_lite_deeplabv3_resnet101.jpg' height="256px">
</div> 

æ›´å¤šå¯ç”¨çš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼ˆäººåƒåˆ†å‰²ã€å®ä¾‹åˆ†å‰²ï¼‰:
```c++
auto *segment = new lite::cv::segmentation::FCNResNet101(onnx_path);
auto *segment = new lite::cv::segmentation::DeepLabV3ResNet101(onnx_path);
```

<div id="lite.ai.toolkit-face-attributes-analysis"></div>    

#### æ¡ˆä¾‹7: ä½¿ç”¨ [SSRNet](https://github.com/oukohou/SSR_Net_Pytorch) è¿›è¡Œå¹´é¾„ä¼°è®¡. è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/ssrnet.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_ssrnet.jpg";
  std::string save_img_path = "../../../logs/test_lite_ssrnet.jpg";

  lite::cv::face::attr::SSRNet *ssrnet = new lite::cv::face::attr::SSRNet(onnx_path);

  lite::types::Age age;
  cv::Mat img_bgr = cv::imread(test_img_path);
  ssrnet->detect(img_bgr, age);
  lite::utils::draw_age_inplace(img_bgr, age);
  cv::imwrite(save_img_path, img_bgr);
  std::cout << "Default Version Done! Detected SSRNet Age: " << age.age << std::endl;

  delete ssrnet;
}
```
è¾“å‡ºçš„ç»“æœæ˜¯:  
<div align='center'>
  <img src='logs/test_lite_ssrnet.jpg' height="224px" width="224px">
  <img src='logs/test_lite_gender_googlenet.jpg' height="224px" width="224px">
  <img src='logs/test_lite_emotion_ferplus.jpg' height="224px" width="224px">
</div>    

æ›´å¤šå¯ç”¨çš„äººè„¸å±æ€§è¯†åˆ«æ¨¡å‹ï¼ˆæ€§åˆ«ã€å¹´é¾„ã€æƒ…ç»ªï¼‰:
```c++
auto *attribute = new lite::cv::face::attr::AgeGoogleNet(onnx_path);  
auto *attribute = new lite::cv::face::attr::GenderGoogleNet(onnx_path); 
auto *attribute = new lite::cv::face::attr::EmotionFerPlus(onnx_path);
auto *attribute = new lite::cv::face::attr::VGG16Age(onnx_path);
auto *attribute = new lite::cv::face::attr::VGG16Gender(onnx_path);
auto *attribute = new lite::cv::face::attr::EfficientEmotion7(onnx_path); // 7 emotions, 15Mb only!
auto *attribute = new lite::cv::face::attr::EfficientEmotion8(onnx_path); // 8 emotions, 15Mb only!
auto *attribute = new lite::cv::face::attr::MobileEmotion7(onnx_path); // 7 emotions, 13Mb only!
auto *attribute = new lite::cv::face::attr::ReXNetEmotion7(onnx_path); // 7 emotions
auto *attribute = new lite::cv::face::attr::SSRNet(onnx_path); // age estimation, 190kb only!!!
```


<div id="lite.ai.toolkit-image-classification"></div>   

#### æ¡ˆä¾‹8: ä½¿ç”¨ [DenseNet](https://pytorch.org/hub/pytorch_vision_densenet/) è¿›è¡Œå›¾ç‰‡1000åˆ†ç±». è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/densenet121.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_densenet.jpg";

  auto *densenet = new lite::cv::classification::DenseNet(onnx_path);

  lite::types::ImageNetContent content;
  cv::Mat img_bgr = cv::imread(test_img_path);
  densenet->detect(img_bgr, content);
  if (content.flag)
  {
    const unsigned int top_k = content.scores.size();
    if (top_k > 0)
    {
      for (unsigned int i = 0; i < top_k; ++i)
        std::cout << i + 1
                  << ": " << content.labels.at(i)
                  << ": " << content.texts.at(i)
                  << ": " << content.scores.at(i)
                  << std::endl;
    }
  }
  delete densenet;
}
```

è¾“å‡ºçš„ç»“æœæ˜¯:  
<div align='center'>
  <img src='examples/lite/resources/test_lite_densenet.jpg' height="224px" width="224px">
  <img src='logs/test_lite_densenet.png' height="224px" width="500px">
</div>  

æ›´å¤šå¯ç”¨çš„å›¾åƒåˆ†ç±»æ¨¡å‹ï¼ˆ1000ç±»ï¼‰:
```c++
auto *classifier = new lite::cv::classification::EfficientNetLite4(onnx_path);  
auto *classifier = new lite::cv::classification::ShuffleNetV2(onnx_path); // 8.7Mb only!
auto *classifier = new lite::cv::classification::GhostNet(onnx_path);
auto *classifier = new lite::cv::classification::HdrDNet(onnx_path);
auto *classifier = new lite::cv::classification::IBNNet(onnx_path);
auto *classifier = new lite::cv::classification::MobileNetV2(onnx_path); // 13Mb only!
auto *classifier = new lite::cv::classification::ResNet(onnx_path); 
auto *classifier = new lite::cv::classification::ResNeXt(onnx_path);
```


<div id="lite.ai.toolkit-head-pose-estimation"></div>   

#### æ¡ˆä¾‹9: ä½¿ç”¨ [FSANet](https://github.com/omasaht/headpose-fsanet-pytorch) è¿›è¡Œå¤´éƒ¨å§¿æ€è¯†åˆ«. è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚

```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/fsanet-var.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_fsanet.jpg";
  std::string save_img_path = "../../../logs/test_lite_fsanet.jpg";

  auto *fsanet = new lite::cv::face::pose::FSANet(onnx_path);
  cv::Mat img_bgr = cv::imread(test_img_path);
  lite::types::EulerAngles euler_angles;
  fsanet->detect(img_bgr, euler_angles);
  
  if (euler_angles.flag)
  {
    lite::utils::draw_axis_inplace(img_bgr, euler_angles);
    cv::imwrite(save_img_path, img_bgr);
    std::cout << "yaw:" << euler_angles.yaw << " pitch:" << euler_angles.pitch << " row:" << euler_angles.roll << std::endl;
  }
  delete fsanet;
}
```

è¾“å‡ºçš„ç»“æœæ˜¯:  
<div align='center'>
  <img src='logs/test_lite_fsanet.jpg' height="224px" width="224px">
  <img src='logs/test_lite_fsanet_2.jpg' height="224px" width="224px">
  <img src='logs/test_lite_fsanet_3.jpg' height="224px" width="224px">
</div>  

æ›´å¤šå¯ç”¨çš„å¤´éƒ¨å§¿æ€è¯†åˆ«æ¨¡å‹ï¼ˆæ¬§æ‹‰è§’ã€yawã€pitchã€rollï¼‰:
```c++
auto *pose = new lite::cv::face::pose::FSANet(onnx_path); // 1.2Mb only!
```

<div id="lite.ai.toolkit-style-transfer"></div>  

#### æ¡ˆä¾‹10: ä½¿ç”¨ [FastStyleTransfer](https://github.com/onnx/models/tree/master/vision/style_transfer/fast_neural_style) è¿›è¡Œé£æ ¼è¿ç§». è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/style-candy-8.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_fast_style_transfer.jpg";
  std::string save_img_path = "../../../logs/test_lite_fast_style_transfer_candy.jpg";
  
  auto *fast_style_transfer = new lite::cv::style::FastStyleTransfer(onnx_path);
 
  lite::types::StyleContent style_content;
  cv::Mat img_bgr = cv::imread(test_img_path);
  fast_style_transfer->detect(img_bgr, style_content);

  if (style_content.flag) cv::imwrite(save_img_path, style_content.mat);
  delete fast_style_transfer;
}
```
è¾“å‡ºçš„ç»“æœæ˜¯:  

<div align='center'>
  <img src='examples/lite/resources/test_lite_fast_style_transfer.jpg' height="224px">
  <img src='logs/test_lite_fast_style_transfer_candy.jpg' height="224px">
  <img src='logs/test_lite_fast_style_transfer_mosaic.jpg' height="224px">  
  <br> 
  <img src='logs/test_lite_fast_style_transfer_pointilism.jpg' height="224px">
  <img src='logs/test_lite_fast_style_transfer_rain_princes.jpg' height="224px">
  <img src='logs/test_lite_fast_style_transfer_udnie.jpg' height="224px">
</div>

æ›´å¤šå¯ç”¨çš„é£æ ¼è¿ç§»æ¨¡å‹ï¼ˆè‡ªç„¶é£æ ¼è¿ç§»ã€å…¶ä»–ï¼‰:
```c++
auto *transfer = new lite::cv::style::FastStyleTransfer(onnx_path); // 6.4Mb only
```

## 7. å¼€æºåè®®

<div id="lite.ai.toolkit-License"></div>

[Lite.Ai.ToolKit](#lite.ai.toolkit-Introduction) çš„ä»£ç é‡‡ç”¨GPL-3.0åè®®ã€‚


## 8. å¼•ç”¨å‚è€ƒ

<div id="lite.ai.toolkit-References"></div>

æœ¬é¡¹ç›®å‚è€ƒäº†ä»¥ä¸‹å¼€æºé¡¹ç›®ã€‚

* [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) (ğŸ”¥ğŸ”¥ğŸ”¥new!!â†‘)
* [nanodet](https://github.com/RangiLyu/nanodet) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) (ğŸ”¥ğŸ”¥ğŸ”¥new!!â†‘)
* [YOLOP](https://github.com/hustvl/YOLOP) (ğŸ”¥ğŸ”¥new!!â†‘)
* [YOLOR](https://github.com/WongKinYiu/yolor) (ğŸ”¥ğŸ”¥new!!â†‘)
* [ScaledYOLOv4](https://github.com/WongKinYiu/ScaledYOLOv4) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [insightface](https://github.com/deepinsight/insightface) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [yolov5](https://github.com/ultralytics/yolov5) (ğŸ”¥ğŸ”¥ğŸ’¥â†‘)
* [TFace](https://github.com/Tencent/TFace) (ğŸ”¥ğŸ”¥â†‘)
* [YOLOv4-pytorch](https://github.com/argusswift/YOLOv4-pytorch) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [Ultra-Light-Fast-Generic-Face-Detector-1MB](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)

<details>
<summary> å±•å¼€æ›´å¤šå¼•ç”¨å‚è€ƒ </summary>  

* [headpose-fsanet-pytorch](https://github.com/omasaht/headpose-fsanet-pytorch) (ğŸ”¥â†‘)
* [pfld_106_face_landmarks](https://github.com/Hsintao/pfld_106_face_landmarks) (ğŸ”¥ğŸ”¥â†‘)
* [onnx-models](https://github.com/onnx/models) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [SSR_Net_Pytorch](https://github.com/oukohou/SSR_Net_Pytorch) (ğŸ”¥â†‘)
* [colorization](https://github.com/richzhang/colorization) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [SUB_PIXEL_CNN](https://github.com/niazwazir/SUB_PIXEL_CNN) (ğŸ”¥â†‘)
* [torchvision](https://github.com/pytorch/vision) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [facenet-pytorch](https://github.com/timesler/facenet-pytorch) (ğŸ”¥â†‘)
* [face.evoLVe.PyTorch](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [center-loss.pytorch](https://github.com/louis-she/center-loss.pytorch) (ğŸ”¥ğŸ”¥â†‘)
* [sphereface_pytorch](https://github.com/clcarwin/sphereface_pytorch) (ğŸ”¥ğŸ”¥â†‘)
* [DREAM](https://github.com/penincillin/DREAM) (ğŸ”¥ğŸ”¥â†‘)
* [MobileFaceNet_Pytorch](https://github.com/Xiaoccer/MobileFaceNet_Pytorch) (ğŸ”¥ğŸ”¥â†‘)
* [cavaface.pytorch](https://github.com/cavalleria/cavaface.pytorch) (ğŸ”¥ğŸ”¥â†‘)
* [CurricularFace](https://github.com/HuangYG123/CurricularFace) (ğŸ”¥ğŸ”¥â†‘)
* [face-emotion-recognition](https://github.com/HSE-asavchenko/face-emotion-recognition) (ğŸ”¥â†‘)
* [face_recognition.pytorch](https://github.com/grib0ed0v/face_recognition.pytorch) (ğŸ”¥ğŸ”¥â†‘)
* [PFLD-pytorch](https://github.com/polarisZhao/PFLD-pytorch) (ğŸ”¥ğŸ”¥â†‘)
* [pytorch_face_landmark](https://github.com/cunjian/pytorch_face_landmark) (ğŸ”¥ğŸ”¥â†‘)
* [FaceLandmark1000](https://github.com/Single430/FaceLandmark1000) (ğŸ”¥ğŸ”¥â†‘)
* [Pytorch_Retinaface](https://github.com/biubug6/Pytorch_Retinaface) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch) (ğŸ”¥ğŸ”¥â†‘)

</details>    


## 9. ç¼–è¯‘é€‰é¡¹
æœªæ¥ä¼šå¢åŠ ä¸€äº›æ¨¡å‹çš„[MNN](https://github.com/alibaba/MNN) ã€[NCNN](https://github.com/Tencent/ncnn) å’Œ [TNN](https://github.com/Tencent/TNN) æ”¯æŒï¼Œä½†ç”±äºç®—å­å…¼å®¹ç­‰åŸå› ï¼Œä¹Ÿæ— æ³•ç¡®ä¿æ‰€æœ‰è¢«[ONNXRuntime C++](https://github.com/microsoft/onnxruntime) æ”¯æŒçš„æ¨¡å‹èƒ½å¤Ÿåœ¨[MNN](https://github.com/alibaba/MNN) ã€[NCNN](https://github.com/Tencent/ncnn) å’Œ [TNN](https://github.com/Tencent/TNN) ä¸‹è·‘é€šã€‚æ‰€ä»¥ï¼Œå¦‚æœæ‚¨æƒ³ä½¿ç”¨æœ¬é¡¹ç›®æ”¯æŒçš„æ‰€æœ‰æ¨¡å‹ï¼Œå¹¶ä¸”ä¸åœ¨æ„*1~2ms*çš„æ€§èƒ½å·®è·çš„è¯ï¼Œè¯·ä½¿ç”¨ONNXRuntimeç‰ˆæœ¬çš„å®ç°ã€‚[ONNXRuntime](https://github.com/microsoft/onnxruntime) æ˜¯æœ¬ä»“åº“é»˜è®¤çš„æ¨ç†å¼•æ“ã€‚ä½†æ˜¯å¦‚æœä½ ç¡®å®å¸Œæœ›ç¼–è¯‘æ”¯æŒ[MNN](https://github.com/alibaba/MNN) ã€[NCNN](https://github.com/Tencent/ncnn) å’Œ [TNN](https://github.com/Tencent/TNN) æ”¯æŒçš„Lite.Ai.ToolKitåŠ¨æ€åº“ï¼Œä½ å¯ä»¥æŒ‰ç…§ä»¥ä¸‹çš„æ­¥éª¤è¿›è¡Œè®¾ç½®ã€‚

* åœ¨`build.sh`ä¸­æ·»åŠ `DENABLE_MNN=ON` ã€`DENABLE_NCNN=ON` æˆ– `DENABLE_TNN=ON`ï¼Œæ¯”å¦‚
```shell
cd build && cmake \
  -DCMAKE_BUILD_TYPE=MinSizeRel \
  -DINCLUDE_OPENCV=ON \   # æ˜¯å¦æ‰“åŒ…OpenCVè¿›lite.ai.toolkitï¼Œé»˜è®¤ONï¼›å¦åˆ™ï¼Œä½ éœ€è¦å•ç‹¬è®¾ç½®OpenCV
  -DENABLE_MNN=ON \       # æ˜¯å¦ç¼–è¯‘MNNç‰ˆæœ¬çš„æ¨¡å‹ï¼Œ é»˜è®¤OFFï¼Œç›®å‰åªæ”¯æŒéƒ¨åˆ†æ¨¡å‹
  -DENABLE_NCNN=OFF \     # æ˜¯å¦ç¼–è¯‘NCNNç‰ˆæœ¬çš„æ¨¡å‹ï¼Œé»˜è®¤OFFï¼Œç›®å‰åªæ”¯æŒéƒ¨åˆ†æ¨¡å‹
  -DENABLE_TNN=OFF \      # æ˜¯å¦ç¼–è¯‘TNNç‰ˆæœ¬çš„æ¨¡å‹ï¼Œ é»˜è®¤OFFï¼Œç›®å‰åªæ”¯æŒéƒ¨åˆ†æ¨¡å‹
  .. && make -j8
```  
* ä½¿ç”¨MNNã€NCNNæˆ–TNNç‰ˆæœ¬çš„æ¥å£ï¼Œè¯¦è§æ¡ˆä¾‹[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_nanodet.cpp) ï¼Œæ¯”å¦‚
```C++
auto *nanodet = new lite::mnn::cv::detection::NanoDet(mnn_path);
auto *nanodet = new lite::tnn::cv::detection::NanoDet(proto_path, model_path);
auto *nanodet = new lite::ncnn::cv::detection::NanoDet(param_path, bin_path);
```

## 10. å¦‚ä½•æ·»åŠ æ‚¨çš„æ¨¡å‹
<div id="lite.ai.toolkit-Contribute"></div>  

å¦‚ä½•æ·»åŠ æ‚¨è‡ªå·±çš„æ¨¡å‹ä»¥åŠæˆä¸ºè´¡çŒ®è€…ï¼Ÿå…·ä½“æ­¥éª¤è¯·å‚è€ƒ [CONTRIBUTING.zh.md](https://github.com/DefTruth/lite.ai.toolkit/issues/191) ï¼Œæˆ–è€…ï¼Œâ¤ï¸ä¸å¦¨ç»™ä¸ªâ­ï¸ğŸŒŸstarï¼Œè¿™åº”è¯¥æ˜¯æœ€ç®€å•çš„æ”¯æŒæ–¹å¼äº†ã€‚

