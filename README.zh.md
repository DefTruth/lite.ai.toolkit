

## Lite.AI.ToolKit ğŸš€ğŸš€ğŸŒŸ: ä¸€ä¸ªå¼€ç®±å³ç”¨çš„C++ AIæ¨¡å‹å·¥å…·ç®±

[![](https://img.shields.io/badge/MacOS-pass-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/releases/tag/v0.1.0) ![](https://img.shields.io/badge/Linux-pass-brightgreen.svg) ![](https://img.shields.io/badge/Windows-pass-brightgreen.svg) [![](https://img.shields.io/badge/Version-0.1.0-green.svg)](https://github.com/DefTruth/lite.ai.toolkit/releases/tag/v0.1.0) ![](https://img.shields.io/badge/Language-C/C%2B%2B-orange.svg) ![](https://img.shields.io/badge/Device-GPU/CPU-yellow.svg) ![](https://img.shields.io/badge/License-GPLv3-blue.svg)


<div id="lite.ai.toolkit-Introduction"></div>  

<div align='center'>
  <img src='logs/test_lite_yolov5_1.jpg' height="100px" width="100px">
  <img src='docs/resources/efficientdet_d0.jpg' height="100px" width="100px">
  <img src='docs/resources/street.jpg' height="100px" width="100px">
  <img src='logs/test_lite_ultraface.jpg' height="100px" width="100px">
  <img src='logs/test_lite_face_landmarks_1000.jpg' height="100px" width="100px">
  <img src='logs/test_lite_fsanet.jpg' height="100px" width="100px">
  <img src='logs/test_lite_deeplabv3_resnet101.jpg' height="100px" width="100px">
  <img src='logs/test_lite_fast_style_transfer_mosaic.jpg' height="100px" width="100px"> 
  <br>
  <img src='docs/resources/teslai.gif' height="100px" width="100px">
  <img src='docs/resources/tesla.gif' height="100px" width="100px">
  <img src='docs/resources/dance3i.gif' height="100px" width="100px">
  <img src='docs/resources/dance3.gif' height="100px" width="100px">  
  <img src='docs/resources/yolop1.png' height="100px" width="100px">
  <img src='docs/resources/yolop1.gif' height="100px" width="100px">
  <img src='docs/resources/yolop2.png' height="100px" width="100px">
  <img src='docs/resources/yolop2.gif' height="100px" width="100px">

</div>    

![](https://img.shields.io/github/stars/DefTruth/lite.ai.toolkit.svg?style=social) ![](https://img.shields.io/github/forks/DefTruth/lite.ai.toolkit.svg?style=social) ![](https://img.shields.io/github/watchers/DefTruth/lite.ai.toolkit.svg?style=social)



---

*Lite.AI.ToolKit* ğŸš€ğŸš€ğŸŒŸ: ä¸€ä¸ªè½»é‡çº§çš„`C++` AIæ¨¡å‹å·¥å…·ç®±ï¼Œç”¨æˆ·å‹å¥½ï¼Œå¼€ç®±å³ç”¨ã€‚å·²ç»åŒ…æ‹¬ *[70+](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md)* æµè¡Œçš„å¼€æºæ¨¡å‹ï¼Œè¿˜ä¼šç»§ç»­å¢åŠ ğŸ˜ã€‚è¿™æ˜¯ä¸€ä¸ªæ ¹æ®ä¸ªäººå…´è¶£æ•´ç†çš„C++å·¥å…·ç®±ï¼Œç›®å‰ä¸»è¦é›†ä¸­åœ¨`æ£€æµ‹ã€åˆ†å‰²ã€æŠ å›¾ã€è¯†åˆ«å’Œç›®æ ‡è·Ÿè¸ª`ç­‰é¢†åŸŸï¼ŒåŒ…å«äº†æœ€æ–°çš„RVM, YOLOX, YOLOP, YOLOR, YoloV5, DeepLabV3, ArcFaceç­‰æ¨¡å‹ã€‚ *Lite.AI.ToolKit* é»˜è®¤æ˜¯åŸºäº *[ONNXRuntime C++](https://github.com/microsoft/onnxruntime)* æ¨ç†å¼•æ“çš„ï¼ŒåæœŸå¯èƒ½ä¼šåŠ å…¥å¯¹ *[ncnn](https://github.com/Tencent/ncnn)* æˆ– *[MNN](https://github.com/alibaba/MNN)* çš„æ”¯æŒï¼Œç›®å‰ä¸»è¦è€ƒè™‘æ˜“ç”¨æ€§ã€‚éœ€è¦æ›´é«˜æ€§èƒ½æ”¯æŒçš„å°ä¼™ä¼´å¯ä»¥åŸºäºæœ¬é¡¹ç›®æä¾›çš„`C++`å®ç°å’Œ`ONNX`æ–‡ä»¶è¿›è¡Œä¼˜åŒ–~ å¦‚æœæ‚¨æœ‰æƒ³æ·»åŠ åˆ°æœ¬é¡¹ç›®çš„æ–°æ¨¡å‹ï¼Œæ¬¢è¿`PR` ~ğŸ‘ğŸ‘‹ 

<p align="center"><a href="README.zh.md">English</a> | ä¸­æ–‡ </p>

---  

### Lite.AI.ToolKit ğŸš€ğŸš€ğŸŒŸ çš„æ ¸å¿ƒç‰¹æ€§  

* â¤ï¸ *ç”¨æˆ·å‹å¥½ï¼Œå¼€ç®±å³ç”¨ã€‚*  
  ä½¿ç”¨ç®€å•ä¸€è‡´çš„è°ƒç”¨è¯­æ³•ï¼Œå¦‚*lite::cv::Type::Class*ï¼Œè¯¦è§[examples](#lite.ai.toolkit-Examples-for-Lite.AI.ToolKit).
  ```c++
    auto *yolox = new lite::cv::detection::YoloX("yolox_nano.onnx");  // 3.5Mb only !
    auto *yolov5 = new lite::cv::detection::YoloV5("yolov5s.onnx");  // for mobile device  
  ```
* âš¡ *å°‘é‡ä¾èµ–ï¼Œæ„å»ºå®¹æ˜“ã€‚*  
  ç›®å‰, åªä¾èµ– *OpenCV* å’Œ *ONNXRuntime*ï¼Œè¯¦è§[build](#lite.ai.toolkit-Build-Lite.AI.ToolKit)ã€‚å¯¹äº MacOSï¼Œåªéœ€è¿è¡Œ.ğŸ‘‡
    ```shell
    git clone --depth=1 https://github.com/DefTruth/lite.ai.toolkit.git  # æœ€æ–°æºç 
    cd lite.ai.toolkit && sh ./build.sh  # å¯¹äºMacOS, ä½ å¯ä»¥ç›´æ¥åˆ©ç”¨æœ¬é¡¹ç›®åŒ…å«çš„ä¾èµ–åº“ï¼Œæ— éœ€é‡æ–°ç¼–è¯‘
    ```
* âœ… *å¤šå¹³å°ç¼–è¯‘æ”¯æŒï¼ŒGPU/CPUæ”¯æŒã€‚*  
  ç›®å‰ï¼Œæ”¯æŒ [MacOS/Linux/Windows](#lite.ai.toolkit-Introduction) å’Œ CPU/GPUã€‚æœªæ¥ä¼šå¯¹æ›´å¤šå¹³å°è¿›è¡Œæ”¯æŒ~

* â¤ï¸ *ä¼—å¤šçš„ç®—æ³•æ¨¡å—ï¼Œä¸”æŒç»­æ›´æ–°*  
  ç›®å‰ï¼ŒåŒ…æ‹¬ 10+ ç®—æ³•æ¨¡å—å’Œ *[70+](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md)* æµè¡Œçš„å¼€æºæ¨¡å‹, æ¶µç›–[ç›®æ ‡æ£€æµ‹](#lite.ai.toolkit-object-detection)ã€[äººè„¸æ£€æµ‹](#lite.ai.toolkit-face-detection)ã€[äººè„¸è¯†åˆ«](#lite.ai.toolkit-face-recognition)ã€[è¯­ä¹‰åˆ†å‰²](#lite.ai.toolkit-segmentation)ã€[æŠ å›¾](#lite.ai.toolkit-matting)ç­‰é¢†åŸŸã€‚è¯¦è§ [Model Zoo](#lite.ai.toolkit-Model-Zoo)ã€‚æ›´å¤šçš„æ–°æ¨¡å‹å°†ä¼šä¸æ–­åœ°åŠ å…¥è¿›æ¥ ~ ğŸ˜

* âœ… *æœ€æ–°çš„å‘è¡Œç‰ˆæœ¬ï¼Œä»¥åŠè¯¦ç»†çš„æ–‡æ¡£ã€‚*

  |å‘è¡Œç‰ˆæœ¬|å¿«é€Ÿå¼€å§‹|è¯¦ç»†ç”¨æ³•| 
    |:---:|:---:|:---:| 
  | ğŸ‘‰ [lite.ai.toolkit.macos.v0.1.0](https://github.com/DefTruth/lite.ai.toolkit.demo/tree/main/releases/macos/v0.1.0) |  ğŸ‘‰ [lite.ai.toolkit.demo](https://github.com/DefTruth/lite.ai.toolkit.demo) & [Quick Start Examples](#lite.ai.toolkit-Examples-for-Lite.AI.ToolKit) |  ğŸ‘‰ [lite.ai.toolkit.examples](https://github.com/DefTruth/lite.ai.toolkit/tree/main/examples/lite/cv) |

---

## é‡è¦æ›´æ–° !!!

* ğŸ”¥ (20210920) å¢åŠ [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) è§†é¢‘æŠ å›¾! é€šè¿‡[*lite::cv::matting::RobustVideoMatting*](#lite.ai.toolkit-matting)è°ƒç”¨! è¯¦è§[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_rvm.cpp).

<div align='center'>
  <img src='docs/resources/interviewi.gif' height="100px" width="100px">
  <img src='docs/resources/interview.gif' height="100px" width="100px">  
  <img src='docs/resources/dance3i.gif' height="100px" width="100px">
  <img src='docs/resources/dance3.gif' height="100px" width="100px">
  <img src='docs/resources/teslai.gif' height="100px" width="100px">
  <img src='docs/resources/tesla.gif' height="100px" width="100px">  
  <img src='docs/resources/b5i.gif' height="100px" width="100px">
  <img src='docs/resources/b5.gif' height="100px" width="100px">
</div>


* ğŸ”¥ (20210915) å¢åŠ [YOLOP](https://github.com/hustvl/YOLOP) å…¨æ™¯ğŸš—æ„ŸçŸ¥! é€šè¿‡[*lite::cv::detection::YOLOP*](#lite.ai.toolkit-object-detection)è°ƒç”¨! è¯¦è§[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolop.cpp).

<div align='center'>
  <img src='docs/resources/yolop1.png' height="100px" width="200px">
  <img src='docs/resources/yolop1.gif' height="100px" width="200px">
  <img src='docs/resources/yolop2.png' height="100px" width="200px">
  <img src='docs/resources/yolop2.gif' height="100px" width="200px">

</div>   


* âœ… (20210807) å¢åŠ [YoloR](https://github.com/WongKinYiu/yolor) ! é€šè¿‡[*lite::cv::detection::YoloR*](#lite.ai.toolkit-object-detection)è°ƒç”¨! è¯¦è§[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolor.cpp).
* âœ… (20210731) å¢åŠ [RetinaFace-CVPR2020](https://github.com/biubug6/Pytorch_Retinaface) è¶…è½»é‡çº§äººè„¸æ£€æµ‹, ä»…1.6Mb! è¯¦è§[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_retinaface.cpp).
* ğŸ”¥ (20210721) å¢åŠ [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)! é€šè¿‡[*lite::cv::detection::YoloX*](#lite.ai.toolkit-object-detection)è°ƒç”¨! è¯¦è§[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolox.cpp).


<details>
<summary> å±•å¼€æ›´å¤šæ›´æ–° </summary>  

## æ›´å¤šæ›´æ–° !!!

* âœ… (20210815) å¢åŠ  [EfficientDet](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch) ç›®æ ‡æ£€æµ‹! è¯¦è§ [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_efficientdet.cpp).
* âœ… (20210808) å¢åŠ  [ScaledYoloV4](https://github.com/WongKinYiu/ScaledYOLOv4) ç›®æ ‡æ£€æµ‹! è¯¦è§ [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_scaled_yolov4.cpp).
* âœ… (20210807) å¢åŠ  [TinyYoloV4VOC](https://github.com/bubbliiiing/yolov4-tiny-pytorch) ç›®æ ‡æ£€æµ‹! è¯¦è§ [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_tiny_yolov4_voc.cpp).
* âœ… (20210807) å¢åŠ  [TinyYoloV4COCO](https://github.com/bubbliiiing/yolov4-tiny-pytorch) ç›®æ ‡æ£€æµ‹! è¯¦è§ [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_tiny_yolov4_coco.cpp).
* âœ… (20210722) æ›´æ–° [lite.ai.toolkit.hub.onnx.md](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md) ! *Lite.AI.Toolkit* ç›®å‰åŒ…å« *[70+](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md)* AI æ¨¡å‹ä»¥åŠ *[150+](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md)* .onnxæ–‡ä»¶.
* âš ï¸ (20210802) å¢åŠ  GPUå…¼å®¹CUDAExecutionProvider. è¯¦è§ [issue#10](https://github.com/DefTruth/lite.ai.toolkit/issues/10).
* âš ï¸ (20210801) ä¿®å¤ [issue#9](https://github.com/DefTruth/lite.ai.toolkit/issues/9) YOLOX éçŸ©å½¢æ¨ç†é”™è¯¯. è¯¦è§ [yolox.cpp](https://github.com/DefTruth/lite.ai.toolkit/blob/main/ort/cv/yolox.cpp).
* âœ… (20210801) å¢åŠ  [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch) äººè„¸æ£€æµ‹! è¯¦è§ [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_faceboxes.cpp).
* âœ… (20210727) å¢åŠ  [MobileNetV2SE68ã€PFLD68](https://github.com/cunjian/pytorch_face_landmark) äººè„¸68å…³é”®ç‚¹æ£€æµ‹! è¯¦è§ [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_pfld68.cpp).
* âœ… (20210726) å¢åŠ  [PFLD98](https://github.com/polarisZhao/PFLD-pytorch) äººè„¸98å…³é”®ç‚¹æ£€æµ‹! è¯¦è§ [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_pfld98.cpp).

</details>


## ç›®å½•
* [ç¼–è¯‘](#lite.ai.toolkit-Build-Lite.AI.ToolKit)
* [æ¨¡å‹ä¸‹è½½](#lite.ai.toolkit-Model-Zoo)
* [åº”ç”¨æ¡ˆä¾‹](#lite.ai.toolkit-Examples-for-Lite.AI.ToolKit)
* [APIæ–‡æ¡£](#lite.ai.toolkit-Lite.AI.ToolKit-API-Docs)
* [å…¶ä»–æ–‡æ¡£](#lite.ai.toolkit-Other-Docs)
* [å¼€æºåè®®](#lite.ai.toolkit-License)
* [å¼•ç”¨å‚è€ƒ](#lite.ai.toolkit-References)


## 1. ç¼–è¯‘Lite.AI.ToolKit

<div id="lite.ai.toolkit-Build-Lite.AI.ToolKit"></div>

ä»*Lite.AI.ToolKit* æºç ç¼–è¯‘*MacOS*ä¸‹çš„åŠ¨æ€åº“ã€‚éœ€è¦æ³¨æ„çš„æ˜¯*Lite.AI.ToolKit* ä½¿ç”¨`onnxruntime`ä½œä¸ºé»˜è®¤çš„åç«¯ï¼Œå› ä¸ºonnxruntimeæ”¯æŒå¤§éƒ¨åˆ†onnxçš„åŸç”Ÿç®—å­ï¼Œå…·æœ‰æ›´é«˜çš„æ˜“ç”¨æ€§ã€‚


<details>
<summary> Linux å’Œ Windows </summary>  

### Linux å’Œ Windows

âš ï¸ *Lite.AI.ToolKit* çš„å‘è¡Œç‰ˆæœ¬ç›®å‰ä¸ç›´æ¥æ”¯æŒLinuxå’ŒWindowsï¼Œä½ éœ€è¦ä»ä¸‹è½½*Lite.AI.ToolKit*çš„æºç è¿›è¡Œæ„å»ºã€‚é¦–å…ˆï¼Œä½ éœ€è¦ä¸‹è½½(å¦‚æœæœ‰å®˜æ–¹ç¼–è¯‘å¥½çš„å‘è¡Œç‰ˆæœ¬çš„è¯)æˆ–ç¼–è¯‘*OpenCV* and *ONNXRuntime*çš„åŠ¨æ€åº“ï¼Œç„¶åæŠŠå®ƒä»¬æ”¾å…¥*third_party*æ–‡ä»¶å¤¹ã€‚è¯·å‚è€ƒä¾èµ–åº“çš„ç¼–è¯‘æ–‡æ¡£[<sup>1</sup>](#lite.ai.toolkit-1)ã€‚


* Windows: ä½ å¯ä»¥å‚è€ƒ[issue#6](https://github.com/DefTruth/lite.ai.toolkit/issues/6) ï¼Œè®¨è®ºäº†å¸¸è§çš„ç¼–è¯‘é—®é¢˜ã€‚
* Linux: å‚è€ƒMacOSä¸‹çš„ç¼–è¯‘ï¼Œæ›¿æ¢Linuxç‰ˆæœ¬çš„ä¾èµ–åº“å³å¯ã€‚Linuxä¸‹çš„å‘è¡Œç‰ˆæœ¬å°†ä¼šåœ¨è¿‘æœŸæ·»åŠ  ~ [issue#2](https://github.com/DefTruth/lite.ai.toolkit/issues/2)
* ä»¤äººå¼€å¿ƒçš„æ¶ˆæ¯!!! : ğŸš€ ä½ å¯ä»¥ç›´æ¥ä¸‹è½½æœ€æ–°çš„*ONNXRuntime*å®˜æ–¹æ„å»ºçš„åŠ¨æ€åº“ï¼ŒåŒ…å«Windows, Linux, MacOS and Armçš„ç‰ˆæœ¬!!! CPUå’ŒGPUçš„ç‰ˆæœ¬å‡å¯è·å¾—ã€‚ä¸éœ€è¦å†ä»æºç è¿›è¡Œç¼–è¯‘äº†ï¼Œniceã€‚å¯ä»¥ä»[v1.8.1](https://github.com/microsoft/onnxruntime/releases) ä¸‹è½½æœ€æ–°çš„åŠ¨æ€åº“. æˆ‘ç›®å‰åœ¨*Lite.AI.ToolKit*ä¸­ç”¨çš„æ˜¯1.7.0ï¼Œä½ å¯ä»¥ä»[v1.7.0](https://github.com/microsoft/onnxruntime/releases/tag/v1.7.0) ä¸‹è½½, ä½†1.8.1åº”è¯¥ä¹Ÿæ˜¯å¯è¡Œçš„ã€‚å¯¹äº*OpenCV*ï¼Œè¯·å°è¯•ä»æºç æ„å»º(Linux) æˆ–è€… ç›´æ¥ä»[OpenCV 4.5.3](https://github.com/opencv/opencv/releases) ä¸‹è½½å®˜æ–¹ç¼–è¯‘å¥½çš„åŠ¨æ€åº“(Windows). ç„¶åæŠŠå¤´æ–‡ä»¶å’Œä¾èµ–åº“æ”¾å…¥ *third_party* æ–‡ä»¶å¤¹.

</details>  


* ä¸‹è½½Lite.AI.ToolKitæºç :
```shell
git clone --depth=1 https://github.com/DefTruth/lite.ai.toolkit.git  # latest
```


* ç¼–è¯‘åŠ¨æ€åº“.
```shell
cd lite.ai.toolkit
sh ./build.sh
```

* GPUå…¼å®¹æ€§: è¯¦è§[issue#10](https://github.com/DefTruth/lite.ai.toolkit/issues/10).

<details>
<summary> å¦‚ä½•é“¾æ¥Lite.AI.ToolKitåŠ¨æ€åº“?</summary>  

```shell
cd ./build/lite.ai.toolkit/lib && otool -L liblite.ai.toolkit.0.0.1.dylib 
liblite.ai.toolkit.0.0.1.dylib:
        @rpath/liblite.ai.toolkit.0.0.1.dylib (compatibility version 0.0.1, current version 0.0.1)
        @rpath/libopencv_highgui.4.5.dylib (compatibility version 4.5.0, current version 4.5.2)
        @rpath/libonnxruntime.1.7.0.dylib (compatibility version 0.0.0, current version 1.7.0)
        ...
```


```shell
cd ../ && tree .
â”œâ”€â”€ bin
â”œâ”€â”€ include
â”‚Â Â  â”œâ”€â”€ lite
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ backend.h
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ config.h
â”‚Â Â  â”‚Â Â  â””â”€â”€ lite.h
â”‚Â Â  â””â”€â”€ ort
â””â”€â”€ lib
    â””â”€â”€ liblite.ai.toolkit.0.0.1.dylib
```
* è¿è¡Œå·²ç»ç¼–è¯‘å¥½çš„examples:
```shell
cd ./build/lite.ai.toolkit/bin && ls -lh | grep lite
-rwxr-xr-x  1 root  staff   301K Jun 26 23:10 liblite.ai.toolkit.0.0.1.dylib
...
-rwxr-xr-x  1 root  staff   196K Jun 26 23:10 lite_yolov4
-rwxr-xr-x  1 root  staff   196K Jun 26 23:10 lite_yolov5
...
```

```shell
./lite_yolov5
LITEORT_DEBUG LogId: ../../../hub/onnx/cv/yolov5s.onnx
=============== Input-Dims ==============
...
detected num_anchors: 25200
generate_bboxes num: 66
Default Version Detected Boxes Num: 5
```

* ä¸ºäº†é“¾æ¥`lite.ai.toolkit`åŠ¨æ€åº“ï¼Œä½ éœ€è¦ç¡®ä¿`OpenCV` and `onnxruntime`ä¹Ÿè¢«æ­£ç¡®åœ°é“¾æ¥ã€‚æ¯”å¦‚:

```cmake
cmake_minimum_required(VERSION 3.17)
project(testlite.ai.toolkit)
set(CMAKE_CXX_STANDARD 11)
set(CMAKE_BUILD_TYPE debug)
# link opencv.
set(OpenCV_DIR ${CMAKE_SOURCE_DIR}/opencv/lib/cmake/opencv4)
find_package(OpenCV 4 REQUIRED)
include_directories(${OpenCV_INCLUDE_DIRS})
# link onnxruntime.
set(ONNXRUNTIME_DIR ${CMAKE_SOURCE_DIR}/onnxruntime/)
set(ONNXRUNTIME_INCLUDE_DIR ${ONNXRUNTIME_DIR}/include)
set(ONNXRUNTIME_LIBRARY_DIR ${ONNXRUNTIME_DIR}/lib)
include_directories(${ONNXRUNTIME_INCLUDE_DIR})
link_directories(${ONNXRUNTIME_LIBRARY_DIR})
# link lite.ai.toolkit.
set(LITEHUB_DIR ${CMAKE_SOURCE_DIR}/lite.ai.toolkit)
set(LITEHUB_INCLUDE_DIR ${LITEHUB_DIR}/include)
set(LITEHUB_LIBRARY_DIR ${LITEHUB_DIR}/lib)
include_directories(${LITEHUB_INCLUDE_DIR})
link_directories(${LITEHUB_LIBRARY_DIR})
# add your executable
add_executable(lite_yolov5 test_lite_yolov5.cpp)
target_link_libraries(lite_yolov5 lite.ai.toolkit onnxruntime ${OpenCV_LIBS})
```
ä½ å¯ä»¥åœ¨[lite.ai.toolkit.demo](https://github.com/DefTruth/lite.ai.toolkit.demo) ä¸­æ‰¾åˆ°ä¸€ä¸ªç®€å•ä¸”å®Œæ•´çš„ï¼Œå…³äºå¦‚ä½•æ­£ç¡®åœ°é“¾æ¥Lite.AI.ToolKitåŠ¨æ€åº“çš„åº”ç”¨æ¡ˆä¾‹ã€‚

</details>


## 2. æ¨¡å‹ä¸‹è½½  

<div id="lite.ai.toolkit-Model-Zoo"></div>

*Lite.AI.ToolKit* ç›®å‰åŒ…æ‹¬ *[70+](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md)* æµè¡Œçš„å¼€æºæ¨¡å‹ä»¥åŠ *[150+](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md)* .onnx æ–‡ä»¶ï¼Œå¤§éƒ¨åˆ†onnxæ–‡ä»¶æ˜¯æˆ‘è‡ªå·±è½¬æ¢çš„ã€‚ä½ å¯ä»¥é€šè¿‡*lite::cv::Type::Class* è¯­æ³•è¿›è¡Œè°ƒç”¨ï¼Œå¦‚ *[lite::cv::detection::YoloV5](#lite.ai.toolkit-object-detection)*ã€‚æ›´å¤šçš„ç»†èŠ‚è§[Examples for Lite.AI.ToolKit](#lite.ai.toolkit-Examples-for-Lite.AI.ToolKit)ã€‚

<details>
<summary> å‘½åç©ºé—´å’ŒLite.AI.ToolKitç®—æ³•æ¨¡å—çš„å¯¹åº”å…³ç³» </summary>  

### å‘½åç©ºé—´å’ŒLite.AI.ToolKitç®—æ³•æ¨¡å—çš„å¯¹åº”å…³ç³»  

| Namepace                   | Details                                                      |
| :------------------------- | :----------------------------------------------------------- |
| *lite::cv::detection*      | Object Detection. one-stage and anchor-free detectors, YoloV5, YoloV4, SSD, etc. âœ… |
| *lite::cv::classification* | Image Classification. DensNet, ShuffleNet, ResNet, IBNNet, GhostNet, etc. âœ… |
| *lite::cv::faceid*         | Face Recognition. ArcFace, CosFace, CurricularFace, etc. â‡ï¸   |
| *lite::cv::face*           | Face Analysis. *detect*, *align*, *pose*, *attr*, etc. â‡ï¸    |
| *lite::cv::face::detect*   | Face Detection. UltraFace, RetinaFace, FaceBoxes, PyramidBox, etc. â‡ï¸ |
| *lite::cv::face::align*    | Face Alignment. PFLD(106), FaceLandmark1000(1000 landmarks), PRNet, etc. â‡ï¸ |
| *lite::cv::face::pose*     | Head Pose Estimation.  FSANet, etc. â‡ï¸                        |
| *lite::cv::face::attr*     | Face Attributes. Emotion, Age, Gender. EmotionFerPlus, VGG16Age, etc. â‡ï¸ |
| *lite::cv::segmentation*   | Object Segmentation. Such as FCN, DeepLabV3, etc. âš ï¸          |
| *lite::cv::style*          | Style Transfer. Contains neural style transfer now, such as FastStyleTransfer.  âš ï¸ |
| *lite::cv::matting*        | Image Matting. Object and Human matting.  âš ï¸                  |
| *lite::cv::colorization*   | Colorization. Make Gray image become RGB. âš ï¸                  |
| *lite::cv::resolution*     | Super Resolution.  âš ï¸                                         |


### Lite.AI.ToolKitçš„ç±»ä¸æƒé‡æ–‡ä»¶å¯¹åº”å…³ç³»è¯´æ˜

Lite.AI.ToolKitçš„ç±»ä¸æƒé‡æ–‡ä»¶å¯¹åº”å…³ç³»è¯´æ˜ï¼Œå¯ä»¥åœ¨[lite.ai.toolkit.hub.onnx.md](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md) ä¸­æ‰¾åˆ°ã€‚æ¯”å¦‚, *lite::cv::detection::YoloV5* å’Œ *lite::cv::detection::YoloX* çš„æƒé‡æ–‡ä»¶ä¸ºï¼š


|             Class             | Pretrained ONNX Files |                   Rename or Converted From (Repo)                   | Size  |
| :---------------------------: | :-------------------: | :----------------------------------------------------: | :---: |
| *lite::cv::detection::YoloV5* |     yolov5l.onnx      | [yolov5](https://github.com/ultralytics/yolov5) (ğŸ”¥ğŸ”¥ğŸ’¥â†‘) | 188Mb |
| *lite::cv::detection::YoloV5* |     yolov5m.onnx      | [yolov5](https://github.com/ultralytics/yolov5) (ğŸ”¥ğŸ”¥ğŸ’¥â†‘) | 85Mb  |
| *lite::cv::detection::YoloV5* |     yolov5s.onnx      | [yolov5](https://github.com/ultralytics/yolov5) (ğŸ”¥ğŸ”¥ğŸ’¥â†‘) | 29Mb  |
| *lite::cv::detection::YoloV5* |     yolov5x.onnx      | [yolov5](https://github.com/ultralytics/yolov5) (ğŸ”¥ğŸ”¥ğŸ’¥â†‘) | 351Mb |
| *lite::cv::detection::YoloX* |     yolox_x.onnx      | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) (ğŸ”¥ğŸ”¥!!â†‘) | 378Mb |
| *lite::cv::detection::YoloX* |     yolox_l.onnx      | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) (ğŸ”¥ğŸ”¥!!â†‘) | 207Mb  |
| *lite::cv::detection::YoloX* |     yolox_m.onnx      | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) (ğŸ”¥ğŸ”¥!!â†‘) | 97Mb  |
| *lite::cv::detection::YoloX* |     yolox_s.onnx      | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) (ğŸ”¥ğŸ”¥!!â†‘) | 34Mb |
| *lite::cv::detection::YoloX* |     yolox_tiny.onnx      | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) (ğŸ”¥ğŸ”¥!!â†‘) | 19Mb |
| *lite::cv::detection::YoloX* |     yolox_nano.onnx      | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) (ğŸ”¥ğŸ”¥!!â†‘) | 3.5Mb |

è¿™æ„å‘³ç€ï¼Œä½ å¯ä»¥é€šè¿‡Lite.AI.ToolKitä¸­çš„åŒä¸€ä¸ªç±»ï¼Œæ ¹æ®ä½ çš„ä½¿ç”¨æƒ…å†µï¼ŒåŠ è½½ä»»æ„ä¸€ä¸ª`yolov5*.onnx`æˆ–`yolox_*.onnx`ï¼Œå¦‚ *YoloV5*, *YoloX*ç­‰.

```c++
auto *yolov5 = new lite::cv::detection::YoloV5("yolov5x.onnx");  // for server
auto *yolov5 = new lite::cv::detection::YoloV5("yolov5l.onnx"); 
auto *yolov5 = new lite::cv::detection::YoloV5("yolov5m.onnx");  
auto *yolov5 = new lite::cv::detection::YoloV5("yolov5s.onnx");  // for mobile device 
auto *yolox = new lite::cv::detection::YoloX("yolox_x.onnx");  
auto *yolox = new lite::cv::detection::YoloX("yolox_l.onnx");  
auto *yolox = new lite::cv::detection::YoloX("yolox_m.onnx");  
auto *yolox = new lite::cv::detection::YoloX("yolox_s.onnx");  
auto *yolox = new lite::cv::detection::YoloX("yolox_tiny.onnx");  
auto *yolox = new lite::cv::detection::YoloX("yolox_nano.onnx");  // 3.5Mb only !
```

</details>

* ä¸‹è½½é“¾æ¥:  
  [Baidu Drive](https://pan.baidu.com/s/1elUGcx7CZkkjEoYhTMwTRQ) code: 8gin && [Google Drive](https://drive.google.com/drive/folders/1p6uBcxGeyS1exc-T61vL8YRhwjYL4iD2?usp=sharing) .   
  æ³¨æ„ï¼Œç”±äºGoogle Driver(15G)çš„å­˜å‚¨é™åˆ¶ï¼Œæˆ‘æ— æ³•ä¸Šä¼ æ‰€æœ‰çš„æ¨¡å‹æ–‡ä»¶ï¼Œå›½å†…çš„å°ä¼™ä¼´è¯·ä½¿ç”¨ç™¾åº¦äº‘ç›˜ã€‚<div id="lite.ai.toolkit-2"></div>


* ç›®æ ‡æ£€æµ‹

|Class|Size|From|Awesome|File|Type|State|Usage|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|[YoloV5](https://github.com/ultralytics/yolov5)|28M|[yolov5](https://github.com/ultralytics/yolov5)|ğŸ”¥ğŸ”¥ğŸ’¥â†‘| [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-object-detection)  | *detection* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolov5.cpp) |
|[YoloV3](https://github.com/onnx/models/blob/master/vision/object_detection_segmentation/yolov3)|236M|[onnx-models](https://github.com/onnx/models)|ğŸ”¥ğŸ”¥ğŸ”¥â†‘| [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-object-detection) | *detection* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolov3.cpp) |
|[TinyYoloV3](https://github.com/onnx/models/blob/master/vision/object_detection_segmentation/tiny-yolov3)|33M|        [onnx-models](https://github.com/onnx/models)         | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-object-detection) | *detection* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_tiny_yolov3.cpp) |
|[YoloV4](https://github.com/argusswift/YOLOv4-pytorch)|176M| [YOLOv4...](https://github.com/argusswift/YOLOv4-pytorch) | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-object-detection) | *detection* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolov4.cpp) |
|[SSD](https://github.com/onnx/models/blob/master/vision/object_detection_segmentation/ssd)|76M|        [onnx-models](https://github.com/onnx/models)         | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-object-detection) | *detection* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_ssd.cpp) |
|[SSDMobileNetV1](https://github.com/onnx/models/blob/master/vision/object_detection_segmentation/ssd-mobilenetv1)|27M|        [onnx-models](https://github.com/onnx/models)         | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-object-detection) | *detection* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_ssd_mobilenetv1.cpp) |
|[YoloX](https://github.com/Megvii-BaseDetection/YOLOX)|3.5M| [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) | ğŸ”¥ğŸ”¥newâ†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-object-detection) | *detection* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolox.cpp) |
|[TinyYoloV4VOC](https://github.com/bubbliiiing/yolov4-tiny-pytorch)|22M| [yolov4-tiny...](https://github.com/bubbliiiing/yolov4-tiny-pytorch) | ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-object-detection) | *detection* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_tiny_yolov4_voc.cpp) |
|[TinyYoloV4COCO](https://github.com/bubbliiiing/yolov4-tiny-pytorch)|22M| [yolov4-tiny...](https://github.com/bubbliiiing/yolov4-tiny-pytorch) | ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-object-detection) | *detection* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_tiny_yolov4_coco.cpp) |
|[YoloR](https://github.com/WongKinYiu/yolor)|39M| [yolor](https://github.com/WongKinYiu/yolor) | ğŸ”¥ğŸ”¥newâ†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-object-detection) | *detection* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolor.cpp) |
|[ScaledYoloV4](https://github.com/WongKinYiu/ScaledYOLOv4)|270M| [ScaledYOLOv4](https://github.com/WongKinYiu/ScaledYOLOv4) | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-object-detection) | *detection* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_scaled_yolov4.cpp) |
|[EfficientDet](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch)|15M| [...EfficientDet...](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch) | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-object-detection) | *detection* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_efficientdet.cpp) |
|[EfficientDetD7](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch)|220M| [...EfficientDet...](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch) | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-object-detection) | *detection* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_efficientdet_d7.cpp) |
|[EfficientDetD8](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch)|322M| [...EfficientDet...](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch) | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-object-detection) | *detection* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_efficientdet_d8.cpp) |
|[YOLOP](https://github.com/hustvl/YOLOP)|30M| [YOLOP](https://github.com/hustvl/YOLOP) | ğŸ”¥ğŸ”¥newâ†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-object-detection) | *detection* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolop.cpp) |


* äººè„¸è¯†åˆ«

|Class|Size|From|Awesome|File|Type|State|Usage|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|[GlintArcFace](https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch)|92M|  [insightface](https://github.com/deepinsight/insightface)   | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-recognition) | *faceid* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_glint_arcface.cpp) |
|[GlintCosFace](https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch)|92M|  [insightface](https://github.com/deepinsight/insightface)   | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-recognition) | *faceid* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_glint_cosface.cpp) |
|[GlintPartialFC](https://github.com/deepinsight/insightface/tree/master/recognition/partial_fc)|170M|  [insightface](https://github.com/deepinsight/insightface)   | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-recognition) | *faceid* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_glint_partial_fc.cpp) |
|[FaceNet](https://github.com/timesler/facenet-pytorch)|89M| [facenet...](https://github.com/timesler/facenet-pytorch) | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-recognition) | *faceid* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_facenet.cpp) |
|[FocalArcFace](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch)|166M| [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch) | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-recognition) | *faceid* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_focal_arcface.cpp) |
|[FocalAsiaArcFace](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch)|166M| [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch) | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-recognition) | *faceid* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_focal_asia_arcface.cpp) |
|[TencentCurricularFace](https://github.com/Tencent/TFace/tree/master/tasks/distfc)|249M|          [TFace](https://github.com/Tencent/TFace)           |  ğŸ”¥ğŸ”¥â†‘  | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-recognition) | *faceid* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_tencent_curricular_face.cpp) |
|[TencentCifpFace](https://github.com/Tencent/TFace/tree/master/tasks/cifp)|130M|          [TFace](https://github.com/Tencent/TFace)           |  ğŸ”¥ğŸ”¥â†‘  | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-recognition) | *faceid* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_tencent_cifp_face.cpp) |
|[CenterLossFace](https://github.com/louis-she/center-loss.pytorch)| 280M |  [center-loss...](https://github.com/louis-she/center-loss.pytorch)           |  ğŸ”¥ğŸ”¥â†‘  | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-recognition) | *faceid* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_center_loss_face.cpp) |
|[SphereFace](https://github.com/clcarwin/sphereface_pytorch)| 80M |  [sphere...](https://github.com/clcarwin/sphereface_pytorch)   |  ğŸ”¥ğŸ”¥â†‘  | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-recognition) | *faceid* | âœ…ï¸ | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_sphere_face.cpp) |
|[PoseRobustFace](https://github.com/penincillin/DREAM)| 92M | [DREAM](https://github.com/penincillin/DREAM)  |  ğŸ”¥ğŸ”¥â†‘  | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-recognition) | *faceid* | âœ…ï¸ | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_pose_robust_face.cpp) |
|[NaivePoseRobustFace](https://github.com/penincillin/DREAM)| 43M | [DREAM](https://github.com/penincillin/DREAM)  |  ğŸ”¥ğŸ”¥â†‘  | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-recognition) | *faceid* | âœ…ï¸ | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_naive_pose_robust_face.cpp) |
|[MobileFaceNet](https://github.com/Xiaoccer/MobileFaceNet_Pytorch)| 3.8M |  [MobileFace...](https://github.com/Xiaoccer/MobileFaceNet_Pytorch)           |  ğŸ”¥ğŸ”¥â†‘  | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-recognition) | *faceid* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_mobile_facenet.cpp) |
|[CavaGhostArcFace](https://github.com/cavalleria/cavaface.pytorch)| 15M | [cavaface...](https://github.com/cavalleria/cavaface.pytorch) |  ğŸ”¥ğŸ”¥â†‘  | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-recognition) | *faceid* |  âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_cava_ghost_arcface.cpp) |
|[CavaCombinedFace](https://github.com/cavalleria/cavaface.pytorch)| 250M | [cavaface...](https://github.com/cavalleria/cavaface.pytorch) |  ğŸ”¥ğŸ”¥â†‘  | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-recognition) | *faceid* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_cava_combined_face.cpp) |
|[MobileSEFocalFace](https://github.com/grib0ed0v/face_recognition.pytorch)|4.5M| [face_recog...](https://github.com/grib0ed0v/face_recognition.pytorch) | ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-recognition) | *faceid* | âœ… |  [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_mobilese_focal_face.cpp) |

* æŠ å›¾

|Class|Size|From|Awesome|File|Type|State|Usage|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|[RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting)|14M| [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting)  |   ğŸ”¥ğŸ”¥ğŸ”¥latestâ†‘   | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-matting) | *matting* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_rvm.cpp) |


<details>
<summary> âš ï¸ å±•å¼€Lite.AI.ToolKitçš„Model Zooä¸­æ›´å¤šçš„æ¨¡å‹ </summary>  

* äººè„¸æ£€æµ‹

|Class|Size|From|Awesome|File|Type|State|Usage|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|[UltraFace](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB)|1.1M| [Ultra-Light...](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-detection) | *face::detect* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_ultraface.cpp) |
|[RetinaFace](https://github.com/biubug6/Pytorch_Retinaface)|1.6M| [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface) | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-detection) | *face::detect* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_retinaface.cpp) |
|[FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)|3.8M| [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch) | ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-detection) | *face::detect* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_faceboxes.cpp) |


* äººè„¸å¯¹é½

|Class|Size|From|Awesome|File|Type|State|Usage|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|[PFLD](https://github.com/Hsintao/pfld_106_face_landmarks)|1.0M| [pfld_106_...](https://github.com/Hsintao/pfld_106_face_landmarks) |  ğŸ”¥ğŸ”¥â†‘  | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-alignment) | *face::align* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_pfld.cpp) |
|[PFLD98](https://github.com/polarisZhao/PFLD-pytorch)|4.8M| [PFLD...](https://github.com/polarisZhao/PFLD-pytorch) | ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-alignment) | *face::align* | âœ…ï¸ | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_pfld98.cpp) |
|[MobileNetV268](https://github.com/cunjian/pytorch_face_landmark)|9.4M| [...landmark](https://github.com/cunjian/pytorch_face_landmark) | ğŸ”¥ğŸ”¥â†‘ |  [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-alignment) | *face::align* | âœ…ï¸ï¸ | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_mobilenetv2_68.cpp) |
|[MobileNetV2SE68](https://github.com/cunjian/pytorch_face_landmark)|11M| [...landmark](https://github.com/cunjian/pytorch_face_landmark) | ğŸ”¥ğŸ”¥â†‘ |  [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-alignment) | *face::align* | âœ…ï¸ï¸ | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_mobilenetv2_se_68.cpp) |
|[PFLD68](https://github.com/cunjian/pytorch_face_landmark)|2.8M| [...landmark](https://github.com/cunjian/pytorch_face_landmark) | ğŸ”¥ğŸ”¥â†‘ |  [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-alignment) | *face::align* | âœ…ï¸ | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_pfld68.cpp) |
|[FaceLandmark1000](https://github.com/Single430/FaceLandmark1000)|2.0M| [FaceLandm...](https://github.com/Single430/FaceLandmark1000) | ğŸ”¥â†‘ |  [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-alignment) | *face::align* | âœ…ï¸ | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_face_landmarks_1000.cpp) |



* å¤´éƒ¨å§¿æ€ä¼°è®¡

|Class|Size|From|Awesome|File|Type|State|Usage|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|[FSANet](https://github.com/omasaht/headpose-fsanet-pytorch)|1.2M| [...fsanet...](https://github.com/omasaht/headpose-fsanet-pytorch) | ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-head-pose-estimation) | *face::pose* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_fsanet.cpp) |


* äººè„¸å±æ€§è¯†åˆ«

|Class|Size|From|Awesome|File|Type|State|Usage|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|[AgeGoogleNet](https://github.com/onnx/models/tree/master/vision/body_analysis/age_gender)|23M|        [onnx-models](https://github.com/onnx/models)         | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-attributes) | *face::attr* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_age_googlenet.cpp) |
|[GenderGoogleNet](https://github.com/onnx/models/tree/master/vision/body_analysis/age_gender)|23M|        [onnx-models](https://github.com/onnx/models)         | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-attributes) | *face::attr* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_gender_googlenet.cpp) |
|[EmotionFerPlus](https://github.com/onnx/models/blob/master/vision/body_analysis/emotion_ferplus)|33M|        [onnx-models](https://github.com/onnx/models)         | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-attributes) | *face::attr* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_emotion_ferplus.cpp) |
|[VGG16Age](https://github.com/onnx/models/tree/master/vision/body_analysis/age_gender)|514M|        [onnx-models](https://github.com/onnx/models)         | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-attributes) | *face::attr* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_vgg16_age.cpp) |
|[VGG16Gender](https://github.com/onnx/models/tree/master/vision/body_analysis/age_gender)|512M|        [onnx-models](https://github.com/onnx/models)         | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-attributes) | *face::attr* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_vgg16_gender.cpp) |
|[SSRNet](https://github.com/oukohou/SSR_Net_Pytorch)|190K| [SSR_Net...](https://github.com/oukohou/SSR_Net_Pytorch) | ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-attributes) | *face::attr* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_ssrnet.cpp) |
|[EfficientEmotion7](https://github.com/HSE-asavchenko/face-emotion-recognition)|15M| [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-attributes) | *face::attr* | âœ…ï¸ | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_efficient_emotion7.cpp) |
|[EfficientEmotion8](https://github.com/HSE-asavchenko/face-emotion-recognition)|15M| [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-attributes) | *face::attr* | âœ…  | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_efficient_emotion8.cpp) |
|[MobileEmotion7](https://github.com/HSE-asavchenko/face-emotion-recognition)|13M| [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-attributes) | *face::attr* |  âœ…  | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_mobile_emotion7.cpp) |
|[ReXNetEmotion7](https://github.com/HSE-asavchenko/face-emotion-recognition)|30M| [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-face-attributes) | *face::attr* |  âœ…  | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_rexnet_emotion7.cpp) |


* å›¾åƒåˆ†ç±»  

|Class|Size|From|Awesome|File|Type|State|Usage|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|[EfficientNetLite4](https://github.com/onnx/models/blob/master/vision/classification/efficientnet-lite4)|49M|        [onnx-models](https://github.com/onnx/models)         | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-classification)  | *classification* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_efficientnet_lite4.cpp) |
|[ShuffleNetV2](https://github.com/onnx/models/blob/master/vision/classification/shufflenet)|8.7M|        [onnx-models](https://github.com/onnx/models)         | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-classification)  | *classification* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_shufflenetv2.cpp) |
|[DenseNet121](https://pytorch.org/hub/pytorch_vision_densenet/)|30.7M|       [torchvision](https://github.com/pytorch/vision)       | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-classification)  | *classification* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_densenet.cpp) |
|[GhostNet](https://pytorch.org/hub/pytorch_vision_ghostnet/)|20M|       [torchvision](https://github.com/pytorch/vision)       | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-classification)  | *classification* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_ghostnet.cpp) |
|[HdrDNet](https://pytorch.org/hub/pytorch_vision_hardnet//)|13M|       [torchvision](https://github.com/pytorch/vision)       | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-classification) | *classification* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_hardnet.cpp) |
|[IBNNet](https://pytorch.org/hub/pytorch_vision_ibnnet/)|97M|       [torchvision](https://github.com/pytorch/vision)       | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-classification)  | *classification* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_ibnnet.cpp) |
|[MobileNetV2](https://pytorch.org/hub/pytorch_vision_mobilenet_v2/)|13M|       [torchvision](https://github.com/pytorch/vision)       | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-classification)  | *classification* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_mobilenetv2.cpp) |
|[ResNet](https://pytorch.org/hub/pytorch_vision_resnet/)|44M|       [torchvision](https://github.com/pytorch/vision)       | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-classification)  | *classification* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_resnet.cpp) |
|[ResNeXt](https://pytorch.org/hub/pytorch_vision_resnext/)|95M|       [torchvision](https://github.com/pytorch/vision)       | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-classification)  | *classification* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_resnext.cpp) |


* è¯­ä¹‰åˆ†å‰²

|Class|Size|From|Awesome|File|Type|State|Usage|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|[DeepLabV3ResNet101](https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/)|232M|       [torchvision](https://github.com/pytorch/vision)       | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-segmentation) | *segmentation* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_deeplabv3_resnet101.cpp) |
|[FCNResNet101](https://pytorch.org/hub/pytorch_vision_fcn_resnet101/)|207M|       [torchvision](https://github.com/pytorch/vision)       | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-segmentation) | *segmentation* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_fcn_resnet101.cpp) |


* é£æ ¼è¿ç§»

|Class|Size|From|Awesome|File|Type|State|Usage|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|[FastStyleTransfer](https://github.com/onnx/models/blob/master/vision/style_transfer/fast_neural_style)|6.4M|        [onnx-models](https://github.com/onnx/models)         | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-style-transfer) | *style* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_fast_style_transfer.cpp) |


* å›¾ç‰‡ç€è‰²

|Class|Size|From|Awesome|File|Type|State|Usage|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|[Colorizer](https://github.com/richzhang/colorization)|123M|  [colorization](https://github.com/richzhang/colorization)   | ğŸ”¥ğŸ”¥ğŸ”¥â†‘ | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-colorization) | *colorization* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_colorizer.cpp) |


* è¶…åˆ†è¾¨ç‡

|Class|Size|From|Awesome|File|Type|State|Usage|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|[SubPixelCNN](https://github.com/niazwazir/SUB_PIXEL_CNN)|234K| [...PIXEL...](https://github.com/niazwazir/SUB_PIXEL_CNN)  |    ğŸ”¥â†‘    | [![](https://img.shields.io/badge/onnx-done-brightgreen.svg)](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md#lite.ai.toolkit.hub.onnx-super-resolution) | *resolution* | âœ… | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_subpixel_cnn.cpp) |

</details>

## 3. åº”ç”¨æ¡ˆä¾‹.

<div id="lite.ai.toolkit-Examples-for-Lite.AI.ToolKit"></div>

æ›´å¤šçš„åº”ç”¨æ¡ˆä¾‹è¯¦è§[lite.ai.toolkit.examples](https://github.com/DefTruth/lite.ai.toolkit/tree/main/examples/lite/cv) ã€‚ç‚¹å‡» â–¶ï¸ å¯ä»¥çœ‹åˆ°è¯¥ä¸»é¢˜ä¸‹æ›´å¤šçš„æ¡ˆä¾‹ã€‚

<div id="lite.ai.toolkit-object-detection"></div>

#### æ¡ˆä¾‹0: ä½¿ç”¨[YoloV5](https://github.com/ultralytics/yolov5) è¿›è¡Œç›®æ ‡æ£€æµ‹ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/yolov5s.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_yolov5_1.jpg";
  std::string save_img_path = "../../../logs/test_lite_yolov5_1.jpg";

  auto *yolov5 = new lite::cv::detection::YoloV5(onnx_path); 
  std::vector<lite::cv::types::Boxf> detected_boxes;
  cv::Mat img_bgr = cv::imread(test_img_path);
  yolov5->detect(img_bgr, detected_boxes);
  
  lite::cv::utils::draw_boxes_inplace(img_bgr, detected_boxes);
  cv::imwrite(save_img_path, img_bgr);  
  
  delete yolov5;
}
```

è¾“å‡ºçš„ç»“æœæ˜¯:
<div align='center'>
  <img src='logs/test_lite_yolov5_1.jpg' height="256px">
  <img src='logs/test_lite_yolov5_2.jpg' height="256px">
</div>

æˆ–è€…ä½ å¯ä»¥ä½¿ç”¨æœ€æ–°çš„ ğŸ”¥ğŸ”¥ ! YOLO ç³»åˆ—æ£€æµ‹å™¨[YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) æˆ– [YoloR](https://github.com/WongKinYiu/yolor) ï¼Œå®ƒä»¬ä¼šè·å¾—æ¥è¿‘çš„ç»“æœã€‚

****

<div id="lite.ai.toolkit-matting"></div>  

#### æ¡ˆä¾‹1: ä½¿ç”¨[RobustVideoMatting2021ğŸ”¥ğŸ”¥ğŸ”¥](https://github.com/PeterL1n/RobustVideoMatting) è¿›è¡Œè§†é¢‘æŠ å›¾ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚

```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/rvm_mobilenetv3_fp32.onnx";
  std::string video_path = "../../../examples/lite/resources/test_lite_rvm_0.mp4";
  std::string output_path = "../../../logs/test_lite_rvm_0.mp4";
  
  auto *rvm = new lite::cv::matting::RobustVideoMatting(onnx_path, 16); // 16 threads
  std::vector<lite::cv::types::MattingContent> contents;
  
  // 1. video matting.
  rvm->detect_video(video_path, output_path, contents, false, 0.4f);
  
  delete rvm;
}
```
è¾“å‡ºçš„ç»“æœæ˜¯:

<div align='center'>
  <img src='docs/resources/interviewi.gif' height="200px" width="200px">
  <img src='docs/resources/interview.gif' height="200px" width="200px">  
  <img src='docs/resources/dance3i.gif' height="200px" width="200px">
  <img src='docs/resources/dance3.gif' height="200px" width="200px">
  <br>
  <img src='docs/resources/teslai.gif' height="200px" width="200px">
  <img src='docs/resources/tesla.gif' height="200px" width="200px">  
  <img src='docs/resources/b5i.gif' height="200px" width="200px">
  <img src='docs/resources/b5.gif' height="200px" width="200px">
</div>


****

<div id="lite.ai.toolkit-face-alignment"></div>

#### æ¡ˆä¾‹2: ä½¿ç”¨[FaceLandmarks1000](https://github.com/Single430/FaceLandmark1000) è¿›è¡Œäººè„¸1000å…³é”®ç‚¹æ£€æµ‹ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/FaceLandmark1000.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_face_landmarks_0.png";
  std::string save_img_path = "../../../logs/test_lite_face_landmarks_1000.jpg";
    
  auto *face_landmarks_1000 = new lite::cv::face::align::FaceLandmark1000(onnx_path);

  lite::cv::types::Landmarks landmarks;
  cv::Mat img_bgr = cv::imread(test_img_path);
  face_landmarks_1000->detect(img_bgr, landmarks);
  lite::cv::utils::draw_landmarks_inplace(img_bgr, landmarks);
  cv::imwrite(save_img_path, img_bgr);
  
  delete face_landmarks_1000;
}
```
è¾“å‡ºçš„ç»“æœæ˜¯:
<div align='center'>
  <img src='logs/test_lite_face_landmarks_1000.jpg' height="224px" width="224px">
  <img src='logs/test_lite_face_landmarks_1000_2.jpg' height="224px" width="224px">
  <img src='logs/test_lite_face_landmarks_1000_0.jpg' height="224px" width="224px">
</div>    

****  

<div id="lite.ai.toolkit-colorization"></div>

#### æ¡ˆä¾‹3: ä½¿ç”¨[colorization](https://github.com/richzhang/colorization) è¿›è¡Œå›¾åƒç€è‰²ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/eccv16-colorizer.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_colorizer_1.jpg";
  std::string save_img_path = "../../../logs/test_lite_eccv16_colorizer_1.jpg";
  
  auto *colorizer = new lite::cv::colorization::Colorizer(onnx_path);
  
  cv::Mat img_bgr = cv::imread(test_img_path);
  lite::cv::types::ColorizeContent colorize_content;
  colorizer->detect(img_bgr, colorize_content);
  
  if (colorize_content.flag) cv::imwrite(save_img_path, colorize_content.mat);
  delete colorizer;
}
```
è¾“å‡ºçš„ç»“æœæ˜¯:

<div align='center'>
  <img src='examples/lite/resources/test_lite_colorizer_1.jpg' height="224px" width="224px">
  <img src='examples/lite/resources/test_lite_colorizer_2.jpg' height="224px" width="224px">
  <img src='examples/lite/resources/test_lite_colorizer_3.jpg' height="224px" width="224px">  
  <br> 
  <img src='logs/test_lite_siggraph17_colorizer_1.jpg' height="224px" width="224px">
  <img src='logs/test_lite_siggraph17_colorizer_2.jpg' height="224px" width="224px">
  <img src='logs/test_lite_siggraph17_colorizer_3.jpg' height="224px" width="224px">
</div>    

****

<div id="lite.ai.toolkit-face-recognition"></div>  

#### æ¡ˆä¾‹4: ä½¿ç”¨[ArcFace](https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch) è¿›è¡Œäººè„¸è¯†åˆ«ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚

```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/ms1mv3_arcface_r100.onnx";
  std::string test_img_path0 = "../../../examples/lite/resources/test_lite_faceid_0.png";
  std::string test_img_path1 = "../../../examples/lite/resources/test_lite_faceid_1.png";
  std::string test_img_path2 = "../../../examples/lite/resources/test_lite_faceid_2.png";

  auto *glint_arcface = new lite::cv::faceid::GlintArcFace(onnx_path);

  lite::cv::types::FaceContent face_content0, face_content1, face_content2;
  cv::Mat img_bgr0 = cv::imread(test_img_path0);
  cv::Mat img_bgr1 = cv::imread(test_img_path1);
  cv::Mat img_bgr2 = cv::imread(test_img_path2);
  glint_arcface->detect(img_bgr0, face_content0);
  glint_arcface->detect(img_bgr1, face_content1);
  glint_arcface->detect(img_bgr2, face_content2);

  if (face_content0.flag && face_content1.flag && face_content2.flag)
  {
    float sim01 = lite::cv::utils::math::cosine_similarity<float>(
        face_content0.embedding, face_content1.embedding);
    float sim02 = lite::cv::utils::math::cosine_similarity<float>(
        face_content0.embedding, face_content2.embedding);
    std::cout << "Detected Sim01: " << sim  << " Sim02: " << sim02 << std::endl;
  }

  delete glint_arcface;
}
```

è¾“å‡ºçš„ç»“æœæ˜¯:
<div align='center'>
  <img src='examples/lite/resources/test_lite_arcface_resnet_0.png' height="224px" width="224px">
  <img src='examples/lite/resources/test_lite_arcface_resnet_1.png' height="224px" width="224px">
  <img src='examples/lite/resources/test_lite_arcface_resnet_2.png' height="224px" width="224px">
</div>  

> Detected Sim01: 0.721159  Sim02: -0.0626267

****

<div id="lite.ai.toolkit-face-detection"></div>

#### æ¡ˆä¾‹5: ä½¿ç”¨[UltraFace](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) è¿›è¡Œäººè„¸æ£€æµ‹ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/ultraface-rfb-640.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_ultraface.jpg";
  std::string save_img_path = "../../../logs/test_lite_ultraface.jpg";

  auto *ultraface = new lite::cv::face::detect::UltraFace(onnx_path);

  std::vector<lite::cv::types::Boxf> detected_boxes;
  cv::Mat img_bgr = cv::imread(test_img_path);
  ultraface->detect(img_bgr, detected_boxes);
  lite::cv::utils::draw_boxes_inplace(img_bgr, detected_boxes);
  cv::imwrite(save_img_path, img_bgr);

  delete ultraface;
}
```
è¾“å‡ºçš„ç»“æœæ˜¯:
<div align='center'>
  <img src='logs/test_lite_ultraface.jpg' height="224px" width="224px">
  <img src='logs/test_lite_ultraface_2.jpg' height="224px" width="224px">
  <img src='logs/test_lite_ultraface_3.jpg' height="224px" width="224px">
</div>  


<div id="lite.ai.toolkit-segmentation"></div>  
<div id="lite.ai.toolkit-face-attributes-analysis"></div>  
<div id="lite.ai.toolkit-image-classification"></div>  
<div id="lite.ai.toolkit-head-pose-estimation"></div>
<div id="lite.ai.toolkit-style-transfer"></div>

<details>
<summary> âš ï¸ å±•å¼€Lite.AI.ToolKitæ‰€æœ‰ç®—æ³•æ¨¡å—çš„åº”ç”¨æ¡ˆä¾‹ </summary>  

<details>
<summary> 3.1 ç›®æ ‡æ£€æµ‹åº”ç”¨æ¡ˆä¾‹ </summary>

#### 3.1 ä½¿ç”¨[YoloV5](https://github.com/ultralytics/yolov5) è¿›è¡Œç›®æ ‡æ£€æµ‹ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/yolov5s.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_yolov5_1.jpg";
  std::string save_img_path = "../../../logs/test_lite_yolov5_1.jpg";
  
  auto *yolov5 = new lite::cv::detection::YoloV5(onnx_path);
  std::vector<lite::cv::types::Boxf> detected_boxes;
  cv::Mat img_bgr = cv::imread(test_img_path);
  yolov5->detect(img_bgr, detected_boxes);
  
  lite::cv::utils::draw_boxes_inplace(img_bgr, detected_boxes);
  cv::imwrite(save_img_path, img_bgr);
  
  delete yolov5;
}
```

è¾“å‡ºçš„ç»“æœæ˜¯:
<div align='center'>
  <img src='logs/test_lite_yolov5_1.jpg' height="256px">
  <img src='logs/test_lite_yolov5_2.jpg' height="256px">
</div>

æˆ–è€…ä½ å¯ä»¥ä½¿ç”¨æœ€æ–°çš„ ğŸ”¥ğŸ”¥ ! YOLO ç³»åˆ—æ£€æµ‹å™¨[YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) æˆ– [YoloR](https://github.com/WongKinYiu/yolor) ï¼Œå®ƒä»¬ä¼šè·å¾—æ¥è¿‘çš„ç»“æœã€‚

```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/yolox_s.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_yolox_1.jpg";
  std::string save_img_path = "../../../logs/test_lite_yolox_1.jpg";

  auto *yolox = new lite::cv::detection::YoloX(onnx_path); 
  std::vector<lite::cv::types::Boxf> detected_boxes;
  cv::Mat img_bgr = cv::imread(test_img_path);
  yolox->detect(img_bgr, detected_boxes);
  
  lite::cv::utils::draw_boxes_inplace(img_bgr, detected_boxes);
  cv::imwrite(save_img_path, img_bgr);  
  
  delete yolox;
}
```
è¾“å‡ºçš„ç»“æœæ˜¯:
<div align='center'>
  <img src='logs/test_lite_yolox_1.jpg' height="256px">
  <img src='logs/test_lite_yolox_2.jpg' height="256px">
</div>    

ç›®æ ‡æ£€æµ‹æ›´å¤šå¯ä½¿ç”¨çš„ç®—æ³•åŒ…æ‹¬ï¼š
```c++
auto *detector = new lite::cv::detection::YoloX(onnx_path);  // Newest YOLO detector !!! 2021-07
auto *detector = new lite::cv::detection::YoloV4(onnx_path); 
auto *detector = new lite::cv::detection::YoloV3(onnx_path); 
auto *detector = new lite::cv::detection::TinyYoloV3(onnx_path); 
auto *detector = new lite::cv::detection::SSD(onnx_path); 
auto *detector = new lite::cv::detection::YoloV5(onnx_path); 
auto *detector = new lite::cv::detection::YoloR(onnx_path);  // Newest YOLO detector !!! 2021-05
auto *detector = new lite::cv::detection::TinyYoloV4VOC(onnx_path); 
auto *detector = new lite::cv::detection::TinyYoloV4COCO(onnx_path); 
auto *detector = new lite::cv::detection::ScaledYoloV4(onnx_path); 
auto *detector = new lite::cv::detection::EfficientDet(onnx_path); 
auto *detector = new lite::cv::detection::EfficientDetD7(onnx_path); 
auto *detector = new lite::cv::detection::EfficientDetD8(onnx_path); 
auto *detector = new lite::cv::detection::YOLOP(onnx_path); 
```

</details>


<details>
<summary> 3.2 äººè„¸è¯†åˆ«åº”ç”¨æ¡ˆä¾‹ </summary>

#### 3.2 ä½¿ç”¨[ArcFace](https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch) è¿›è¡Œäººè„¸è¯†åˆ«ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚

```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/ms1mv3_arcface_r100.onnx";
  std::string test_img_path0 = "../../../examples/lite/resources/test_lite_faceid_0.png";
  std::string test_img_path1 = "../../../examples/lite/resources/test_lite_faceid_1.png";
  std::string test_img_path2 = "../../../examples/lite/resources/test_lite_faceid_2.png";

  auto *glint_arcface = new lite::cv::faceid::GlintArcFace(onnx_path);

  lite::cv::types::FaceContent face_content0, face_content1, face_content2;
  cv::Mat img_bgr0 = cv::imread(test_img_path0);
  cv::Mat img_bgr1 = cv::imread(test_img_path1);
  cv::Mat img_bgr2 = cv::imread(test_img_path2);
  glint_arcface->detect(img_bgr0, face_content0);
  glint_arcface->detect(img_bgr1, face_content1);
  glint_arcface->detect(img_bgr2, face_content2);

  if (face_content0.flag && face_content1.flag && face_content2.flag)
  {
    float sim01 = lite::cv::utils::math::cosine_similarity<float>(
        face_content0.embedding, face_content1.embedding);
    float sim02 = lite::cv::utils::math::cosine_similarity<float>(
        face_content0.embedding, face_content2.embedding);
    std::cout << "Detected Sim01: " << sim  << " Sim02: " << sim02 << std::endl;
  }

  delete glint_arcface;
}
```

è¾“å‡ºçš„ç»“æœæ˜¯:
<div align='center'>
  <img src='examples/lite/resources/test_lite_arcface_resnet_0.png' height="224px" width="224px">
  <img src='examples/lite/resources/test_lite_arcface_resnet_1.png' height="224px" width="224px">
  <img src='examples/lite/resources/test_lite_arcface_resnet_2.png' height="224px" width="224px">
</div>  

> Detected Sim01: 0.721159  Sim02: -0.0626267

äººè„¸è¯†åˆ«æ›´å¤šå¯ä»¥ä½¿ç”¨çš„ç®—æ³•åŒ…æ‹¬ï¼š
```c++
auto *recognition = new lite::cv::faceid::GlintCosFace(onnx_path);  // DeepGlint(insightface)
auto *recognition = new lite::cv::faceid::GlintArcFace(onnx_path);  // DeepGlint(insightface)
auto *recognition = new lite::cv::faceid::GlintPartialFC(onnx_path); // DeepGlint(insightface)
auto *recognition = new lite::cv::faceid::FaceNet(onnx_path);
auto *recognition = new lite::cv::faceid::FocalArcFace(onnx_path);
auto *recognition = new lite::cv::faceid::FocalAsiaArcFace(onnx_path);
auto *recognition = new lite::cv::faceid::TencentCurricularFace(onnx_path); // Tencent(TFace)
auto *recognition = new lite::cv::faceid::TencentCifpFace(onnx_path); // Tencent(TFace)
auto *recognition = new lite::cv::faceid::CenterLossFace(onnx_path);
auto *recognition = new lite::cv::faceid::SphereFace(onnx_path);
auto *recognition = new lite::cv::faceid::PoseRobustFace(onnx_path);
auto *recognition = new lite::cv::faceid::NaivePoseRobustFace(onnx_path);
auto *recognition = new lite::cv::faceid::MobileFaceNet(onnx_path); // 3.8Mb only !
auto *recognition = new lite::cv::faceid::CavaGhostArcFace(onnx_path);
auto *recognition = new lite::cv::faceid::CavaCombinedFace(onnx_path);
auto *recognition = new lite::cv::faceid::MobileSEFocalFace(onnx_path); // 4.5Mb only !
```

</details>


<details>
<summary> 3.3 è¯­ä¹‰åˆ†å‰²åº”ç”¨æ¡ˆä¾‹ </summary>

#### 3.3 ä½¿ç”¨[DeepLabV3ResNet101](https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/) è¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/deeplabv3_resnet101_coco.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_deeplabv3_resnet101.png";
  std::string save_img_path = "../../../logs/test_lite_deeplabv3_resnet101.jpg";

  auto *deeplabv3_resnet101 = new lite::cv::segmentation::DeepLabV3ResNet101(onnx_path, 16); // 16 threads

  lite::cv::types::SegmentContent content;
  cv::Mat img_bgr = cv::imread(test_img_path);
  deeplabv3_resnet101->detect(img_bgr, content);

  if (content.flag)
  {
    cv::Mat out_img;
    cv::addWeighted(img_bgr, 0.2, content.color_mat, 0.8, 0., out_img);
    cv::imwrite(save_img_path, out_img);
    if (!content.names_map.empty())
    {
      for (auto it = content.names_map.begin(); it != content.names_map.end(); ++it)
      {
        std::cout << it->first << " Name: " << it->second << std::endl;
      }
    }
  }
  delete deeplabv3_resnet101;
}
```

è¾“å‡ºçš„ç»“æœæ˜¯:
<div align='center'>
  <img src='examples/lite/resources/test_lite_deeplabv3_resnet101.png' height="256px">
  <img src='logs/test_lite_deeplabv3_resnet101.jpg' height="256px">
</div> 

è¯­ä¹‰åˆ†å‰²æ›´å¤šå¯ç”¨çš„ç®—æ³•åŒ…æ‹¬ï¼š
```c++
auto *segment = new lite::cv::segmentation::FCNResNet101(onnx_path);
```

</details>


<details>
<summary> 3.4 äººè„¸å±æ€§ä¼°è®¡åº”ç”¨æ¡ˆä¾‹ </summary>

#### 3.4 ä½¿ç”¨[SSRNet](https://github.com/oukohou/SSR_Net_Pytorch) è¿›è¡Œå¹´é¾„ä¼°è®¡ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/ssrnet.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_ssrnet.jpg";
  std::string save_img_path = "../../../logs/test_lite_ssrnet.jpg";

  lite::cv::face::attr::SSRNet *ssrnet = new lite::cv::face::attr::SSRNet(onnx_path);

  lite::cv::types::Age age;
  cv::Mat img_bgr = cv::imread(test_img_path);
  ssrnet->detect(img_bgr, age);
  lite::cv::utils::draw_age_inplace(img_bgr, age);
  cv::imwrite(save_img_path, img_bgr);
  std::cout << "Default Version Done! Detected SSRNet Age: " << age.age << std::endl;

  delete ssrnet;
}
```
è¾“å‡ºçš„ç»“æœæ˜¯:
<div align='center'>
  <img src='logs/test_lite_ssrnet.jpg' height="224px" width="224px">
  <img src='logs/test_lite_gender_googlenet.jpg' height="224px" width="224px">
  <img src='logs/test_lite_emotion_ferplus.jpg' height="224px" width="224px">
</div>    

äººè„¸å±æ€§ä¼°è®¡æ›´å¤šå¯ç”¨çš„ç®—æ³•åŒ…æ‹¬ï¼š
```c++
auto *attribute = new lite::cv::face::attr::AgeGoogleNet(onnx_path);  
auto *attribute = new lite::cv::face::attr::GenderGoogleNet(onnx_path); 
auto *attribute = new lite::cv::face::attr::EmotionFerPlus(onnx_path);
auto *attribute = new lite::cv::face::attr::VGG16Age(onnx_path);
auto *attribute = new lite::cv::face::attr::VGG16Gender(onnx_path);
auto *attribute = new lite::cv::face::attr::EfficientEmotion7(onnx_path); // 7 emotions, 15Mb only!
auto *attribute = new lite::cv::face::attr::EfficientEmotion8(onnx_path); // 8 emotions, 15Mb only!
auto *attribute = new lite::cv::face::attr::MobileEmotion7(onnx_path); // 7 emotions
auto *attribute = new lite::cv::face::attr::ReXNetEmotion7(onnx_path); // 7 emotions
```

</details>



<details>
<summary> 3.5 å›¾åƒåˆ†ç±»åº”ç”¨æ¡ˆä¾‹ </summary>

#### 3.5 ä½¿ç”¨[DenseNet](https://pytorch.org/hub/pytorch_vision_densenet/) è¿›è¡Œå›¾åƒåˆ†ç±»ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/densenet121.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_densenet.jpg";

  auto *densenet = new lite::cv::classification::DenseNet(onnx_path);

  lite::cv::types::ImageNetContent content;
  cv::Mat img_bgr = cv::imread(test_img_path);
  densenet->detect(img_bgr, content);
  if (content.flag)
  {
    const unsigned int top_k = content.scores.size();
    if (top_k > 0)
    {
      for (unsigned int i = 0; i < top_k; ++i)
        std::cout << i + 1
                  << ": " << content.labels.at(i)
                  << ": " << content.texts.at(i)
                  << ": " << content.scores.at(i)
                  << std::endl;
    }
  }
  delete densenet;
}
```

è¾“å‡ºçš„ç»“æœæ˜¯:
<div align='center'>
  <img src='examples/lite/resources/test_lite_densenet.jpg' height="224px" width="224px">
  <img src='logs/test_lite_densenet.png' height="224px" width="500px">
</div>  

å›¾åƒåˆ†ç±»æ›´å¤šå¯ç”¨çš„ç®—æ³•åŒ…æ‹¬ï¼š
```c++
auto *classifier = new lite::cv::classification::EfficientNetLite4(onnx_path);  
auto *classifier = new lite::cv::classification::ShuffleNetV2(onnx_path); 
auto *classifier = new lite::cv::classification::GhostNet(onnx_path);
auto *classifier = new lite::cv::classification::HdrDNet(onnx_path);
auto *classifier = new lite::cv::classification::IBNNet(onnx_path);
auto *classifier = new lite::cv::classification::MobileNetV2(onnx_path); 
auto *classifier = new lite::cv::classification::ResNet(onnx_path); 
auto *classifier = new lite::cv::classification::ResNeXt(onnx_path);
```

</details>



<details>
<summary> 3.6 äººè„¸æ£€æµ‹åº”ç”¨æ¡ˆä¾‹ </summary>

#### 3.6 ä½¿ç”¨[UltraFace](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) è¿›è¡Œäººè„¸æ£€æµ‹ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/ultraface-rfb-640.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_ultraface.jpg";
  std::string save_img_path = "../../../logs/test_lite_ultraface.jpg";

  auto *ultraface = new lite::cv::face::detect::UltraFace(onnx_path);

  std::vector<lite::cv::types::Boxf> detected_boxes;
  cv::Mat img_bgr = cv::imread(test_img_path);
  ultraface->detect(img_bgr, detected_boxes);
  lite::cv::utils::draw_boxes_inplace(img_bgr, detected_boxes);
  cv::imwrite(save_img_path, img_bgr);

  delete ultraface;
}
```
è¾“å‡ºçš„ç»“æœæ˜¯:
<div align='center'>
  <img src='logs/test_lite_ultraface.jpg' height="224px" width="224px">
  <img src='logs/test_lite_ultraface_2.jpg' height="224px" width="224px">
  <img src='logs/test_lite_ultraface_3.jpg' height="224px" width="224px">
</div>  

äººè„¸æ£€æµ‹æ›´å¤šå¯ç”¨çš„ç®—æ³•åŒ…æ‹¬ï¼š
```c++
auto *detector = new lite::face::detect::UltraFace(onnx_path);  // 1.1Mb only !
auto *detector = new lite::face::detect::FaceBoxes(onnx_path);  // 3.8Mb only ! 
auto *detector = new lite::face::detect::RetinaFace(onnx_path);  // 1.6Mb only ! CVPR2020
```

</details>


<details>
<summary> 3.7 å›¾åƒç€è‰²åº”ç”¨æ¡ˆä¾‹ </summary>

#### 3.7 ä½¿ç”¨[colorization](https://github.com/richzhang/colorization) è¿›è¡Œå›¾åƒç€è‰²ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/eccv16-colorizer.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_colorizer_1.jpg";
  std::string save_img_path = "../../../logs/test_lite_eccv16_colorizer_1.jpg";
  
  auto *colorizer = new lite::cv::colorization::Colorizer(onnx_path);
  
  cv::Mat img_bgr = cv::imread(test_img_path);
  lite::cv::types::ColorizeContent colorize_content;
  colorizer->detect(img_bgr, colorize_content);
  
  if (colorize_content.flag) cv::imwrite(save_img_path, colorize_content.mat);
  delete colorizer;
}
```
è¾“å‡ºçš„ç»“æœæ˜¯:

<div align='center'>
  <img src='examples/lite/resources/test_lite_colorizer_1.jpg' height="224px" width="224px">
  <img src='examples/lite/resources/test_lite_colorizer_2.jpg' height="224px" width="224px">
  <img src='examples/lite/resources/test_lite_colorizer_3.jpg' height="224px" width="224px">  
  <br> 
  <img src='logs/test_lite_siggraph17_colorizer_1.jpg' height="224px" width="224px">
  <img src='logs/test_lite_siggraph17_colorizer_2.jpg' height="224px" width="224px">
  <img src='logs/test_lite_siggraph17_colorizer_3.jpg' height="224px" width="224px">
</div>  

</details>


<details>
<summary> 3.8 å¤´éƒ¨å§¿æ€ä¼°è®¡åº”ç”¨æ¡ˆä¾‹ </summary>

#### 3.8 ä½¿ç”¨[FSANet](https://github.com/omasaht/headpose-fsanet-pytorch) è¿›è¡Œå¤´éƒ¨å§¿æ€ä¼°è®¡ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚

```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/fsanet-var.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_fsanet.jpg";
  std::string save_img_path = "../../../logs/test_lite_fsanet.jpg";

  auto *fsanet = new lite::cv::face::pose::FSANet(onnx_path);
  cv::Mat img_bgr = cv::imread(test_img_path);
  lite::cv::types::EulerAngles euler_angles;
  fsanet->detect(img_bgr, euler_angles);
  
  if (euler_angles.flag)
  {
    lite::cv::utils::draw_axis_inplace(img_bgr, euler_angles);
    cv::imwrite(save_img_path, img_bgr);
    std::cout << "yaw:" << euler_angles.yaw << " pitch:" << euler_angles.pitch << " row:" << euler_angles.roll << std::endl;
  }
  delete fsanet;
}
```

è¾“å‡ºçš„ç»“æœæ˜¯:
<div align='center'>
  <img src='logs/test_lite_fsanet.jpg' height="224px" width="224px">
  <img src='logs/test_lite_fsanet_2.jpg' height="224px" width="224px">
  <img src='logs/test_lite_fsanet_3.jpg' height="224px" width="224px">
</div>  

</details>


<details>
<summary> 3.9 äººè„¸å…³é”®ç‚¹æ£€æµ‹åº”ç”¨æ¡ˆä¾‹ </summary>

#### 3.9 ä½¿ç”¨[FaceLandmarks1000](https://github.com/Single430/FaceLandmark1000) è¿›è¡Œäººè„¸1000å…³é”®ç‚¹æ£€æµ‹ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/FaceLandmark1000.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_face_landmarks_0.png";
  std::string save_img_path = "../../../logs/test_lite_face_landmarks_1000.jpg";
    
  auto *face_landmarks_1000 = new lite::cv::face::align::FaceLandmark1000(onnx_path);

  lite::cv::types::Landmarks landmarks;
  cv::Mat img_bgr = cv::imread(test_img_path);
  face_landmarks_1000->detect(img_bgr, landmarks);
  lite::cv::utils::draw_landmarks_inplace(img_bgr, landmarks);
  cv::imwrite(save_img_path, img_bgr);
  
  delete face_landmarks_1000;
}
```
è¾“å‡ºçš„ç»“æœæ˜¯:
<div align='center'>
  <img src='logs/test_lite_face_landmarks_1000.jpg' height="224px" width="224px">
  <img src='logs/test_lite_face_landmarks_1000_2.jpg' height="224px" width="224px">
  <img src='logs/test_lite_face_landmarks_1000_0.jpg' height="224px" width="224px">
</div>    

äººè„¸å…³é”®ç‚¹æ£€æµ‹æ›´å¤šå¯ç”¨çš„ç®—æ³•åŒ…æ‹¬ï¼š
```c++
auto *align = new lite::cv::face::align::PFLD(onnx_path);  // 106 landmarks
auto *align = new lite::cv::face::align::PFLD98(onnx_path);  // 98 landmarks
auto *align = new lite::cv::face::align::PFLD68(onnx_path);  // 68 landmarks
auto *align = new lite::cv::face::align::MobileNetV268(onnx_path);  // 68 landmarks
auto *align = new lite::cv::face::align::MobileNetV2SE68(onnx_path);  // 68 landmarks
auto *align = new lite::cv::face::align::FaceLandmark1000(onnx_path);  // 1000 landmarks !
```

</details>


<details>
<summary> 3.10 é£æ ¼è¿ç§»åº”ç”¨æ¡ˆä¾‹ </summary>

#### 3.10 ä½¿ç”¨[FastStyleTransfer](https://github.com/onnx/models/tree/master/vision/style_transfer/fast_neural_style) è¿›è¡Œè‡ªç„¶é£æ ¼è¿ç§»ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/style-candy-8.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_fast_style_transfer.jpg";
  std::string save_img_path = "../../../logs/test_lite_fast_style_transfer_candy.jpg";
  
  auto *fast_style_transfer = new lite::cv::style::FastStyleTransfer(onnx_path);
 
  lite::cv::types::StyleContent style_content;
  cv::Mat img_bgr = cv::imread(test_img_path);
  fast_style_transfer->detect(img_bgr, style_content);

  if (style_content.flag) cv::imwrite(save_img_path, style_content.mat);
  delete fast_style_transfer;
}
```
è¾“å‡ºçš„ç»“æœæ˜¯:

<div align='center'>
  <img src='examples/lite/resources/test_lite_fast_style_transfer.jpg' height="224px">
  <img src='logs/test_lite_fast_style_transfer_candy.jpg' height="224px">
  <img src='logs/test_lite_fast_style_transfer_mosaic.jpg' height="224px">  
  <br> 
  <img src='logs/test_lite_fast_style_transfer_pointilism.jpg' height="224px">
  <img src='logs/test_lite_fast_style_transfer_rain_princes.jpg' height="224px">
  <img src='logs/test_lite_fast_style_transfer_udnie.jpg' height="224px">
</div>

</details>


<details>
<summary> 3.11 æŠ å›¾åº”ç”¨æ¡ˆä¾‹ </summary>

#### 3.11 ä½¿ç”¨[RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) è¿›è¡Œè§†é¢‘æŠ å›¾ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚

```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/rvm_mobilenetv3_fp32.onnx";
  std::string video_path = "../../../examples/lite/resources/test_lite_rvm_0.mp4";
  std::string output_path = "../../../logs/test_lite_rvm_0.mp4";
  
  auto *rvm = new lite::cv::matting::RobustVideoMatting(onnx_path, 16); // 16 threads
  std::vector<lite::cv::types::MattingContent> contents;
  
  // 1. video matting.
  rvm->detect_video(video_path, output_path, contents);
  
  delete rvm;
}
```
è¾“å‡ºçš„ç»“æœæ˜¯:

<div align='center'>
  <img src='docs/resources/interviewi.gif' height="200px" width="200px">
  <img src='docs/resources/interview.gif' height="200px" width="200px">  
  <img src='docs/resources/dance3i.gif' height="200px" width="200px">
  <img src='docs/resources/dance3.gif' height="200px" width="200px">
  <br>
  <img src='docs/resources/teslai.gif' height="200px" width="200px">
  <img src='docs/resources/tesla.gif' height="200px" width="200px">  
  <img src='docs/resources/b5i.gif' height="200px" width="200px">
  <img src='docs/resources/b5.gif' height="200px" width="200px">
</div>


</details>

</details>


## 4. Lite.AI.ToolKit APIæ–‡æ¡£

<div id="lite.ai.toolkit-Lite.AI.ToolKit-API-Docs"></div>

### 4.1 é»˜è®¤ç‰ˆæœ¬çš„APIs.
æ›´å¤šé»˜è®¤ç‰ˆæœ¬çš„APIæ–‡æ¡£ï¼Œè¯¦è§ [api.default.md](https://github.com/DefTruth/lite.ai.toolkit/blob/main/docs/api/api.default.md) ã€‚ æ¯”å¦‚ï¼ŒYoloV5çš„APIæ˜¯:

> `lite::cv::detection::YoloV5`
```c++
void detect(const cv::Mat &mat, std::vector<types::Boxf> &detected_boxes, 
            float score_threshold = 0.25f, float iou_threshold = 0.45f,
            unsigned int topk = 100, unsigned int nms_type = NMS::OFFSET);
```


<details>
<summary> ONNXRuntimeï¼ŒMNN å’Œ NCNN ç‰ˆæœ¬çš„APIs.</summary>

### 4.2 ONNXRuntime ç‰ˆæœ¬ APIs.
æ›´å¤šONNXRuntimeç‰ˆæœ¬çš„APIæ–‡æ¡£ï¼Œè¯¦è§ [api.onnxruntime.md](https://github.com/DefTruth/lite.ai.toolkit/blob/main/docs/api/api.onnxruntime.md) ã€‚æ¯”å¦‚ï¼ŒYoloV5çš„APIæ˜¯:

> `lite::onnxruntime::cv::detection::YoloV5`
```c++
void detect(const cv::Mat &mat, std::vector<types::Boxf> &detected_boxes, 
            float score_threshold = 0.25f, float iou_threshold = 0.45f,
            unsigned int topk = 100, unsigned int nms_type = NMS::OFFSET);
```


### 4.3 MNN ç‰ˆæœ¬ APIs.

(*todo*âš ï¸: å¾…å®ç°)

> `lite::mnn::cv::detection::YoloV5`

> `lite::mnn::cv::detection::YoloV4`

> `lite::mnn::cv::detection::YoloV3`

> `lite::mnn::cv::detection::SSD`

...


### 4.4 NCNN ç‰ˆæœ¬ APIs.

(*todo*âš ï¸: å¾…å®ç°)

> `lite::ncnn::cv::detection::YoloV5`

> `lite::ncnn::cv::detection::YoloV4`

> `lite::ncnn::cv::detection::YoloV3`

> `lite::ncnn::cv::detection::SSD`

...

</details>



## 5. å…¶ä»–æ–‡æ¡£

<div id="lite.ai.toolkit-Other-Docs"></div>  
<div id="lite.ai.toolkit-1"></div>

<details>
<summary> å±•å¼€å…¶ä»–æ–‡æ¡£ </summary>

### 5.1 ONNXRuntimeç›¸å…³çš„æ–‡æ¡£.
* [Rapid implementation of your inference using BasicOrtHandler](https://github.com/DefTruth/lite.ai.toolkit/blob/main/docs/ort/ort_handler.zh.md)
* [Some very useful onnxruntime c++ interfaces](https://github.com/DefTruth/lite.ai.toolkit/blob/main/docs/ort/ort_useful_api.zh.md)
* [How to compile a single model in this library you needed](https://github.com/DefTruth/lite.ai.toolkit/blob/main/docs/ort/ort_build_single.zh.md)
* [How to convert SubPixelCNN to ONNX and implements with onnxruntime c++](https://github.com/DefTruth/lite.ai.toolkit/blob/main/docs/ort/ort_subpixel_cnn.zh.md)
* [How to convert Colorizer to ONNX and implements with onnxruntime c++](https://github.com/DefTruth/lite.ai.toolkit/blob/main/docs/ort/ort_colorizer.zh.md)
* [How to convert SSRNet to ONNX and implements with onnxruntime c++](https://github.com/DefTruth/lite.ai.toolkit/blob/main/docs/ort/ort_ssrnet.zh.md)
* [How to convert YoloV3 to ONNX and implements with onnxruntime c++](https://github.com/DefTruth/lite.ai.toolkit/blob/main/docs/ort/ort_yolov3.zh.md)
* [How to convert YoloV5 to ONNX and implements with onnxruntime c++](https://github.com/DefTruth/lite.ai.toolkit/blob/main/docs/ort/ort_yolov5.zh.md)


### 5.2 [third_party](https://github.com/DefTruth/lite.ai.toolkit/tree/main/third_party) ç›¸å…³çš„æ–‡æ¡£  

|Library|Target|Docs|
|:---:|:---:|:---:|
|OpenCV| mac-x86_64 | [opencv-mac-x86_64-build.zh.md](https://github.com/DefTruth/lite.ai.toolkit/blob/main/docs/third_party/opencv-mac-x86_64-build.zh.md) |
|OpenCV| android-arm | [opencv-static-android-arm-build.zh.md](https://github.com/DefTruth/lite.ai.toolkit/blob/main/docs/third_party/opencv-static-android-arm-build.zh.md) |
|onnxruntime| mac-x86_64 | [onnxruntime-mac-x86_64-build.zh.md](https://github.com/DefTruth/lite.ai.toolkit/blob/main/docs/third_party/onnxruntime-mac-x86_64-build.zh.md) |
|onnxruntime| android-arm | [onnxruntime-android-arm-build.zh.md](https://github.com/DefTruth/lite.ai.toolkit/blob/main/docs/third_party/onnxruntime-android-arm-build.zh.md) |
|NCNN| mac-x86_64 | todoâš ï¸ |
|MNN| mac-x86_64 | todoâš ï¸ |
|TNN| mac-x86_64 | todoâš ï¸ |

</details>

## 6. å¼€æºåè®® 

<div id="lite.ai.toolkit-License"></div>

[Lite.AI.ToolKit](#lite.ai.toolkit-Introduction) çš„ä»£ç é‡‡ç”¨GPL-3.0åè®®ã€‚


## 7. å¼•ç”¨å‚è€ƒ 

<div id="lite.ai.toolkit-References"></div>

æœ¬é¡¹ç›®å‚è€ƒäº†ä»¥ä¸‹å¼€æºé¡¹ç›®ã€‚

* [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) (ğŸ”¥ğŸ”¥ğŸ”¥new!!â†‘)
* [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) (ğŸ”¥ğŸ”¥ğŸ”¥new!!â†‘)
* [YOLOP](https://github.com/hustvl/YOLOP) (ğŸ”¥ğŸ”¥new!!â†‘)
* [YOLOR](https://github.com/WongKinYiu/yolor) (ğŸ”¥ğŸ”¥new!!â†‘)
* [ScaledYOLOv4](https://github.com/WongKinYiu/ScaledYOLOv4) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [insightface](https://github.com/deepinsight/insightface) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [yolov5](https://github.com/ultralytics/yolov5) (ğŸ”¥ğŸ”¥ğŸ’¥â†‘)
* [TFace](https://github.com/Tencent/TFace) (ğŸ”¥ğŸ”¥â†‘)
* [YOLOv4-pytorch](https://github.com/argusswift/YOLOv4-pytorch) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [Ultra-Light-Fast-Generic-Face-Detector-1MB](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)

<details>
<summary> å±•å¼€æ›´å¤šå¼•ç”¨å‚è€ƒ </summary>  

* [headpose-fsanet-pytorch](https://github.com/omasaht/headpose-fsanet-pytorch) (ğŸ”¥â†‘)
* [pfld_106_face_landmarks](https://github.com/Hsintao/pfld_106_face_landmarks) (ğŸ”¥ğŸ”¥â†‘)
* [onnx-models](https://github.com/onnx/models) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [SSR_Net_Pytorch](https://github.com/oukohou/SSR_Net_Pytorch) (ğŸ”¥â†‘)
* [colorization](https://github.com/richzhang/colorization) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [SUB_PIXEL_CNN](https://github.com/niazwazir/SUB_PIXEL_CNN) (ğŸ”¥â†‘)
* [torchvision](https://github.com/pytorch/vision) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [facenet-pytorch](https://github.com/timesler/facenet-pytorch) (ğŸ”¥â†‘)
* [face.evoLVe.PyTorch](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [center-loss.pytorch](https://github.com/louis-she/center-loss.pytorch) (ğŸ”¥ğŸ”¥â†‘)
* [sphereface_pytorch](https://github.com/clcarwin/sphereface_pytorch) (ğŸ”¥ğŸ”¥â†‘)
* [DREAM](https://github.com/penincillin/DREAM) (ğŸ”¥ğŸ”¥â†‘)
* [MobileFaceNet_Pytorch](https://github.com/Xiaoccer/MobileFaceNet_Pytorch) (ğŸ”¥ğŸ”¥â†‘)
* [cavaface.pytorch](https://github.com/cavalleria/cavaface.pytorch) (ğŸ”¥ğŸ”¥â†‘)
* [CurricularFace](https://github.com/HuangYG123/CurricularFace) (ğŸ”¥ğŸ”¥â†‘)
* [face-emotion-recognition](https://github.com/HSE-asavchenko/face-emotion-recognition) (ğŸ”¥â†‘)
* [face_recognition.pytorch](https://github.com/grib0ed0v/face_recognition.pytorch) (ğŸ”¥ğŸ”¥â†‘)
* [PFLD-pytorch](https://github.com/polarisZhao/PFLD-pytorch) (ğŸ”¥ğŸ”¥â†‘)
* [pytorch_face_landmark](https://github.com/cunjian/pytorch_face_landmark) (ğŸ”¥ğŸ”¥â†‘)
* [FaceLandmark1000](https://github.com/Single430/FaceLandmark1000) (ğŸ”¥ğŸ”¥â†‘)
* [Pytorch_Retinaface](https://github.com/biubug6/Pytorch_Retinaface) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch) (ğŸ”¥ğŸ”¥â†‘)

</details>    

## å¼•ç”¨æœ¬é¡¹ç›® ![](https://img.shields.io/github/stars/DefTruth/lite.ai.toolkit.svg?style=social) ![](https://img.shields.io/github/forks/DefTruth/lite.ai.toolkit.svg?style=social) ![](https://img.shields.io/github/watchers/DefTruth/lite.ai.toolkit.svg?style=social)


å¦‚æœæ‚¨åœ¨è‡ªå·±çš„é¡¹ç›®ä¸­ä½¿ç”¨äº†*Lite.AI.ToolKit*ï¼Œå¯è€ƒè™‘æŒ‰ä»¥ä¸‹æ–¹å¼è¿›è¡Œå¼•ç”¨ã€‚æ•´ç†ä¸æ˜“ï¼Œæ¬¢è¿å…³æ³¨ï¼ŒğŸŒŸç‚¹èµæ”¶è—~ ğŸ™ƒğŸ¤ªğŸ€
```BibTeX
@misc{lite.ai.toolkit2021,
  title={lite.ai.toolkit: A lite C++ toolkit of awesome AI models.},
  url={https://github.com/DefTruth/lite.ai.toolkit},
  note={Open-source software available at https://github.com/DefTruth/lite.ai.toolkit},
  author={Yan Jun},
  year={2021}
}
```  



