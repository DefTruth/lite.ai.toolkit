

## <p align="center"> ğŸ…ğŸ…Lite.AI.ToolKit: ä¸€ä¸ªå¼€ç®±å³ç”¨çš„C++ AIæ¨¡å‹å·¥å…·ç®±</p>

<div id="lite.ai.toolkit-Introduction"></div>  

<div align='center'>
  <img src='logs/test_lite_yolov5_1.jpg' height="80px" width="80px">
  <img src='docs/resources/efficientdet_d0.jpg' height="80px" width="80px">
  <img src='docs/resources/street.jpg' height="80px" width="80px">
  <img src='logs/test_lite_ultraface.jpg' height="80px" width="80px">
  <img src='logs/test_lite_face_landmarks_1000.jpg' height="80px" width="80px">
  <img src='logs/test_lite_fsanet.jpg' height="80px" width="80px">
  <img src='logs/test_lite_deeplabv3_resnet101.jpg' height="80px" width="80px">
  <img src='logs/test_lite_fast_style_transfer_mosaic.jpg' height="80px" width="80px"> 
  <br>
  <img src='docs/resources/teslai.gif' height="80px" width="80px">
  <img src='docs/resources/tesla.gif' height="80px" width="80px">
  <img src='docs/resources/dance3i.gif' height="80px" width="80px">
  <img src='docs/resources/dance3.gif' height="80px" width="80px">  
  <img src='docs/resources/yolop1.png' height="80px" width="80px">
  <img src='docs/resources/yolop1.gif' height="80px" width="80px">
  <img src='docs/resources/yolop2.png' height="80px" width="80px">
  <img src='docs/resources/yolop2.gif' height="80px" width="80px">

</div>    

<p align="center"><a href="README.md">English</a> | ä¸­æ–‡æ–‡æ¡£ | <a href=#lite.ai.toolkit-Build-MacOS>MacOS</a> | <a href=#lite.ai.toolkit-Build-Linux>Linux</a> | <a href=#lite.ai.toolkit-Build-Windows>Windows</a> </p>

<div align='center'>
  <img src=https://img.shields.io/badge/MacOS-pass-brightgreen.svg >
  <img src=https://img.shields.io/badge/Linux-unofficial-blue.svg >
  <img src=https://img.shields.io/badge/Windows-unofficial-blue.svg >
  <img src=https://img.shields.io/badge/Version-0.1.0-green.svg >
  <img src=https://img.shields.io/badge/Language-C%2B%2B-orange.svg >
  <img src=https://img.shields.io/badge/Device-GPU/CPU-yellow.svg >
  <img src=https://img.shields.io/badge/License-GPLv3-blue.svg >
</div>   

<div align='center'> 
  <img src=https://img.shields.io/badge/ONNXRuntime-support-brightgreen.svg >
  <img src=https://img.shields.io/badge/MNN-support-brightgreen.svg >
  <img src=https://img.shields.io/badge/NCNN-support-brightgreen.svg >
  <img src=https://img.shields.io/badge/TNN-support-brightgreen.svg >
</div>    

<!-----
<div align='center'> 
  <img src=https://img.shields.io/badge/ONNXRuntime-support-brightgreen.svg >
  <img src=https://img.shields.io/badge/MNN-support-brightgreen.svg >
  <img src=https://img.shields.io/badge/NCNN-support-brightgreen.svg >
  <img src=https://img.shields.io/badge/TNN-support-brightgreen.svg >
<br>
  <img src=https://img.shields.io/github/stars/DefTruth/lite.ai.toolkit.svg?style=social >
  <img src=https://img.shields.io/github/forks/DefTruth/lite.ai.toolkit.svg?style=social >
  <img src=https://img.shields.io/github/watchers/DefTruth/lite.ai.toolkit.svg?style=social > 
</div>    

----->

ğŸ…ğŸ…*Lite.AI.ToolKit*: ä¸€ä¸ªè½»é‡çº§çš„`C++` AIæ¨¡å‹å·¥å…·ç®±ï¼Œç”¨æˆ·å‹å¥½ï¼ˆè¿˜è¡Œå§ï¼‰ï¼Œå¼€ç®±å³ç”¨ã€‚å·²ç»åŒ…æ‹¬ *[70+](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md)* æµè¡Œçš„å¼€æºæ¨¡å‹ï¼Œå¦‚æœ€æ–°çš„RVM, YOLOX, YOLOP, YOLOR, YoloV5, DeepLabV3, ArcFaceç­‰æ¨¡å‹ï¼Œè¿˜ä¼šç»§ç»­å¢åŠ ğŸ˜ã€‚è¿™æ˜¯ä¸€ä¸ªæ ¹æ®ä¸ªäººå…´è¶£æ•´ç†çš„C++å·¥å…·ç®±ï¼ŒemmmğŸ˜ ... å…¶å®è¿˜ä¸æ˜¯å¾ˆå®Œå–„ï¼Œç¼–ä¸ªlibæ¥ç”¨è¿˜æ˜¯å¯ä»¥çš„ã€‚`å…³äºè§„åˆ’ï¼Œå…¶å®æ²¡ä»€ä¹ˆå¾ˆæ˜ç¡®çš„è§„åˆ’ï¼Œçœ‹åˆ°ä¸€äº›æœ‰æ„æ€çš„ç®—æ³•å¯èƒ½ä¼šæŠŠå®ƒæè¿›æ¥ï¼Œéšç¼˜å§ã€‚` ä¸ªäººçš„å…´è¶£ç›®å‰ä¸»è¦é›†ä¸­åœ¨`æ£€æµ‹ã€åˆ†å‰²ã€æŠ å›¾ã€è¯†åˆ«å’Œç›®æ ‡è·Ÿè¸ª`ç­‰é¢†åŸŸã€‚ *Lite.AI.ToolKit* é»˜è®¤æ˜¯åŸºäº *[ONNXRuntime C++](https://github.com/microsoft/onnxruntime)* æ¨ç†å¼•æ“çš„ï¼ŒåæœŸä¼šé™†ç»­åŠ å…¥å¯¹ *[NCNN](https://github.com/Tencent/ncnn)*, *[MNN](https://github.com/alibaba/MNN)* å’Œ *[TNN](https://github.com/Tencent/TNN)* çš„æ”¯æŒï¼Œå·²ç»æ”¯æŒéƒ¨åˆ†æ¨¡å‹çš„MNNã€NCNNå’ŒTNNæ¨ç†ã€‚

## æ ¸å¿ƒç‰¹å¾ğŸ‘ğŸ‘‹

* *ç”¨æˆ·å‹å¥½ï¼Œå¼€ç®±å³ç”¨ã€‚* ä½¿ç”¨ç®€å•ä¸€è‡´çš„è°ƒç”¨è¯­æ³•ï¼Œå¦‚*lite::cv::Type::Class*ï¼Œè¯¦è§[examples](#lite.ai.toolkit-Examples-for-Lite.AI.ToolKit).
* *å°‘é‡ä¾èµ–ï¼Œæ„å»ºå®¹æ˜“ã€‚* ç›®å‰, é»˜è®¤åªä¾èµ– *OpenCV* å’Œ *ONNXRuntime*ï¼Œè¯¦è§[build](#lite.ai.toolkit-Build-Lite.AI.ToolKit)ã€‚
* *ä¼—å¤šçš„ç®—æ³•æ¨¡å—ï¼Œä¸”æŒç»­æ›´æ–°ã€‚* ç›®å‰ï¼ŒåŒ…æ‹¬ 10+ ç®—æ³•æ¨¡å—ã€*[70+](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md)* æµè¡Œçš„å¼€æºæ¨¡å‹ä»¥åŠ *[500+](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md)* æƒé‡æ–‡ä»¶, æ¶µç›–[ç›®æ ‡æ£€æµ‹](#lite.ai.toolkit-object-detection)ã€[äººè„¸æ£€æµ‹](#lite.ai.toolkit-face-detection)ã€[äººè„¸è¯†åˆ«](#lite.ai.toolkit-face-recognition)ã€[è¯­ä¹‰åˆ†å‰²](#lite.ai.toolkit-segmentation)ã€[æŠ å›¾](#lite.ai.toolkit-matting)ç­‰é¢†åŸŸã€‚è¯¦è§ [Model Zoo](#lite.ai.toolkit-Model-Zoo) å’Œ [ONNX Hub](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md) ã€[MNN Hub](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.mnn.md) ã€[TNN Hub](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.tnn.md) ã€[NCNN Hub](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.ncnn.md).

<p align="center"> è‹¥æ˜¯æœ‰ç”¨ï¼Œâ¤ï¸ä¸å¦¨ç»™ä¸ªâ­ï¸ğŸŒŸæ”¯æŒä¸€ä¸‹å§ï¼Œæ„Ÿè°¢æ”¯æŒ~  </p>

## é‡è¦æ›´æ–° !!

|Date|Model|C++ Demo|Paper|Code|Awesome|Type
|:----:|:----:|:----:|:----:|:----:|:----:|:----:| 
|ã€2021/12/08ã€‘|[MGMatting](https://github.com/yucornetto/MGMatting)|[[c++ demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_mg_matting.cpp)]|[[arXiv CVPR 2021](https://arxiv.org/abs/2012.06722)]|[[code](https://github.com/yucornetto/MGMatting)]|![](https://img.shields.io/github/stars/yucornetto/MGMatting.svg?style=social)| matting |
|ã€2021/11/11ã€‘|[YoloV5_V_6_0](https://github.com/ultralytics/yolov5/releases/tag/v6.0)|[[c++ demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolov5_v6.0.cpp)]|[[doi](https://zenodo.org/record/5563715#.YbXffH1Bzfs)]|[[code](https://github.com/ultralytics/yolov5/releases/tag/v6.0)]|![](https://img.shields.io/github/stars/ultralytics/yolov5.svg?style=social)| detection |
|ã€2021/10/26ã€‘|[YoloX_V_0_1_1](https://github.com/Megvii-BaseDetection/YOLOX/releases/tag/0.1.1rc0)|[[c++ demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolox_v0.1.1.cpp)]|[[arXiv report 2021](https://arxiv.org/abs/2107.08430)]|[[code](https://github.com/Megvii-BaseDetection/YOLOX)]|![](https://img.shields.io/github/stars/Megvii-BaseDetection/YOLOX.svg?style=social)| detection |
|ã€2021/10/02ã€‘|[NanoDet](https://github.com/RangiLyu/nanodet)|[[c++ demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_nanodet.cpp)]|[[blog](https://zhuanlan.zhihu.com/p/306530300)]|[[code](https://github.com/RangiLyu/nanodet)]|![](https://img.shields.io/github/stars/RangiLyu/nanodet.svg?style=social)| detection |
|ã€2021/09/20ã€‘|[RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting)|[[c++ demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_rvm.cpp)]|[[arXiv WACV 2021](https://arxiv.org/abs/2108.11515)]|[[code](https://github.com/PeterL1n/RobustVideoMatting)]|![](https://img.shields.io/github/stars/PeterL1n/RobustVideoMatting.svg?style=social)| matting |
|ã€2021/09/02ã€‘|[YOLOP](https://github.com/hustvl/YOLOP)|[[c++ demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolop.cpp)]|[[arXiv report 2021](https://arxiv.org/abs/2108.11250)]|[[code](https://github.com/hustvl/YOLOP)]|![](https://img.shields.io/github/stars/hustvl/YOLOP.svg?style=social)| detection |


## æ¨¡å‹æ”¯æŒçŸ©é˜µ

* / = æš‚ä¸æ”¯æŒ.
* âœ… = å¯ä»¥è¿è¡Œï¼Œä¸”å®˜æ–¹æ”¯æŒ.
* âœ”ï¸ = å¯ä»¥è¿è¡Œï¼Œä½†éå®˜æ–¹æ”¯æŒ.
* â” = è®¡åˆ’ä¸­ï¼Œä½†ä¸ä¼šå¾ˆå¿«å®ç°ï¼Œä¹Ÿè®¸å‡ ä¸ªæœˆå.

|Class|Size|Type|Demo|ONNXRuntime|MNN|NCNN|TNN|MacOS|Linux|Windows|Android|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|[YoloV5](https://github.com/ultralytics/yolov5)|28M|*detection*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolov5.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[YoloV3](https://github.com/onnx/models/blob/master/vision/object_detection_segmentation/yolov3)|236M|*detection*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolov3.cpp)|âœ…| / | / | / | âœ… |âœ”ï¸|âœ”ï¸|/|
|[TinyYoloV3](https://github.com/onnx/models/blob/master/vision/object_detection_segmentation/tiny-yolov3)|33M|  *detection*     | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_tiny_yolov3.cpp) | âœ… | / | / | / | âœ… |âœ”ï¸|âœ”ï¸|/|
|[YoloV4](https://github.com/argusswift/YOLOv4-pytorch)|176M| *detection* | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolov4.cpp) | âœ… | / | / | / | âœ… |âœ”ï¸|âœ”ï¸|/|
|[SSD](https://github.com/onnx/models/blob/master/vision/object_detection_segmentation/ssd)|76M|        *detection*         | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_ssd.cpp) | âœ…| / | / | / | âœ… |âœ”ï¸|âœ”ï¸|/|
|[SSDMobileNetV1](https://github.com/onnx/models/blob/master/vision/object_detection_segmentation/ssd-mobilenetv1)|27M|        *detection*         | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_ssd_mobilenetv1.cpp) | âœ… | / | / | / | âœ… |âœ”ï¸|âœ”ï¸|/|
|[YoloX](https://github.com/Megvii-BaseDetection/YOLOX)|3.5M|*detection* |[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolox.cpp) | âœ… | âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[TinyYoloV4VOC](https://github.com/bubbliiiing/yolov4-tiny-pytorch)|22M|*detection* |[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_tiny_yolov4_voc.cpp) | âœ… | / | / | / | âœ… |âœ”ï¸|âœ”ï¸|/|
|[TinyYoloV4COCO](https://github.com/bubbliiiing/yolov4-tiny-pytorch)|22M| *detection* | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_tiny_yolov4_coco.cpp) | âœ…| / | / | / | âœ… |âœ”ï¸|âœ”ï¸|/|
|[YoloR](https://github.com/WongKinYiu/yolor)|39M| *detection* | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolor.cpp) | âœ… | âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[ScaledYoloV4](https://github.com/WongKinYiu/ScaledYOLOv4)|270M| *detection* | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_scaled_yolov4.cpp) | âœ… | / | / | / | âœ… |âœ”ï¸|âœ”ï¸|/|
|[EfficientDet](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch)|15M| *detection* | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_efficientdet.cpp) | âœ… | / | / | / | âœ… |âœ”ï¸|âœ”ï¸|/|
|[EfficientDetD7](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch)|220M| *detection* | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_efficientdet_d7.cpp) | âœ… | / | / | / | âœ… |âœ”ï¸|âœ”ï¸|/|
|[EfficientDetD8](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch)|322M| *detection* | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_efficientdet_d8.cpp) | âœ… | / | / | / | âœ… |âœ”ï¸|âœ”ï¸|/|
|[YOLOP](https://github.com/hustvl/YOLOP)|30M| *detection* | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolop.cpp) | âœ… | âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[NanoDet](https://github.com/RangiLyu/nanodet)|1.1M| *detection* | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_nanodet.cpp) | âœ… | âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[NanoDetEffi...](https://github.com/RangiLyu/nanodet)|12M| *detection* | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_nanodet_efficientnet_lite.cpp) | âœ… | âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[YoloX_V_0_1_1](https://github.com/Megvii-BaseDetection/YOLOX)|3.5M| *detection* | [demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolox_v0.1.1.cpp) | âœ… | âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[YoloV5_V_6_0](https://github.com/ultralytics/yolov5)|7.5M|*detection*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_yolov5_v6.0.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
| [GlintArcFace](https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch) |92M|*faceid*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_glint_arcface.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[GlintCosFace](https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch)|92M|*faceid*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_glint_cosface.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|/|
|[GlintPartialFC](https://github.com/deepinsight/insightface/tree/master/recognition/partial_fc)|170M|*faceid*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_glint_partial_fc.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|/|
|[FaceNet](https://github.com/timesler/facenet-pytorch)|89M|*faceid*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_facenet.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|/|
|[FocalArcFace](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch)|166M|*faceid*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_focal_arcface.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|/|
|[FocalAsiaArcFace](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch)|166M|*faceid*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_focal_asia_arcface.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|/|
|[TencentCurricularFace](https://github.com/Tencent/TFace/tree/master/tasks/distfc)|249M|*faceid*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_tencent_curricular_face.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|/|
|[TencentCifpFace](https://github.com/Tencent/TFace/tree/master/tasks/cifp)|130M|*faceid*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_tencent_cifp_face.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|/|
|[CenterLossFace](https://github.com/louis-she/center-loss.pytorch)|280M|*faceid*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_center_loss_face.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|/|
|[SphereFace](https://github.com/clcarwin/sphereface_pytorch)|80M|*faceid*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_sphere_face.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|/|
|[PoseRobustFace](https://github.com/penincillin/DREAM)|92M|*faceid*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_pose_robust_face.cpp)|âœ…| / | / | / | âœ… |âœ”ï¸|âœ”ï¸|/|
|[NaivePoseRobustFace](https://github.com/penincillin/DREAM)|43M|*faceid*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_naive_pose_robust_face.cpp)|âœ…| / | / | / | âœ… |âœ”ï¸|âœ”ï¸|/|
|[MobileFaceNet](https://github.com/Xiaoccer/MobileFaceNet_Pytorch)|3.8M|*faceid*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_mobile_facenet.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[CavaGhostArcFace](https://github.com/cavalleria/cavaface.pytorch)|15M|*faceid*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_cava_ghost_arcface.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[CavaCombinedFace](https://github.com/cavalleria/cavaface.pytorch)|250M|*faceid*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_cava_combined_face.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|/|
|[MobileSEFocalFace](https://github.com/grib0ed0v/face_recognition.pytorch)|4.5M|*faceid*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_mobilese_focal_face.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting)|14M|*matting*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_rvm.cpp)|âœ…| âœ… | / | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[MGMatting](https://github.com/yucornetto/MGMatting)|113M|*matting*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_mg_matting.cpp)|âœ…| âœ… | / | âœ… | âœ… |âœ”ï¸|âœ”ï¸|/|
|[UltraFace](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB)|1.1M|*face::detect*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_ultraface.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[RetinaFace](https://github.com/biubug6/Pytorch_Retinaface)|1.6M|*face::detect*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_retinaface.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)|3.8M|*face::detect*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_faceboxes.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[PFLD](https://github.com/Hsintao/pfld_106_face_landmarks)|1.0M|*face::align*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_pfld.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[PFLD98](https://github.com/polarisZhao/PFLD-pytorch)|4.8M|*face::align*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_pfld98.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[MobileNetV268](https://github.com/cunjian/pytorch_face_landmark)|9.4M|*face::align*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_mobilenetv2_68.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[MobileNetV2SE68](https://github.com/cunjian/pytorch_face_landmark)|11M|*face::align*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_mobilenetv2_se_68.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[PFLD68](https://github.com/cunjian/pytorch_face_landmark)|2.8M|*face::align*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_pfld68.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[FaceLandmark1000](https://github.com/Single430/FaceLandmark1000)|2.0M|*face::align*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_face_landmarks_1000.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[FSANet](https://github.com/omasaht/headpose-fsanet-pytorch)|1.2M|*face::pose*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_fsanet.cpp)|âœ…| âœ… | / | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[AgeGoogleNet](https://github.com/onnx/models/tree/master/vision/body_analysis/age_gender)|23M|*face::attr*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_age_googlenet.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[GenderGoogleNet](https://github.com/onnx/models/tree/master/vision/body_analysis/age_gender)|23M|*face::attr*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_gender_googlenet.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[EmotionFerPlus](https://github.com/onnx/models/blob/master/vision/body_analysis/emotion_ferplus)|33M|*face::attr*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_emotion_ferplus.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[VGG16Age](https://github.com/onnx/models/tree/master/vision/body_analysis/age_gender)|514M|*face::attr*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_vgg16_age.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|/|
|[VGG16Gender](https://github.com/onnx/models/tree/master/vision/body_analysis/age_gender)|512M|*face::attr*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_vgg16_gender.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|/|
|[SSRNet](https://github.com/oukohou/SSR_Net_Pytorch)|190K|*face::attr*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_ssrnet.cpp)|âœ…| âœ… | / | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[EfficientEmotion7](https://github.com/HSE-asavchenko/face-emotion-recognition)|15M|*face::attr*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_efficient_emotion7.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[EfficientEmotion8](https://github.com/HSE-asavchenko/face-emotion-recognition)|15M|*face::attr*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_efficient_emotion8.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[MobileEmotion7](https://github.com/HSE-asavchenko/face-emotion-recognition)|13M|*face::attr*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_mobile_emotion7.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[ReXNetEmotion7](https://github.com/HSE-asavchenko/face-emotion-recognition)|30M|*face::attr*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_rexnet_emotion7.cpp)|âœ…| âœ… | / | âœ… | âœ… |âœ”ï¸|âœ”ï¸|/|
|[EfficientNetLite4](https://github.com/onnx/models/blob/master/vision/classification/efficientnet-lite4)|49M|*classification*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_efficientnet_lite4.cpp)|âœ…| âœ… | / | âœ… | âœ… |âœ”ï¸|âœ”ï¸|/|
|[ShuffleNetV2](https://github.com/onnx/models/blob/master/vision/classification/shufflenet)|8.7M|*classification*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_shufflenetv2.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[DenseNet121](https://pytorch.org/hub/pytorch_vision_densenet/)|30.7M|*classification*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_densenet.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|/|
|[GhostNet](https://pytorch.org/hub/pytorch_vision_ghostnet/)|20M|*classification*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_ghostnet.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[HdrDNet](https://pytorch.org/hub/pytorch_vision_hardnet//)|13M|*classification*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_hardnet.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[IBNNet](https://pytorch.org/hub/pytorch_vision_ibnnet/)|97M|*classification*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_ibnnet.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|/|
|[MobileNetV2](https://pytorch.org/hub/pytorch_vision_mobilenet_v2/)|13M|*classification*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_mobilenetv2.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[ResNet](https://pytorch.org/hub/pytorch_vision_resnet/)|44M|*classification*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_resnet.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|/|
|[ResNeXt](https://pytorch.org/hub/pytorch_vision_resnext/)|95M|*classification*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_resnext.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|/|
|[DeepLabV3ResNet101](https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/)|232M|*segmentation*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_deeplabv3_resnet101.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|/|
|[FCNResNet101](https://pytorch.org/hub/pytorch_vision_fcn_resnet101/)|207M|*segmentation*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_fcn_resnet101.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|/|
|[FastStyleTransfer](https://github.com/onnx/models/blob/master/vision/style_transfer/fast_neural_style)|6.4M|*style*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_fast_style_transfer.cpp)|âœ…| âœ… | âœ… | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|
|[Colorizer](https://github.com/richzhang/colorization)|123M|*colorization*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_colorizer.cpp)|âœ…| âœ… | / | âœ… | âœ… |âœ”ï¸|âœ”ï¸|/|
|[SubPixelCNN](https://github.com/niazwazir/SUB_PIXEL_CNN)|234K|*resolution*|[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_subpixel_cnn.cpp)|âœ…| âœ… | / | âœ… | âœ… |âœ”ï¸|âœ”ï¸|â”|


## ç›®å½•
* [ç¼–è¯‘](#lite.ai.toolkit-Build-Lite.AI.ToolKit)
* [æ¨¡å‹ä¸‹è½½](#lite.ai.toolkit-Model-Zoo)
* [åº”ç”¨æ¡ˆä¾‹](#lite.ai.toolkit-Examples-for-Lite.AI.ToolKit)
* [å¼€æºåè®®](#lite.ai.toolkit-License)
* [å¼•ç”¨å‚è€ƒ](#lite.ai.toolkit-References)


## 1. ç¼–è¯‘
<div id="lite.ai.toolkit-Build-MacOS"></div>
<div id="lite.ai.toolkit-Build-Lite.AI.ToolKit"></div>

* MacOS: ä»*Lite.AI.ToolKit* æºç ç¼–è¯‘*MacOS*ä¸‹çš„åŠ¨æ€åº“ã€‚éœ€è¦æ³¨æ„çš„æ˜¯*Lite.AI.ToolKit* ä½¿ç”¨`onnxruntime`ä½œä¸ºé»˜è®¤çš„åç«¯ï¼Œå› ä¸ºonnxruntimeæ”¯æŒå¤§éƒ¨åˆ†onnxçš„åŸç”Ÿç®—å­ï¼Œå…·æœ‰æ›´é«˜çš„æ˜“ç”¨æ€§ã€‚å¦‚ä½•ç¼–è¯‘Linuxå’ŒWindowsç‰ˆæœ¬ï¼Ÿç‚¹å‡» â–¶ï¸ æŸ¥çœ‹ã€‚
```shell
    git clone --depth=1 https://github.com/DefTruth/lite.ai.toolkit.git  # æœ€æ–°æºç 
    cd lite.ai.toolkit && sh ./build.sh  # å¯¹äºMacOS, ä½ å¯ä»¥ç›´æ¥åˆ©ç”¨æœ¬é¡¹ç›®åŒ…å«çš„OpenCV, ONNXRuntime, MNN, NCNN and TNNä¾èµ–åº“ï¼Œæ— éœ€é‡æ–°ç¼–è¯‘
```

<div id="lite.ai.toolkit-Build-Linux"></div>
<div id="lite.ai.toolkit-Build-Windows"></div>

<details>
<summary>ğŸ’¡ï¸ Linux å’Œ Windows </summary>  

### Linux å’Œ Windows

âš ï¸ *Lite.AI.ToolKit* çš„å‘è¡Œç‰ˆæœ¬ç›®å‰ä¸ç›´æ¥æ”¯æŒLinuxå’ŒWindowsï¼Œä½ éœ€è¦ä»ä¸‹è½½*Lite.AI.ToolKit*çš„æºç è¿›è¡Œæ„å»ºã€‚é¦–å…ˆï¼Œä½ éœ€è¦ä¸‹è½½(å¦‚æœæœ‰å®˜æ–¹ç¼–è¯‘å¥½çš„å‘è¡Œç‰ˆæœ¬çš„è¯)æˆ–ç¼–è¯‘*OpenCV* ã€*ONNXRuntime* å’Œå…¶ä»–ä½ éœ€è¦çš„æ¨ç†å¼•æ“ï¼Œå¦‚MNNã€NCNNã€TNNï¼Œç„¶åæŠŠå®ƒä»¬çš„å¤´æ–‡ä»¶åˆ†åˆ«æ”¾å…¥å„è‡ªå¯¹åº”çš„æ–‡ä»¶å¤¹ï¼Œæˆ–è€…ç›´æ¥ä½¿ç”¨æœ¬é¡¹ç›®æä¾›çš„å¤´æ–‡ä»¶ã€‚æœ¬é¡¹ç›®çš„ä¾èµ–åº“å¤´æ–‡ä»¶æ˜¯ç›´æ¥ä»ç›¸åº”çš„å®˜æ–¹åº“æ‹·è´è€Œæ¥çš„ï¼Œä½†ä¸åŒæ“ä½œç³»ç»Ÿä¸‹çš„åŠ¨æ€åº“éœ€è¦é‡æ–°ç¼–è¯‘æˆ–ä¸‹è½½ï¼ŒMacOSç”¨æˆ·å¯ä»¥ç›´æ¥ä½¿ç”¨æœ¬é¡¹ç›®æä¾›çš„å„ä¸ªä¾èµ–åº“çš„åŠ¨æ€åº“ã€‚
* *lite.ai.toolkit/opencv2*
  ```shell
    cp -r you-path-to-downloaded-or-built-opencv/include/opencv4/opencv2 lite.ai.toolkit/opencv2
  ```
* *lite.ai.toolkit/onnxruntime*
  ```shell
    cp -r you-path-to-downloaded-or-built-onnxruntime/include/onnxruntime lite.ai.toolkit/onnxruntime
  ```
* *lite.ai.toolkit/MNN*
  ```shell
    cp -r you-path-to-downloaded-or-built-MNN/include/MNN lite.ai.toolkit/MNN
  ```
* *lite.ai.toolkit/ncnn*
  ```shell
    cp -r you-path-to-downloaded-or-built-ncnn/include/ncnn lite.ai.toolkit/ncnn
  ```
* *lite.ai.toolkit/tnn*
  ```shell
    cp -r you-path-to-downloaded-or-built-TNN/include/tnn lite.ai.toolkit/tnn
  ```

ç„¶åæŠŠå„ä¸ªä¾èµ–åº“æ‹·è´åˆ°*lite.ai.toolkit/lib* æ–‡ä»¶å¤¹ã€‚ è¯·å‚è€ƒä¾èµ–åº“çš„ç¼–è¯‘æ–‡æ¡£[<sup>1</sup>](#lite.ai.toolkit-1)ã€‚
* *lite.ai.toolkit/lib*
  ```shell
    cp you-path-to-downloaded-or-built-opencv/lib/*opencv* lite.ai.toolkit/lib
    cp you-path-to-downloaded-or-built-onnxruntime/lib/*onnxruntime* lite.ai.toolkit/lib
    cp you-path-to-downloaded-or-built-MNN/lib/*MNN* lite.ai.toolkit/lib
    cp you-path-to-downloaded-or-built-ncnn/lib/*ncnn* lite.ai.toolkit/lib
    cp you-path-to-downloaded-or-built-TNN/lib/*TNN* lite.ai.toolkit/lib
  ```


* Windows: ä½ å¯ä»¥å‚è€ƒ[issue#6](https://github.com/DefTruth/lite.ai.toolkit/issues/6) ï¼Œè®¨è®ºäº†å¸¸è§çš„ç¼–è¯‘é—®é¢˜ã€‚
* Linux: å‚è€ƒMacOSä¸‹çš„ç¼–è¯‘ï¼Œæ›¿æ¢Linuxç‰ˆæœ¬çš„ä¾èµ–åº“å³å¯ã€‚Linuxä¸‹çš„å‘è¡Œç‰ˆæœ¬å°†ä¼šåœ¨è¿‘æœŸæ·»åŠ  ~ [issue#2](https://github.com/DefTruth/lite.ai.toolkit/issues/2)
* ä»¤äººå¼€å¿ƒçš„æ¶ˆæ¯!!! : ğŸš€ ä½ å¯ä»¥ç›´æ¥ä¸‹è½½æœ€æ–°çš„*ONNXRuntime*å®˜æ–¹æ„å»ºçš„åŠ¨æ€åº“ï¼ŒåŒ…å«Windows, Linux, MacOS and Armçš„ç‰ˆæœ¬!!! CPUå’ŒGPUçš„ç‰ˆæœ¬å‡å¯è·å¾—ã€‚ä¸éœ€è¦å†ä»æºç è¿›è¡Œç¼–è¯‘äº†ï¼Œniceã€‚å¯ä»¥ä»[v1.8.1](https://github.com/microsoft/onnxruntime/releases) ä¸‹è½½æœ€æ–°çš„åŠ¨æ€åº“. æˆ‘ç›®å‰åœ¨*Lite.AI.ToolKit*ä¸­ç”¨çš„æ˜¯1.7.0ï¼Œä½ å¯ä»¥ä»[v1.7.0](https://github.com/microsoft/onnxruntime/releases/tag/v1.7.0) ä¸‹è½½, ä½†1.8.1åº”è¯¥ä¹Ÿæ˜¯å¯è¡Œçš„ã€‚å¯¹äº*OpenCV*ï¼Œè¯·å°è¯•ä»æºç æ„å»º(Linux) æˆ–è€… ç›´æ¥ä»[OpenCV 4.5.3](https://github.com/opencv/opencv/releases) ä¸‹è½½å®˜æ–¹ç¼–è¯‘å¥½çš„åŠ¨æ€åº“(Windows). ç„¶åæŠŠå¤´æ–‡ä»¶å’Œä¾èµ–åº“æ”¾å…¥ä¸Šè¿°çš„æ–‡ä»¶å¤¹ä¸­.

* Windows GPU å…¼å®¹æ€§: è¯¦è§[issue#10](https://github.com/DefTruth/lite.ai.toolkit/issues/10).
* Linux GPU å…¼å®¹æ€§: è¯¦è§[issue#97](https://github.com/DefTruth/lite.ai.toolkit/issues/97).

</details>  

<details>
<summary>ğŸ”‘ï¸ å¦‚ä½•é“¾æ¥Lite.AI.ToolKitåŠ¨æ€åº“?</summary>  

* ä½ å¯å‚è€ƒä»¥ä¸‹çš„CMakeLists.txtè®¾ç½®æ¥é“¾æ¥åŠ¨æ€åº“.

```cmake
cmake_minimum_required(VERSION 3.17)
project(lite.ai.toolkit.demo)

set(CMAKE_CXX_STANDARD 11)

# setting up lite.ai.toolkit
set(LITE_AI_DIR ${CMAKE_SOURCE_DIR}/lite.ai.toolkit)
set(LITE_AI_INCLUDE_DIR ${LITE_AI_DIR}/include)
set(LITE_AI_LIBRARY_DIR ${LITE_AI_DIR}/lib)
include_directories(${LITE_AI_INCLUDE_DIR})
link_directories(${LITE_AI_LIBRARY_DIR})

set(OpenCV_LIBS
        opencv_highgui
        opencv_core
        opencv_imgcodecs
        opencv_imgproc
        opencv_video
        opencv_videoio
        )
# add your executable
set(EXECUTABLE_OUTPUT_PATH ${CMAKE_SOURCE_DIR}/examples/build)

add_executable(lite_rvm examples/test_lite_rvm.cpp)
target_link_libraries(lite_rvm
        lite.ai.toolkit
        onnxruntime
        MNN  # need, if built lite.ai.toolkit with ENABLE_MNN=ON,  default OFF
        ncnn # need, if built lite.ai.toolkit with ENABLE_NCNN=ON, default OFF 
        TNN  # need, if built lite.ai.toolkit with ENABLE_TNN=ON,  default OFF 
        ${OpenCV_LIBS})  # link lite.ai.toolkit & other libs.
```

```shell
cd ./build/lite.ai.toolkit/lib && otool -L liblite.ai.toolkit.0.0.1.dylib 
liblite.ai.toolkit.0.0.1.dylib:
        @rpath/liblite.ai.toolkit.0.0.1.dylib (compatibility version 0.0.1, current version 0.0.1)
        @rpath/libopencv_highgui.4.5.dylib (compatibility version 4.5.0, current version 4.5.2)
        @rpath/libonnxruntime.1.7.0.dylib (compatibility version 0.0.0, current version 1.7.0)
        ...
```


```shell
cd ../ && tree .
â”œâ”€â”€ bin
â”œâ”€â”€ include
â”‚Â Â  â”œâ”€â”€ lite
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ backend.h
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ config.h
â”‚Â Â  â”‚Â Â  â””â”€â”€ lite.h
â”‚Â Â  â””â”€â”€ ort
â””â”€â”€ lib
    â””â”€â”€ liblite.ai.toolkit.0.0.1.dylib
```
* è¿è¡Œå·²ç»ç¼–è¯‘å¥½çš„examples:
```shell
cd ./build/lite.ai.toolkit/bin && ls -lh | grep lite
-rwxr-xr-x  1 root  staff   301K Jun 26 23:10 liblite.ai.toolkit.0.0.1.dylib
...
-rwxr-xr-x  1 root  staff   196K Jun 26 23:10 lite_yolov4
-rwxr-xr-x  1 root  staff   196K Jun 26 23:10 lite_yolov5
...
```

```shell
./lite_yolov5
LITEORT_DEBUG LogId: ../../../hub/onnx/cv/yolov5s.onnx
=============== Input-Dims ==============
...
detected num_anchors: 25200
generate_bboxes num: 66
Default Version Detected Boxes Num: 5
```

ä¸ºäº†é“¾æ¥`lite.ai.toolkit`åŠ¨æ€åº“ï¼Œä½ éœ€è¦ç¡®ä¿`OpenCV` and `onnxruntime`ä¹Ÿè¢«æ­£ç¡®åœ°é“¾æ¥ã€‚ä½ å¯ä»¥åœ¨[CMakeLists.txt](https://github.com/DefTruth/RobustVideoMatting-ncnn-mnn-tnn-onnxruntime/blob/main/CMakeLists.txt) ä¸­æ‰¾åˆ°ä¸€ä¸ªç®€å•ä¸”å®Œæ•´çš„ï¼Œå…³äºå¦‚ä½•æ­£ç¡®åœ°é“¾æ¥Lite.AI.ToolKitåŠ¨æ€åº“çš„åº”ç”¨æ¡ˆä¾‹ã€‚

</details>


## 2. æ¨¡å‹ä¸‹è½½  
<div id="lite.ai.toolkit-2"></div>
<div id="lite.ai.toolkit-Model-Zoo"></div>

*Lite.AI.ToolKit* ç›®å‰åŒ…æ‹¬ *[70+](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md)* æµè¡Œçš„å¼€æºæ¨¡å‹ä»¥åŠ *[500+](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md)* æ–‡ä»¶ï¼Œå¤§éƒ¨åˆ†æ–‡ä»¶æ˜¯æˆ‘è‡ªå·±è½¬æ¢çš„ã€‚ä½ å¯ä»¥é€šè¿‡*lite::cv::Type::Class* è¯­æ³•è¿›è¡Œè°ƒç”¨ï¼Œå¦‚ *[lite::cv::detection::YoloV5](#lite.ai.toolkit-object-detection)*ã€‚æ›´å¤šçš„ç»†èŠ‚è§[Examples for Lite.AI.ToolKit](#lite.ai.toolkit-Examples-for-Lite.AI.ToolKit)ã€‚æ³¨æ„ï¼Œç”±äºGoogle Driver(15G)çš„å­˜å‚¨é™åˆ¶ï¼Œæˆ‘æ— æ³•ä¸Šä¼ æ‰€æœ‰çš„æ¨¡å‹æ–‡ä»¶ï¼Œå›½å†…çš„å°ä¼™ä¼´è¯·ä½¿ç”¨ç™¾åº¦äº‘ç›˜ã€‚

|File|Baidu Drive|Google Drive|Hub|  
|:---:|:---:|:---:|:---:|
|ONNX|[Baidu Drive](https://pan.baidu.com/s/1elUGcx7CZkkjEoYhTMwTRQ) code: 8gin|[Google Drive](https://drive.google.com/drive/folders/1p6uBcxGeyS1exc-T61vL8YRhwjYL4iD2?usp=sharing)|[ONNX Hub](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md)|    
|MNN|[Baidu Drive](https://pan.baidu.com/s/1KyO-bCYUv6qPq2M8BH_Okg) code: 9v63 |â”|[MNN Hub](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.mnn.md)|  
|NCNN|[Baidu Drive](https://pan.baidu.com/s/1hlnqyNsFbMseGFWscgVhgQ) code: sc7f |â”|[NCNN Hub](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.ncnn.md)|
|TNN|[Baidu Drive](https://pan.baidu.com/s/1lvM2YKyUbEc5HKVtqITpcw) code: 6o6k|â”|[TNN Hub](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.tnn.md)|

<details>
<summary> å‘½åç©ºé—´å’ŒLite.AI.ToolKitç®—æ³•æ¨¡å—çš„å¯¹åº”å…³ç³» </summary>  

### å‘½åç©ºé—´å’ŒLite.AI.ToolKitç®—æ³•æ¨¡å—çš„å¯¹åº”å…³ç³»

| Namepace                   | Details                                                      |
| :------------------------- | :----------------------------------------------------------- |
| *lite::cv::detection*      | Object Detection. one-stage and anchor-free detectors, YoloV5, YoloV4, SSD, etc. âœ… |
| *lite::cv::classification* | Image Classification. DensNet, ShuffleNet, ResNet, IBNNet, GhostNet, etc. âœ… |
| *lite::cv::faceid*         | Face Recognition. ArcFace, CosFace, CurricularFace, etc. â‡ï¸   |
| *lite::cv::face*           | Face Analysis. *detect*, *align*, *pose*, *attr*, etc. â‡ï¸    |
| *lite::cv::face::detect*   | Face Detection. UltraFace, RetinaFace, FaceBoxes, PyramidBox, etc. â‡ï¸ |
| *lite::cv::face::align*    | Face Alignment. PFLD(106), FaceLandmark1000(1000 landmarks), PRNet, etc. â‡ï¸ |
| *lite::cv::face::pose*     | Head Pose Estimation.  FSANet, etc. â‡ï¸                        |
| *lite::cv::face::attr*     | Face Attributes. Emotion, Age, Gender. EmotionFerPlus, VGG16Age, etc. â‡ï¸ |
| *lite::cv::segmentation*   | Object Segmentation. Such as FCN, DeepLabV3, etc. âš ï¸          |
| *lite::cv::style*          | Style Transfer. Contains neural style transfer now, such as FastStyleTransfer.  âš ï¸ |
| *lite::cv::matting*        | Image Matting. Object and Human matting.  âš ï¸                  |
| *lite::cv::colorization*   | Colorization. Make Gray image become RGB. âš ï¸                  |
| *lite::cv::resolution*     | Super Resolution.  âš ï¸                                         |


### Lite.AI.ToolKitçš„ç±»ä¸æƒé‡æ–‡ä»¶å¯¹åº”å…³ç³»è¯´æ˜

Lite.AI.ToolKitçš„ç±»ä¸æƒé‡æ–‡ä»¶å¯¹åº”å…³ç³»è¯´æ˜ï¼Œå¯ä»¥åœ¨[lite.ai.toolkit.hub.onnx.md](https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md) ä¸­æ‰¾åˆ°ã€‚æ¯”å¦‚, *lite::cv::detection::YoloV5* å’Œ *lite::cv::detection::YoloX* çš„æƒé‡æ–‡ä»¶ä¸ºï¼š


|             Class             | Pretrained ONNX Files |                   Rename or Converted From (Repo)                   | Size  |
| :---------------------------: | :-------------------: | :----------------------------------------------------: | :---: |
| *lite::cv::detection::YoloV5* |     yolov5l.onnx      | [yolov5](https://github.com/ultralytics/yolov5) (ğŸ”¥ğŸ”¥ğŸ’¥â†‘) | 188Mb |
| *lite::cv::detection::YoloV5* |     yolov5m.onnx      | [yolov5](https://github.com/ultralytics/yolov5) (ğŸ”¥ğŸ”¥ğŸ’¥â†‘) | 85Mb  |
| *lite::cv::detection::YoloV5* |     yolov5s.onnx      | [yolov5](https://github.com/ultralytics/yolov5) (ğŸ”¥ğŸ”¥ğŸ’¥â†‘) | 29Mb  |
| *lite::cv::detection::YoloV5* |     yolov5x.onnx      | [yolov5](https://github.com/ultralytics/yolov5) (ğŸ”¥ğŸ”¥ğŸ’¥â†‘) | 351Mb |
| *lite::cv::detection::YoloX* |     yolox_x.onnx      | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) (ğŸ”¥ğŸ”¥!!â†‘) | 378Mb |
| *lite::cv::detection::YoloX* |     yolox_l.onnx      | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) (ğŸ”¥ğŸ”¥!!â†‘) | 207Mb  |
| *lite::cv::detection::YoloX* |     yolox_m.onnx      | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) (ğŸ”¥ğŸ”¥!!â†‘) | 97Mb  |
| *lite::cv::detection::YoloX* |     yolox_s.onnx      | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) (ğŸ”¥ğŸ”¥!!â†‘) | 34Mb |
| *lite::cv::detection::YoloX* |     yolox_tiny.onnx      | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) (ğŸ”¥ğŸ”¥!!â†‘) | 19Mb |
| *lite::cv::detection::YoloX* |     yolox_nano.onnx      | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) (ğŸ”¥ğŸ”¥!!â†‘) | 3.5Mb |

è¿™æ„å‘³ç€ï¼Œä½ å¯ä»¥é€šè¿‡Lite.AI.ToolKitä¸­çš„åŒä¸€ä¸ªç±»ï¼Œæ ¹æ®ä½ çš„ä½¿ç”¨æƒ…å†µï¼ŒåŠ è½½ä»»æ„ä¸€ä¸ª`yolov5*.onnx`æˆ–`yolox_*.onnx`ï¼Œå¦‚ *YoloV5*, *YoloX*ç­‰.

```c++
auto *yolov5 = new lite::cv::detection::YoloV5("yolov5x.onnx");  // for server
auto *yolov5 = new lite::cv::detection::YoloV5("yolov5l.onnx"); 
auto *yolov5 = new lite::cv::detection::YoloV5("yolov5m.onnx");  
auto *yolov5 = new lite::cv::detection::YoloV5("yolov5s.onnx");  // for mobile device 
auto *yolox = new lite::cv::detection::YoloX("yolox_x.onnx");  
auto *yolox = new lite::cv::detection::YoloX("yolox_l.onnx");  
auto *yolox = new lite::cv::detection::YoloX("yolox_m.onnx");  
auto *yolox = new lite::cv::detection::YoloX("yolox_s.onnx");  
auto *yolox = new lite::cv::detection::YoloX("yolox_tiny.onnx");  
auto *yolox = new lite::cv::detection::YoloX("yolox_nano.onnx");  // 3.5Mb only !
```

</details>

## 3. åº”ç”¨æ¡ˆä¾‹

<div id="lite.ai.toolkit-Examples-for-Lite.AI.ToolKit"></div>

æ›´å¤šçš„åº”ç”¨æ¡ˆä¾‹è¯¦è§[examples](https://github.com/DefTruth/lite.ai.toolkit/tree/main/examples/lite/cv) ã€‚ç‚¹å‡» â–¶ï¸ å¯ä»¥çœ‹åˆ°è¯¥ä¸»é¢˜ä¸‹æ›´å¤šçš„æ¡ˆä¾‹ã€‚

<div id="lite.ai.toolkit-object-detection"></div>

#### æ¡ˆä¾‹0: ä½¿ç”¨[YoloV5](https://github.com/ultralytics/yolov5) è¿›è¡Œç›®æ ‡æ£€æµ‹ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/yolov5s.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_yolov5_1.jpg";
  std::string save_img_path = "../../../logs/test_lite_yolov5_1.jpg";

  auto *yolov5 = new lite::cv::detection::YoloV5(onnx_path); 
  std::vector<lite::types::Boxf> detected_boxes;
  cv::Mat img_bgr = cv::imread(test_img_path);
  yolov5->detect(img_bgr, detected_boxes);
  
  lite::utils::draw_boxes_inplace(img_bgr, detected_boxes);
  cv::imwrite(save_img_path, img_bgr);  
  
  delete yolov5;
}
```

è¾“å‡ºçš„ç»“æœæ˜¯:
<div align='center'>
  <img src='logs/test_lite_yolov5_1.jpg' height="256px">
  <img src='logs/test_lite_yolov5_2.jpg' height="256px">
</div>

æˆ–è€…ä½ å¯ä»¥ä½¿ç”¨æœ€æ–°çš„ ğŸ”¥ğŸ”¥ ! YOLO ç³»åˆ—æ£€æµ‹å™¨[YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) æˆ– [YoloR](https://github.com/WongKinYiu/yolor) ï¼Œå®ƒä»¬ä¼šè·å¾—æ¥è¿‘çš„ç»“æœã€‚

æ›´å¤šå¯ç”¨çš„é€šç”¨ç›®æ ‡æ£€æµ‹å™¨ï¼ˆ80ç±»ã€COCOï¼‰:  
```c++
auto *detector = new lite::cv::detection::YoloX(onnx_path);  // Newest YOLO detector !!! 2021-07
auto *detector = new lite::cv::detection::YoloV4(onnx_path); 
auto *detector = new lite::cv::detection::YoloV3(onnx_path); 
auto *detector = new lite::cv::detection::TinyYoloV3(onnx_path); 
auto *detector = new lite::cv::detection::SSD(onnx_path); 
auto *detector = new lite::cv::detection::YoloV5(onnx_path); 
auto *detector = new lite::cv::detection::YoloR(onnx_path);  // Newest YOLO detector !!! 2021-05
auto *detector = new lite::cv::detection::TinyYoloV4VOC(onnx_path); 
auto *detector = new lite::cv::detection::TinyYoloV4COCO(onnx_path); 
auto *detector = new lite::cv::detection::ScaledYoloV4(onnx_path); 
auto *detector = new lite::cv::detection::EfficientDet(onnx_path); 
auto *detector = new lite::cv::detection::EfficientDetD7(onnx_path); 
auto *detector = new lite::cv::detection::EfficientDetD8(onnx_path); 
auto *detector = new lite::cv::detection::YOLOP(onnx_path);
auto *detector = new lite::cv::detection::NanoDet(onnx_path); // Super fast and tiny!
auto *detector = new lite::cv::detection::NanoDetEfficientNetLite(onnx_path); // Super fast and tiny!
```

****

<div id="lite.ai.toolkit-matting"></div>  

#### æ¡ˆä¾‹1: ä½¿ç”¨[RobustVideoMatting2021ğŸ”¥ğŸ”¥ğŸ”¥](https://github.com/PeterL1n/RobustVideoMatting) è¿›è¡Œè§†é¢‘æŠ å›¾ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚

```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/rvm_mobilenetv3_fp32.onnx";
  std::string video_path = "../../../examples/lite/resources/test_lite_rvm_0.mp4";
  std::string output_path = "../../../logs/test_lite_rvm_0.mp4";
  
  auto *rvm = new lite::cv::matting::RobustVideoMatting(onnx_path, 16); // 16 threads
  std::vector<lite::types::MattingContent> contents;
  
  // 1. video matting.
  rvm->detect_video(video_path, output_path, contents, false, 0.4f);
  
  delete rvm;
}
```
è¾“å‡ºçš„ç»“æœæ˜¯:

<div align='center'>
  <img src='docs/resources/interviewi.gif' height="80px" width="150px">
  <img src='docs/resources/interview.gif' height="80px" width="150px">  
  <img src='docs/resources/dance3i.gif' height="80px" width="150px">
  <img src='docs/resources/dance3.gif' height="80px" width="150px">
  <br>
  <img src='docs/resources/teslai.gif' height="80px" width="150px">
  <img src='docs/resources/tesla.gif' height="80px" width="150px">  
  <img src='docs/resources/b5i.gif' height="80px" width="150px">
  <img src='docs/resources/b5.gif' height="80px" width="150px">
</div>

æ›´å¤šå¯ç”¨çš„æŠ å›¾æ¨¡å‹ï¼ˆå›¾ç‰‡æŠ å›¾ã€è§†é¢‘æŠ å›¾ã€trimap/mask-freeã€trimap/mask-basedï¼‰:
```c++
auto *matting = new lite::cv::matting::RobustVideoMatting:(onnx_path);  //  WACV 2022.
auto *matting = new lite::cv::matting::MGMatting(onnx_path); // CVPR 2021
```

****

<div id="lite.ai.toolkit-face-alignment"></div>

#### æ¡ˆä¾‹2: ä½¿ç”¨[FaceLandmarks1000](https://github.com/Single430/FaceLandmark1000) è¿›è¡Œäººè„¸1000å…³é”®ç‚¹æ£€æµ‹ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/FaceLandmark1000.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_face_landmarks_0.png";
  std::string save_img_path = "../../../logs/test_lite_face_landmarks_1000.jpg";
    
  auto *face_landmarks_1000 = new lite::cv::face::align::FaceLandmark1000(onnx_path);

  lite::types::Landmarks landmarks;
  cv::Mat img_bgr = cv::imread(test_img_path);
  face_landmarks_1000->detect(img_bgr, landmarks);
  lite::utils::draw_landmarks_inplace(img_bgr, landmarks);
  cv::imwrite(save_img_path, img_bgr);
  
  delete face_landmarks_1000;
}
```
è¾“å‡ºçš„ç»“æœæ˜¯:
<div align='center'>
  <img src='logs/test_lite_face_landmarks_1000.jpg' height="224px" width="224px">
  <img src='logs/test_lite_face_landmarks_1000_2.jpg' height="224px" width="224px">
  <img src='logs/test_lite_face_landmarks_1000_0.jpg' height="224px" width="224px">
</div>    

æ›´å¤šå¯ç”¨çš„äººè„¸å…³é”®ç‚¹æ£€æµ‹å™¨ï¼ˆ68ç‚¹ã€98ç‚¹ã€106ç‚¹ã€1000ç‚¹ï¼‰:
```c++
auto *align = new lite::cv::face::align::PFLD(onnx_path);  // 106 landmarks, 1.0Mb only!
auto *align = new lite::cv::face::align::PFLD98(onnx_path);  // 98 landmarks, 4.8Mb only!
auto *align = new lite::cv::face::align::PFLD68(onnx_path);  // 68 landmarks, 2.8Mb only!
auto *align = new lite::cv::face::align::MobileNetV268(onnx_path);  // 68 landmarks, 9.4Mb only!
auto *align = new lite::cv::face::align::MobileNetV2SE68(onnx_path);  // 68 landmarks, 11Mb only!
auto *align = new lite::cv::face::align::FaceLandmark1000(onnx_path);  // 1000 landmarks, 2.0Mb only!
```

****  

<div id="lite.ai.toolkit-colorization"></div>

#### æ¡ˆä¾‹3: ä½¿ç”¨[colorization](https://github.com/richzhang/colorization) è¿›è¡Œå›¾åƒç€è‰²ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/eccv16-colorizer.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_colorizer_1.jpg";
  std::string save_img_path = "../../../logs/test_lite_eccv16_colorizer_1.jpg";
  
  auto *colorizer = new lite::cv::colorization::Colorizer(onnx_path);
  
  cv::Mat img_bgr = cv::imread(test_img_path);
  lite::types::ColorizeContent colorize_content;
  colorizer->detect(img_bgr, colorize_content);
  
  if (colorize_content.flag) cv::imwrite(save_img_path, colorize_content.mat);
  delete colorizer;
}
```
è¾“å‡ºçš„ç»“æœæ˜¯:

<div align='center'>
  <img src='examples/lite/resources/test_lite_colorizer_1.jpg' height="224px" width="224px">
  <img src='examples/lite/resources/test_lite_colorizer_2.jpg' height="224px" width="224px">
  <img src='examples/lite/resources/test_lite_colorizer_3.jpg' height="224px" width="224px">  
  <br> 
  <img src='logs/test_lite_siggraph17_colorizer_1.jpg' height="224px" width="224px">
  <img src='logs/test_lite_siggraph17_colorizer_2.jpg' height="224px" width="224px">
  <img src='logs/test_lite_siggraph17_colorizer_3.jpg' height="224px" width="224px">
</div>    

æ›´å¤šå¯ç”¨çš„ç€è‰²å™¨æ¨¡å‹ï¼ˆç°åº¦å›¾è½¬å½©è‰²å›¾ï¼‰:
```c++
auto *colorizer = new lite::cv::colorization::Colorizer(onnx_path);
```

****

<div id="lite.ai.toolkit-face-recognition"></div>  

#### æ¡ˆä¾‹4: ä½¿ç”¨[ArcFace](https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch) è¿›è¡Œäººè„¸è¯†åˆ«ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚

```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/ms1mv3_arcface_r100.onnx";
  std::string test_img_path0 = "../../../examples/lite/resources/test_lite_faceid_0.png";
  std::string test_img_path1 = "../../../examples/lite/resources/test_lite_faceid_1.png";
  std::string test_img_path2 = "../../../examples/lite/resources/test_lite_faceid_2.png";

  auto *glint_arcface = new lite::cv::faceid::GlintArcFace(onnx_path);

  lite::types::FaceContent face_content0, face_content1, face_content2;
  cv::Mat img_bgr0 = cv::imread(test_img_path0);
  cv::Mat img_bgr1 = cv::imread(test_img_path1);
  cv::Mat img_bgr2 = cv::imread(test_img_path2);
  glint_arcface->detect(img_bgr0, face_content0);
  glint_arcface->detect(img_bgr1, face_content1);
  glint_arcface->detect(img_bgr2, face_content2);

  if (face_content0.flag && face_content1.flag && face_content2.flag)
  {
    float sim01 = lite::utils::math::cosine_similarity<float>(
        face_content0.embedding, face_content1.embedding);
    float sim02 = lite::utils::math::cosine_similarity<float>(
        face_content0.embedding, face_content2.embedding);
    std::cout << "Detected Sim01: " << sim  << " Sim02: " << sim02 << std::endl;
  }

  delete glint_arcface;
}
```

è¾“å‡ºçš„ç»“æœæ˜¯:
<div align='center'>
  <img src='examples/lite/resources/test_lite_arcface_resnet_0.png' height="224px" width="224px">
  <img src='examples/lite/resources/test_lite_arcface_resnet_1.png' height="224px" width="224px">
  <img src='examples/lite/resources/test_lite_arcface_resnet_2.png' height="224px" width="224px">
</div>  

> Detected Sim01: 0.721159  Sim02: -0.0626267

æ›´å¤šå¯ç”¨çš„äººè„¸è¯†åˆ«æ¨¡å‹ï¼ˆäººè„¸ç‰¹å¾æå–ï¼‰:
```c++
auto *recognition = new lite::cv::faceid::GlintCosFace(onnx_path);  // DeepGlint(insightface)
auto *recognition = new lite::cv::faceid::GlintArcFace(onnx_path);  // DeepGlint(insightface)
auto *recognition = new lite::cv::faceid::GlintPartialFC(onnx_path); // DeepGlint(insightface)
auto *recognition = new lite::cv::faceid::FaceNet(onnx_path);
auto *recognition = new lite::cv::faceid::FocalArcFace(onnx_path);
auto *recognition = new lite::cv::faceid::FocalAsiaArcFace(onnx_path);
auto *recognition = new lite::cv::faceid::TencentCurricularFace(onnx_path); // Tencent(TFace)
auto *recognition = new lite::cv::faceid::TencentCifpFace(onnx_path); // Tencent(TFace)
auto *recognition = new lite::cv::faceid::CenterLossFace(onnx_path);
auto *recognition = new lite::cv::faceid::SphereFace(onnx_path);
auto *recognition = new lite::cv::faceid::PoseRobustFace(onnx_path);
auto *recognition = new lite::cv::faceid::NaivePoseRobustFace(onnx_path);
auto *recognition = new lite::cv::faceid::MobileFaceNet(onnx_path); // 3.8Mb only !
auto *recognition = new lite::cv::faceid::CavaGhostArcFace(onnx_path);
auto *recognition = new lite::cv::faceid::CavaCombinedFace(onnx_path);
auto *recognition = new lite::cv::faceid::MobileSEFocalFace(onnx_path); // 4.5Mb only !
```

****

<div id="lite.ai.toolkit-face-detection"></div>

#### æ¡ˆä¾‹5: ä½¿ç”¨[UltraFace](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) è¿›è¡Œäººè„¸æ£€æµ‹ã€‚è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/ultraface-rfb-640.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_ultraface.jpg";
  std::string save_img_path = "../../../logs/test_lite_ultraface.jpg";

  auto *ultraface = new lite::cv::face::detect::UltraFace(onnx_path);

  std::vector<lite::types::Boxf> detected_boxes;
  cv::Mat img_bgr = cv::imread(test_img_path);
  ultraface->detect(img_bgr, detected_boxes);
  lite::utils::draw_boxes_inplace(img_bgr, detected_boxes);
  cv::imwrite(save_img_path, img_bgr);

  delete ultraface;
}
```
è¾“å‡ºçš„ç»“æœæ˜¯:
<div align='center'>
  <img src='logs/test_lite_ultraface.jpg' height="224px" width="224px">
  <img src='logs/test_lite_ultraface_2.jpg' height="224px" width="224px">
  <img src='logs/test_lite_ultraface_3.jpg' height="224px" width="224px">
</div>  

æ›´å¤šå¯ç”¨çš„äººè„¸æ£€æµ‹å™¨ï¼ˆè½»é‡çº§äººè„¸æ£€æµ‹å™¨ï¼‰:
```c++
auto *detector = new lite::face::detect::UltraFace(onnx_path);  // 1.1Mb only !
auto *detector = new lite::face::detect::FaceBoxes(onnx_path);  // 3.8Mb only ! 
auto *detector = new lite::face::detect::RetinaFace(onnx_path);  // 1.6Mb only ! CVPR2020
```

<div id="lite.ai.toolkit-segmentation"></div>  

#### æ¡ˆä¾‹6: ä½¿ç”¨ [DeepLabV3ResNet101](https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/) è¿›è¡Œè¯­ä¹‰åˆ†å‰². è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/deeplabv3_resnet101_coco.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_deeplabv3_resnet101.png";
  std::string save_img_path = "../../../logs/test_lite_deeplabv3_resnet101.jpg";

  auto *deeplabv3_resnet101 = new lite::cv::segmentation::DeepLabV3ResNet101(onnx_path, 16); // 16 threads

  lite::types::SegmentContent content;
  cv::Mat img_bgr = cv::imread(test_img_path);
  deeplabv3_resnet101->detect(img_bgr, content);

  if (content.flag)
  {
    cv::Mat out_img;
    cv::addWeighted(img_bgr, 0.2, content.color_mat, 0.8, 0., out_img);
    cv::imwrite(save_img_path, out_img);
    if (!content.names_map.empty())
    {
      for (auto it = content.names_map.begin(); it != content.names_map.end(); ++it)
      {
        std::cout << it->first << " Name: " << it->second << std::endl;
      }
    }
  }
  delete deeplabv3_resnet101;
}
```

è¾“å‡ºçš„ç»“æœæ˜¯:  
<div align='center'>
  <img src='examples/lite/resources/test_lite_deeplabv3_resnet101.png' height="256px">
  <img src='logs/test_lite_deeplabv3_resnet101.jpg' height="256px">
</div> 

æ›´å¤šå¯ç”¨çš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼ˆäººåƒåˆ†å‰²ã€å®ä¾‹åˆ†å‰²ï¼‰:
```c++
auto *segment = new lite::cv::segmentation::FCNResNet101(onnx_path);
auto *segment = new lite::cv::segmentation::DeepLabV3ResNet101(onnx_path);
```

<div id="lite.ai.toolkit-face-attributes-analysis"></div>    

#### æ¡ˆä¾‹7: ä½¿ç”¨ [SSRNet](https://github.com/oukohou/SSR_Net_Pytorch) è¿›è¡Œå¹´é¾„ä¼°è®¡. è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/ssrnet.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_ssrnet.jpg";
  std::string save_img_path = "../../../logs/test_lite_ssrnet.jpg";

  lite::cv::face::attr::SSRNet *ssrnet = new lite::cv::face::attr::SSRNet(onnx_path);

  lite::types::Age age;
  cv::Mat img_bgr = cv::imread(test_img_path);
  ssrnet->detect(img_bgr, age);
  lite::utils::draw_age_inplace(img_bgr, age);
  cv::imwrite(save_img_path, img_bgr);
  std::cout << "Default Version Done! Detected SSRNet Age: " << age.age << std::endl;

  delete ssrnet;
}
```
è¾“å‡ºçš„ç»“æœæ˜¯:  
<div align='center'>
  <img src='logs/test_lite_ssrnet.jpg' height="224px" width="224px">
  <img src='logs/test_lite_gender_googlenet.jpg' height="224px" width="224px">
  <img src='logs/test_lite_emotion_ferplus.jpg' height="224px" width="224px">
</div>    

æ›´å¤šå¯ç”¨çš„äººè„¸å±æ€§è¯†åˆ«æ¨¡å‹ï¼ˆæ€§åˆ«ã€å¹´é¾„ã€æƒ…ç»ªï¼‰:
```c++
auto *attribute = new lite::cv::face::attr::AgeGoogleNet(onnx_path);  
auto *attribute = new lite::cv::face::attr::GenderGoogleNet(onnx_path); 
auto *attribute = new lite::cv::face::attr::EmotionFerPlus(onnx_path);
auto *attribute = new lite::cv::face::attr::VGG16Age(onnx_path);
auto *attribute = new lite::cv::face::attr::VGG16Gender(onnx_path);
auto *attribute = new lite::cv::face::attr::EfficientEmotion7(onnx_path); // 7 emotions, 15Mb only!
auto *attribute = new lite::cv::face::attr::EfficientEmotion8(onnx_path); // 8 emotions, 15Mb only!
auto *attribute = new lite::cv::face::attr::MobileEmotion7(onnx_path); // 7 emotions, 13Mb only!
auto *attribute = new lite::cv::face::attr::ReXNetEmotion7(onnx_path); // 7 emotions
auto *attribute = new lite::cv::face::attr::SSRNet(onnx_path); // age estimation, 190kb only!!!
```


<div id="lite.ai.toolkit-image-classification"></div>   

#### æ¡ˆä¾‹8: ä½¿ç”¨ [DenseNet](https://pytorch.org/hub/pytorch_vision_densenet/) è¿›è¡Œå›¾ç‰‡1000åˆ†ç±». è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/densenet121.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_densenet.jpg";

  auto *densenet = new lite::cv::classification::DenseNet(onnx_path);

  lite::types::ImageNetContent content;
  cv::Mat img_bgr = cv::imread(test_img_path);
  densenet->detect(img_bgr, content);
  if (content.flag)
  {
    const unsigned int top_k = content.scores.size();
    if (top_k > 0)
    {
      for (unsigned int i = 0; i < top_k; ++i)
        std::cout << i + 1
                  << ": " << content.labels.at(i)
                  << ": " << content.texts.at(i)
                  << ": " << content.scores.at(i)
                  << std::endl;
    }
  }
  delete densenet;
}
```

è¾“å‡ºçš„ç»“æœæ˜¯:  
<div align='center'>
  <img src='examples/lite/resources/test_lite_densenet.jpg' height="224px" width="224px">
  <img src='logs/test_lite_densenet.png' height="224px" width="500px">
</div>  

æ›´å¤šå¯ç”¨çš„å›¾åƒåˆ†ç±»æ¨¡å‹ï¼ˆ1000ç±»ï¼‰:
```c++
auto *classifier = new lite::cv::classification::EfficientNetLite4(onnx_path);  
auto *classifier = new lite::cv::classification::ShuffleNetV2(onnx_path); // 8.7Mb only!
auto *classifier = new lite::cv::classification::GhostNet(onnx_path);
auto *classifier = new lite::cv::classification::HdrDNet(onnx_path);
auto *classifier = new lite::cv::classification::IBNNet(onnx_path);
auto *classifier = new lite::cv::classification::MobileNetV2(onnx_path); // 13Mb only!
auto *classifier = new lite::cv::classification::ResNet(onnx_path); 
auto *classifier = new lite::cv::classification::ResNeXt(onnx_path);
```


<div id="lite.ai.toolkit-head-pose-estimation"></div>   

#### æ¡ˆä¾‹9: ä½¿ç”¨ [FSANet](https://github.com/omasaht/headpose-fsanet-pytorch) è¿›è¡Œå¤´éƒ¨å§¿æ€è¯†åˆ«. è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚

```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/fsanet-var.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_fsanet.jpg";
  std::string save_img_path = "../../../logs/test_lite_fsanet.jpg";

  auto *fsanet = new lite::cv::face::pose::FSANet(onnx_path);
  cv::Mat img_bgr = cv::imread(test_img_path);
  lite::types::EulerAngles euler_angles;
  fsanet->detect(img_bgr, euler_angles);
  
  if (euler_angles.flag)
  {
    lite::utils::draw_axis_inplace(img_bgr, euler_angles);
    cv::imwrite(save_img_path, img_bgr);
    std::cout << "yaw:" << euler_angles.yaw << " pitch:" << euler_angles.pitch << " row:" << euler_angles.roll << std::endl;
  }
  delete fsanet;
}
```

è¾“å‡ºçš„ç»“æœæ˜¯:  
<div align='center'>
  <img src='logs/test_lite_fsanet.jpg' height="224px" width="224px">
  <img src='logs/test_lite_fsanet_2.jpg' height="224px" width="224px">
  <img src='logs/test_lite_fsanet_3.jpg' height="224px" width="224px">
</div>  

æ›´å¤šå¯ç”¨çš„å¤´éƒ¨å§¿æ€è¯†åˆ«æ¨¡å‹ï¼ˆæ¬§æ‹‰è§’ã€yawã€pitchã€rollï¼‰:
```c++
auto *pose = new lite::cv::face::pose::FSANet(onnx_path); // 1.2Mb only!
```

<div id="lite.ai.toolkit-style-transfer"></div>  

#### æ¡ˆä¾‹10: ä½¿ç”¨ [FastStyleTransfer](https://github.com/onnx/models/tree/master/vision/style_transfer/fast_neural_style) è¿›è¡Œé£æ ¼è¿ç§». è¯·ä»Model-Zoo[<sup>2</sup>](#lite.ai.toolkit-2) ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
```c++
#include "lite/lite.h"

static void test_default()
{
  std::string onnx_path = "../../../hub/onnx/cv/style-candy-8.onnx";
  std::string test_img_path = "../../../examples/lite/resources/test_lite_fast_style_transfer.jpg";
  std::string save_img_path = "../../../logs/test_lite_fast_style_transfer_candy.jpg";
  
  auto *fast_style_transfer = new lite::cv::style::FastStyleTransfer(onnx_path);
 
  lite::types::StyleContent style_content;
  cv::Mat img_bgr = cv::imread(test_img_path);
  fast_style_transfer->detect(img_bgr, style_content);

  if (style_content.flag) cv::imwrite(save_img_path, style_content.mat);
  delete fast_style_transfer;
}
```
è¾“å‡ºçš„ç»“æœæ˜¯:  

<div align='center'>
  <img src='examples/lite/resources/test_lite_fast_style_transfer.jpg' height="224px">
  <img src='logs/test_lite_fast_style_transfer_candy.jpg' height="224px">
  <img src='logs/test_lite_fast_style_transfer_mosaic.jpg' height="224px">  
  <br> 
  <img src='logs/test_lite_fast_style_transfer_pointilism.jpg' height="224px">
  <img src='logs/test_lite_fast_style_transfer_rain_princes.jpg' height="224px">
  <img src='logs/test_lite_fast_style_transfer_udnie.jpg' height="224px">
</div>

æ›´å¤šå¯ç”¨çš„é£æ ¼è¿ç§»æ¨¡å‹ï¼ˆè‡ªç„¶é£æ ¼è¿ç§»ã€å…¶ä»–ï¼‰:
```c++
auto *transfer = new lite::cv::style::FastStyleTransfer(onnx_path); // 6.4Mb only
```

## 4. å¼€æºåè®®

<div id="lite.ai.toolkit-License"></div>

[Lite.AI.ToolKit](#lite.ai.toolkit-Introduction) çš„ä»£ç é‡‡ç”¨GPL-3.0åè®®ã€‚


## 5. å¼•ç”¨å‚è€ƒ

<div id="lite.ai.toolkit-References"></div>

æœ¬é¡¹ç›®å‚è€ƒäº†ä»¥ä¸‹å¼€æºé¡¹ç›®ã€‚

* [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) (ğŸ”¥ğŸ”¥ğŸ”¥new!!â†‘)
* [nanodet](https://github.com/RangiLyu/nanodet) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) (ğŸ”¥ğŸ”¥ğŸ”¥new!!â†‘)
* [YOLOP](https://github.com/hustvl/YOLOP) (ğŸ”¥ğŸ”¥new!!â†‘)
* [YOLOR](https://github.com/WongKinYiu/yolor) (ğŸ”¥ğŸ”¥new!!â†‘)
* [ScaledYOLOv4](https://github.com/WongKinYiu/ScaledYOLOv4) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [insightface](https://github.com/deepinsight/insightface) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [yolov5](https://github.com/ultralytics/yolov5) (ğŸ”¥ğŸ”¥ğŸ’¥â†‘)
* [TFace](https://github.com/Tencent/TFace) (ğŸ”¥ğŸ”¥â†‘)
* [YOLOv4-pytorch](https://github.com/argusswift/YOLOv4-pytorch) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [Ultra-Light-Fast-Generic-Face-Detector-1MB](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)

<details>
<summary> å±•å¼€æ›´å¤šå¼•ç”¨å‚è€ƒ </summary>  

* [headpose-fsanet-pytorch](https://github.com/omasaht/headpose-fsanet-pytorch) (ğŸ”¥â†‘)
* [pfld_106_face_landmarks](https://github.com/Hsintao/pfld_106_face_landmarks) (ğŸ”¥ğŸ”¥â†‘)
* [onnx-models](https://github.com/onnx/models) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [SSR_Net_Pytorch](https://github.com/oukohou/SSR_Net_Pytorch) (ğŸ”¥â†‘)
* [colorization](https://github.com/richzhang/colorization) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [SUB_PIXEL_CNN](https://github.com/niazwazir/SUB_PIXEL_CNN) (ğŸ”¥â†‘)
* [torchvision](https://github.com/pytorch/vision) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [facenet-pytorch](https://github.com/timesler/facenet-pytorch) (ğŸ”¥â†‘)
* [face.evoLVe.PyTorch](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [center-loss.pytorch](https://github.com/louis-she/center-loss.pytorch) (ğŸ”¥ğŸ”¥â†‘)
* [sphereface_pytorch](https://github.com/clcarwin/sphereface_pytorch) (ğŸ”¥ğŸ”¥â†‘)
* [DREAM](https://github.com/penincillin/DREAM) (ğŸ”¥ğŸ”¥â†‘)
* [MobileFaceNet_Pytorch](https://github.com/Xiaoccer/MobileFaceNet_Pytorch) (ğŸ”¥ğŸ”¥â†‘)
* [cavaface.pytorch](https://github.com/cavalleria/cavaface.pytorch) (ğŸ”¥ğŸ”¥â†‘)
* [CurricularFace](https://github.com/HuangYG123/CurricularFace) (ğŸ”¥ğŸ”¥â†‘)
* [face-emotion-recognition](https://github.com/HSE-asavchenko/face-emotion-recognition) (ğŸ”¥â†‘)
* [face_recognition.pytorch](https://github.com/grib0ed0v/face_recognition.pytorch) (ğŸ”¥ğŸ”¥â†‘)
* [PFLD-pytorch](https://github.com/polarisZhao/PFLD-pytorch) (ğŸ”¥ğŸ”¥â†‘)
* [pytorch_face_landmark](https://github.com/cunjian/pytorch_face_landmark) (ğŸ”¥ğŸ”¥â†‘)
* [FaceLandmark1000](https://github.com/Single430/FaceLandmark1000) (ğŸ”¥ğŸ”¥â†‘)
* [Pytorch_Retinaface](https://github.com/biubug6/Pytorch_Retinaface) (ğŸ”¥ğŸ”¥ğŸ”¥â†‘)
* [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch) (ğŸ”¥ğŸ”¥â†‘)

</details>    


## 6. ç¼–è¯‘é€‰é¡¹
æœªæ¥ä¼šå¢åŠ ä¸€äº›æ¨¡å‹çš„[MNN](https://github.com/alibaba/MNN) ã€[NCNN](https://github.com/Tencent/ncnn) å’Œ [TNN](https://github.com/Tencent/TNN) æ”¯æŒï¼Œä½†ç”±äºç®—å­å…¼å®¹ç­‰åŸå› ï¼Œä¹Ÿæ— æ³•ç¡®ä¿æ‰€æœ‰è¢«[ONNXRuntime C++](https://github.com/microsoft/onnxruntime) æ”¯æŒçš„æ¨¡å‹èƒ½å¤Ÿåœ¨[MNN](https://github.com/alibaba/MNN) ã€[NCNN](https://github.com/Tencent/ncnn) å’Œ [TNN](https://github.com/Tencent/TNN) ä¸‹è·‘é€šã€‚æ‰€ä»¥ï¼Œå¦‚æœæ‚¨æƒ³ä½¿ç”¨æœ¬é¡¹ç›®æ”¯æŒçš„æ‰€æœ‰æ¨¡å‹ï¼Œå¹¶ä¸”ä¸åœ¨æ„*1~2ms*çš„æ€§èƒ½å·®è·çš„è¯ï¼Œè¯·ä½¿ç”¨ONNXRuntimeç‰ˆæœ¬çš„å®ç°ã€‚[ONNXRuntime](https://github.com/microsoft/onnxruntime) æ˜¯æœ¬ä»“åº“é»˜è®¤çš„æ¨ç†å¼•æ“ã€‚ä½†æ˜¯å¦‚æœä½ ç¡®å®å¸Œæœ›ç¼–è¯‘æ”¯æŒ[MNN](https://github.com/alibaba/MNN) ã€[NCNN](https://github.com/Tencent/ncnn) å’Œ [TNN](https://github.com/Tencent/TNN) æ”¯æŒçš„Lite.AI.ToolKitğŸ…ğŸ…åŠ¨æ€åº“ï¼Œä½ å¯ä»¥æŒ‰ç…§ä»¥ä¸‹çš„æ­¥éª¤è¿›è¡Œè®¾ç½®ã€‚

* åœ¨`build.sh`ä¸­æ·»åŠ `DENABLE_MNN=ON` ã€`DENABLE_NCNN=ON` æˆ– `DENABLE_TNN=ON`ï¼Œæ¯”å¦‚
```shell
cd build && cmake \
  -DCMAKE_BUILD_TYPE=MinSizeRel \
  -DINCLUDE_OPENCV=ON \   # æ˜¯å¦æ‰“åŒ…OpenCVè¿›lite.ai.toolkitï¼Œé»˜è®¤ONï¼›å¦åˆ™ï¼Œä½ éœ€è¦å•ç‹¬è®¾ç½®OpenCV
  -DENABLE_MNN=ON \       # æ˜¯å¦ç¼–è¯‘MNNç‰ˆæœ¬çš„æ¨¡å‹ï¼Œ é»˜è®¤OFFï¼Œç›®å‰åªæ”¯æŒéƒ¨åˆ†æ¨¡å‹
  -DENABLE_NCNN=OFF \     # æ˜¯å¦ç¼–è¯‘NCNNç‰ˆæœ¬çš„æ¨¡å‹ï¼Œé»˜è®¤OFFï¼Œç›®å‰åªæ”¯æŒéƒ¨åˆ†æ¨¡å‹
  -DENABLE_TNN=OFF \      # æ˜¯å¦ç¼–è¯‘TNNç‰ˆæœ¬çš„æ¨¡å‹ï¼Œ é»˜è®¤OFFï¼Œç›®å‰åªæ”¯æŒéƒ¨åˆ†æ¨¡å‹
  .. && make -j8
```  
* ä½¿ç”¨MNNã€NCNNæˆ–TNNç‰ˆæœ¬çš„æ¥å£ï¼Œè¯¦è§æ¡ˆä¾‹[demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/examples/lite/cv/test_lite_nanodet.cpp) ï¼Œæ¯”å¦‚
```C++
auto *nanodet = new lite::mnn::cv::detection::NanoDet(mnn_path);
auto *nanodet = new lite::tnn::cv::detection::NanoDet(proto_path, model_path);
auto *nanodet = new lite::ncnn::cv::detection::NanoDet(param_path, bin_path);
```

## 7. å¼•ç”¨

å¦‚æœæ‚¨åœ¨è‡ªå·±çš„é¡¹ç›®ä¸­ä½¿ç”¨äº†*Lite.AI.ToolKit*ï¼Œå¯è€ƒè™‘æŒ‰ä»¥ä¸‹æ–¹å¼è¿›è¡Œå¼•ç”¨ã€‚
```BibTeX
@misc{lite.ai.toolkit2021,
  title={lite.ai.toolkit: A lite C++ toolkit of awesome AI models.},
  url={https://github.com/DefTruth/lite.ai.toolkit},
  note={Open-source software available at https://github.com/DefTruth/lite.ai.toolkit},
  author={Yan Jun},
  year={2021}
}
```  

<p align="center"> è‹¥æ˜¯æœ‰ç”¨ï¼Œâ¤ï¸ä¸å¦¨ç»™ä¸ªâ­ï¸ğŸŒŸæ”¯æŒä¸€ä¸‹å§ï¼Œæ„Ÿè°¢æ”¯æŒ~  </p>
