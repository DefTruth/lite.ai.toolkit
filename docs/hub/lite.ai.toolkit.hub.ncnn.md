# Lite.AI.ToolKit.Hub.NCNN  

You can download all the pretrained models files of NCNN format from ([Baidu Drive](https://pan.baidu.com/s/1hlnqyNsFbMseGFWscgVhgQ) code: sc7f)

## Object Detection.

<div id="lite.ai.toolkit.hub.ncnn-object-detection"></div>

|                 Class                 |      Pretrained NCNN Files      |              Rename or Converted From (Repo)              | Size  |
| :-----------------------------------: | :-----------------------------: | :-------------------------------------------------------: | :---: |
|     *lite::ncnn::cv::detection::YoloV5*     |          yolov5l.opt.param&bin            |      [yolov5](https://github.com/ultralytics/yolov5)      | 188Mb |
|     *lite::ncnn::cv::detection::YoloV5*     |          yolov5m.opt.param&bin            |      [yolov5](https://github.com/ultralytics/yolov5)      | 85Mb  |
|     *lite::ncnn::cv::detection::YoloV5*     |          yolov5s.opt.param&bin            |      [yolov5](https://github.com/ultralytics/yolov5)      | 29Mb  |
|     *lite::ncnn::cv::detection::YoloV5*     |          yolov5x.opt.param&bin            |      [yolov5](https://github.com/ultralytics/yolov5)      | 351Mb | 
|     *lite::ncnn::cv::detection::YoloX*      |          yolox_x.opt.param&bin           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 378Mb |
|     *lite::ncnn::cv::detection::YoloX*      |          yolox_l.opt.param&bin           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 207Mb |
|     *lite::ncnn::cv::detection::YoloX*      |          yolox_m.opt.param&bin          |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 97Mb  |
|     *lite::ncnn::cv::detection::YoloX*      |          yolox_s.opt.param&bin           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 34Mb  |
|     *lite::ncnn::cv::detection::YoloX*      |         yolox_tiny.opt.param&bin         |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 19Mb  |
|     *lite::ncnn::cv::detection::YoloX*      |         yolox_nano.opt.param&bin         |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 3.5Mb |
|     *lite::ncnn::cv::detection::YOLOP*      |          yolop-640-640.opt.param&bin           |  [YOLOP](https://github.com/hustvl/YOLOP)   | 30Mb  |
| *lite::ncnn::cv::detection::NanoDet* |    nanodet_m_0.5x-opt.param&bin     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 1.1Mb  |
| *lite::ncnn::cv::detection::NanoDet* |    nanodet_m-opt.param&bin     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 3.6Mb  |
| *lite::ncnn::cv::detection::NanoDet* |    nanodet_m_1.5x-opt.param&bin     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 7.9Mb  |
| *lite::ncnn::cv::detection::NanoDet* |    nanodet_m_1.5x_416-opt.param&bin     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 7.9Mb  |
| *lite::ncnn::cv::detection::NanoDet* |    nanodet_m_416-opt.param&bin     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 3.6Mb  |
| *lite::ncnn::cv::detection::NanoDet* |    nanodet_g-opt.param&bin     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 14Mb  |
| *lite::ncnn::cv::detection::NanoDet* |    nanodet_t-opt.param&bin     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 5.1Mb  |
| *lite::ncnn::cv::detection::NanoDet* |    nanodet-RepVGG-A0_416-opt.param&bin     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 26Mb  |
| *lite::ncnn::cv::detection::NanoDetEfficientNetLite* |    nanodet-EfficientNet-Lite0_320-opt.param&bin     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 12Mb  |
| *lite::ncnn::cv::detection::NanoDetEfficientNetLite* |    nanodet-EfficientNet-Lite1_416-opt.param&bin     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 15Mb  |
| *lite::ncnn::cv::detection::NanoDetEfficientNetLite* |    nanodet-EfficientNet-Lite2_512-opt.param&bin     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 18Mb  |
| *lite::ncnn::cv::detection::NanoDetDepreciated* |    nanodet_m_0.5x-depreciated-opt.param&bin     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 1.1Mb  |
| *lite::ncnn::cv::detection::NanoDetDepreciated* |    nanodet_m-depreciated-opt.param&bin     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 3.6Mb  |
| *lite::ncnn::cv::detection::NanoDetDepreciated* |    nanodet_m_1.5x-depreciated-opt.param&bin     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 7.9Mb  |
| *lite::ncnn::cv::detection::NanoDetDepreciated* |    nanodet_m_1.5x_416-depreciated-opt.param&bin     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 7.9Mb  |
| *lite::ncnn::cv::detection::NanoDetDepreciated* |    nanodet_m_416-depreciated-opt.param&bin     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 3.6Mb  |
| *lite::ncnn::cv::detection::NanoDetDepreciated* |    nanodet_g-depreciated-opt.param&bin     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 14Mb  |
| *lite::ncnn::cv::detection::NanoDetDepreciated* |    nanodet_t-depreciated-opt.param&bin     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 5.1Mb  |
| *lite::ncnn::cv::detection::NanoDetDepreciated* |    nanodet-RepVGG-A0_416-depreciated-opt.param&bin     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 26Mb  |
| *lite::ncnn::cv::detection::NanoDetEfficientNetLiteDepreciated* |    nanodet-EfficientNet-Lite0_320-depreciated-opt.param&bin     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 12Mb  |
| *lite::ncnn::cv::detection::NanoDetEfficientNetLiteDepreciated* |    nanodet-EfficientNet-Lite1_416-depreciated-opt.param&bin     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 15Mb  |
| *lite::ncnn::cv::detection::NanoDetEfficientNetLiteDepreciated* |    nanodet-EfficientNet-Lite2_512-depreciated-opt.param&bin     |       [nanodet](https://github.com/RangiLyu/nanodet)       | 18Mb  |
|     *lite::ncnn::cv::detection::YoloX_V_0_1_1*      |          yolox_x_v0.1.1.opt.param&bin           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 378Mb |
|     *lite::ncnn::cv::detection::YoloX_V_0_1_1*      |          yolox_l_v0.1.1.opt.param&bin           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 207Mb |
|     *lite::ncnn::cv::detection::YoloX_V_0_1_1*      |          yolox_m_v0.1.1.opt.param&bin           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 97Mb  |
|     *lite::ncnn::cv::detection::YoloX_V_0_1_1*      |          yolox_s_v0.1.1.opt.param&bin           |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 34Mb  |
|     *lite::ncnn::cv::detection::YoloX_V_0_1_1*      |         yolox_tiny_v0.1.1.opt.param&bin         |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 19Mb  |
|     *lite::ncnn::cv::detection::YoloX_V_0_1_1*      |         yolox_nano_v0.1.1.opt.param&bin         |  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)   | 3.5Mb |
|     *lite::ncnn::cv::detection::YoloR*     |          yolor-p6-320-320.opt.param&bin            |      [yolor](https://github.com/WongKinYiu/yolor)      | 157Mb |
|     *lite::ncnn::cv::detection::YoloR*     |          yolor-p6-640-640.opt.param&bin            |      [yolor](https://github.com/WongKinYiu/yolor)      | 157Mb  |
|     *lite::ncnn::cv::detection::YoloR*     |          yolor-ssss-s2d-640-640.opt.param&bin            |      [yolor](https://github.com/WongKinYiu/yolor)      | 50Mb  |
|     *lite::ncnn::cv::detection::YoloV5_V_6_0*     |          yolov5m.640-640.v.6.0.opt.param&bin           |      [yolov5](https://github.com/ultralytics/yolov5)      | 81Mb  |
|     *lite::ncnn::cv::detection::YoloV5_V_6_0*     |          yolov5s.640-640.v.6.0.opt.param&bin           |      [yolov5](https://github.com/ultralytics/yolov5)      | 28Mb  |
|     *lite::ncnn::cv::detection::YoloV5_V_6_0*     |          yolov5n.640-640.v.6.0.opt.param&bin           |      [yolov5](https://github.com/ultralytics/yolov5)      | 7.5Mb |
|     *lite::ncnn::cv::detection::YoloV5_V_6_0_P6*     |          yolov5m6.640-640.v.6.0.opt.param&bin           |      [yolov5](https://github.com/ultralytics/yolov5)      | 128Mb  |
|     *lite::ncnn::cv::detection::YoloV5_V_6_0_P6*     |          yolov5s6.640-640.v.6.0.opt.param&bin           |      [yolov5](https://github.com/ultralytics/yolov5)      | 50Mb  |
|     *lite::ncnn::cv::detection::YoloV5_V_6_0_P6*     |          yolov5n6.640-640.v.6.0.opt.param&bin           |      [yolov5](https://github.com/ultralytics/yolov5)      | 14Mb |
|     *lite::ncnn::cv::detection::YoloV5_V_6_0_P6*     |          yolov5m6.1280-1280.v.6.0.opt.param&bin           |      [yolov5](https://github.com/ultralytics/yolov5)      | 128Mb  |
|     *lite::ncnn::cv::detection::YoloV5_V_6_0_P6*     |          yolov5s6.1280-1280.v.6.0.opt.param&bin           |      [yolov5](https://github.com/ultralytics/yolov5)      | 50Mb  |
|     *lite::ncnn::cv::detection::YoloV5_V_6_0_P6*     |          yolov5n6.1280-1280.v.6.0.opt.param&bin           |      [yolov5](https://github.com/ultralytics/yolov5)      | 14Mb |


## Matting.

<div id="lite.ai.toolkit.hub.ncnn-matting"></div>

|                Class                | Pretrained NCNN Files |              Rename or Converted From (Repo)              | Size  |
| :---------------------------------: | :-------------------: | :-------------------------------------------------------: | :---: |
| *lite::ncnn::cv::matting::RobustVideoMatting* |   rvm_mobilenetv3_fp32-opt.param&bin   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::ncnn::cv::matting::RobustVideoMatting* |   rvm_mobilenetv3_fp32-480-480-opt.param&bin   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::ncnn::cv::matting::RobustVideoMatting* |   rvm_mobilenetv3_fp32-480-640-opt.param&bin   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::ncnn::cv::matting::RobustVideoMatting* |   rvm_mobilenetv3_fp32-640-480-opt.param&bin   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::ncnn::cv::matting::RobustVideoMatting* |   rvm_mobilenetv3_fp32-1080-1920-opt.param&bin   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::ncnn::cv::matting::RobustVideoMatting* |   rvm_resnet50_fp32-opt.param&bin   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 50Mb |
| *lite::ncnn::cv::matting::RobustVideoMatting* |   rvm_resnet50_fp32-480-480-opt.param&bin   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 50Mb |
| *lite::ncnn::cv::matting::RobustVideoMatting* |   rvm_resnet50_fp32-480-640-opt.param&bin   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 50Mb |
| *lite::ncnn::cv::matting::RobustVideoMatting* |   rvm_resnet50_fp32-640-480-opt.param&bin   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 50Mb |
| *lite::ncnn::cv::matting::RobustVideoMatting* |   rvm_resnet50_fp32-1080-1920-opt.param&bin   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 50Mb |

## Face Recognition. 

<div id="lite.ai.toolkit.hub.ncnn-face-recognition"></div>  


|                   Class                   |            Pretrained NCNN Files             |               Rename or Converted From (Repo)                | Size  |
| :---------------------------------------: | :------------------------------------------: | :----------------------------------------------------------: | :---: |
|     *lite::ncnn::cv::faceid::GlintArcFace*      |           ms1mv3_arcface_r100.opt.param&bin           |  [insightface](https://github.com/deepinsight/insightface)   | 248Mb |
|     *lite::ncnn::cv::faceid::GlintArcFace*      |           ms1mv3_arcface_r50.opt.param&bin            |  [insightface](https://github.com/deepinsight/insightface)   | 166Mb |
|     *lite::ncnn::cv::faceid::GlintArcFace*      |           ms1mv3_arcface_r34.opt.param&bin            |  [insightface](https://github.com/deepinsight/insightface)   | 130Mb |
|     *lite::ncnn::cv::faceid::GlintArcFace*      |           ms1mv3_arcface_r18.opt.param&bin            |  [insightface](https://github.com/deepinsight/insightface)   | 91Mb  |
|     *lite::ncnn::cv::faceid::GlintCosFace*      |         glint360k_cosface_r100.opt.param&bin          |  [insightface](https://github.com/deepinsight/insightface)   | 248Mb |
|     *lite::ncnn::cv::faceid::GlintCosFace*      |          glint360k_cosface_r50.opt.param&bin          |  [insightface](https://github.com/deepinsight/insightface)   | 166Mb |
|     *lite::ncnn::cv::faceid::GlintCosFace*      |          glint360k_cosface_r34.opt.param&bin          |  [insightface](https://github.com/deepinsight/insightface)   | 130Mb |
|     *lite::ncnn::cv::faceid::GlintCosFace*      |          glint360k_cosface_r18.opt.param&bin          |  [insightface](https://github.com/deepinsight/insightface)   | 91Mb  |
|    *lite::ncnn::cv::faceid::GlintPartialFC*     |        partial_fc_glint360k_r100.opt.param&bin        |  [insightface](https://github.com/deepinsight/insightface)   | 248Mb |
|    *lite::ncnn::cv::faceid::GlintPartialFC*     |        partial_fc_glint360k_r50.opt.param&bin         |  [insightface](https://github.com/deepinsight/insightface)   | 91Mb  |
|        *lite::ncnn::cv::faceid::FaceNet*        |         facenet_vggface2_resnet.opt.param&bin         |  [facenet...](https://github.com/timesler/facenet-pytorch)   | 89Mb  |
|        *lite::ncnn::cv::faceid::FaceNet*        |      facenet_casia-webface_resnet.opt.param&bin       |  [facenet...](https://github.com/timesler/facenet-pytorch)   | 89Mb  |
|     *lite::ncnn::cv::faceid::FocalArcFace*      |        focal-arcface-ms1m-ir152.opt.param&bin         | [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch) | 269Mb |
|     *lite::ncnn::cv::faceid::FocalArcFace*      |    focal-arcface-ms1m-ir50-epoch120.opt.param&bin     | [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch) | 166Mb |
|     *lite::ncnn::cv::faceid::FocalArcFace*      |     focal-arcface-ms1m-ir50-epoch63.opt.param&bin     | [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch) | 166Mb |
|   *lite::ncnn::cv::faceid::FocalAsiaArcFace*    |       focal-arcface-bh-ir50-asia.opt.param&bin        | [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch) | 166Mb |
| *lite::ncnn::cv::faceid::TencentCurricularFace* |     Tencent_CurricularFace_Backbone.opt.param&bin     |          [TFace](https://github.com/Tencent/TFace)           | 249Mb |
|    *lite::ncnn::cv::faceid::TencentCifpFace*    |  Tencent_Cifp_BUPT_Balancedface_IR_34.opt.param&bin   |          [TFace](https://github.com/Tencent/TFace)           | 130Mb |
|    *lite::ncnn::cv::faceid::CenterLossFace*     |        CenterLossFace_epoch_100.opt.param&bin         | [center-loss...](https://github.com/louis-she/center-loss.pytorch) | 280Mb |
|      *lite::ncnn::cv::faceid::SphereFace*       |           sphere20a_20171020.opt.param&bin            | [sphere...](https://github.com/clcarwin/sphereface_pytorch)  | 86Mb  |
|     *lite::ncnn::cv::faceid:MobileFaceNet*      |        MobileFaceNet_Pytorch_068.opt.param&bin        | [MobileFace...](https://github.com/Xiaoccer/MobileFaceNet_Pytorch) | 3.8Mb |
|    *lite::ncnn::cv::faceid:CavaGhostArcFace*    | cavaface_GhostNet_x1.3_Arcface_Epoch_24.opt.param&bin | [cavaface...](https://github.com/cavalleria/cavaface.pytorch) | 15Mb  |
|    *lite::ncnn::cv::faceid:CavaCombinedFace*    |  cavaface_IR_SE_100_Combined_Epoch_24.opt.param&bin   | [cavaface...](https://github.com/cavalleria/cavaface.pytorch) | 250Mb |
|    *lite::ncnn::cv::faceid:MobileSEFocalFace*   | face_recognition.pytorch_Mobilenet_se_focal_121000.opt.param&bin | [face_recog...](https://github.com/grib0ed0v/face_recognition.pytorch) | 4.5Mb |

## Face Detection.

<div id="lite.ai.toolkit.hub.ncnn-face-detection"></div>  

|                Class                | Pretrained NCNN Files  |               Rename or Converted From (Repo)                | Size  |
| :---------------------------------: | :--------------------: | :----------------------------------------------------------: | :---: |
| *lite::ncnn::cv::face::detect::UltraFace* | ultraface-rfb-320.param&bin | [Ultra-Light...](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) | 1.5Mb |
| *lite::ncnn::cv::face::detect::UltraFace* | ultraface-slim-320.param&bin | [Ultra-Light...](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) | 1.2Mb |
| *lite::ncnn::cv::face::detect::RetinaFace* | Pytorch_RetinaFace_mobile0.25.opt.param&bin | [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface) | 1.6Mb |
| *lite::ncnn::cv::face::detect::RetinaFace* | Pytorch_RetinaFace_mobile0.25-640-640.opt.param&bin | [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface) | 1.6Mb |
| *lite::ncnn::cv::face::detect::RetinaFace* | Pytorch_RetinaFace_mobile0.25-320-320.opt.param&bin | [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface) | 1.6Mb |
| *lite::ncnn::cv::face::detect::RetinaFace* | Pytorch_RetinaFace_mobile0.25-720-1080.opt.param&bin | [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface) | 1.6Mb |
| *lite::ncnn::cv::face::detect::FaceBoxes* | FaceBoxes.opt.param&bin | [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)  | 3.8Mb |
| *lite::ncnn::cv::face::detect::FaceBoxes* | FaceBoxes-640-640.opt.param&bin | [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)  | 3.8Mb |
| *lite::ncnn::cv::face::detect::FaceBoxes* | FaceBoxes-320-320.opt.param&bin | [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)  | 3.8Mb |
| *lite::ncnn::cv::face::detect::FaceBoxes* | FaceBoxes-720-1080.opt.param&bin | [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)  | 3.8Mb |


## Face Alignment.

<div id="lite.ai.toolkit.hub.ncnn-face-alignment"></div>  


|             Class             | Pretrained NCNN Files |               Rename or Converted From (Repo)                | Size  |
| :---------------------------: | :-------------------: | :----------------------------------------------------------: | :---: |
| *lite::ncnn::cv::face::align::PFLD* |  pfld-106-lite.opt.param&bin   | [pfld_106_...](https://github.com/Hsintao/pfld_106_face_landmarks) | 1.0Mb |
| *lite::ncnn::cv::face::align::PFLD* |   pfld-106-v3.opt.param&bin    | [pfld_106_...](https://github.com/Hsintao/pfld_106_face_landmarks) | 5.5Mb |
| *lite::ncnn::cv::face::align::PFLD* |   pfld-106-v2.opt.param&bin    | [pfld_106_...](https://github.com/Hsintao/pfld_106_face_landmarks) | 5.0Mb |
| *lite::ncnn::cv::face::align::PFLD98* |   PFLD-pytorch-pfld.opt.param&bin  | [PFLD...](https://github.com/polarisZhao/PFLD-pytorch) | 4.8Mb |
| *lite::ncnn::cv::face::align::MobileNetV268* |   pytorch_face_landmarks_landmark_detection_56.opt.param&bin  | [...landmark](https://github.com/cunjian/pytorch_face_landmark) | 9.4Mb |
| *lite::ncnn::cv::face::align::MobileNetV2SE68* |   pytorch_face_landmarks_landmark_detection_56_se_external.opt.param&bin  | [...landmark](https://github.com/cunjian/pytorch_face_landmark) | 11Mb |
| *lite::ncnn::cv::face::align::PFLD68* |   pytorch_face_landmarks_pfld.opt.param&bin  | [...landmark](https://github.com/cunjian/pytorch_face_landmark) | 2.8Mb |
| *lite::ncnn::cv::face::align::FaceLandmarks1000* |   FaceLandmark1000.opt.param&bin  | [FaceLandm...](https://github.com/Single430/FaceLandmark1000) | 2.0Mb |

## Face Attributes.

<div id="lite.ai.toolkit.hub.ncnn-face-attributes"></div>  


|                  Class                  |          Pretrained NCNN Files           |             Rename or Converted From (Repo)              | Size  |
| :-------------------------------------: | :--------------------------------------: | :------------------------------------------------------: | :---: |
|  *lite::ncnn::cv::face::attr::AgeGoogleNet*   |            age_googlenet.opt.param&bin           |      [onnx-models](https://github.com/onnx/models)       | 23Mb  |
| *lite::ncnn::cv::face::attr::GenderGoogleNet* |          gender_googlenet.opt.param&bin            |      [onnx-models](https://github.com/onnx/models)       | 23Mb  |
| *lite::ncnn::cv::face::attr::EmotionFerPlus*  |          emotion-ferplus-7.opt.param&bin          |      [onnx-models](https://github.com/onnx/models)       | 33Mb  |
| *lite::ncnn::cv::face::attr::EmotionFerPlus*  |          emotion-ferplus-8.opt.param&bin          |      [onnx-models](https://github.com/onnx/models)       | 33Mb  |
|     *lite::ncnn::cv::face::attr::EfficientEmotion7*      |  face-emotion-recognition-enet_b0_7.opt.param&bin   | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 15Mb |
|     *lite::ncnn::cv::face::attr::EfficientEmotion8*      |  face-emotion-recognition-enet_b0_8_best_afew.opt.param&bin  | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 15Mb |
|     *lite::ncnn::cv::face::attr::EfficientEmotion8*      |  face-emotion-recognition-enet_b0_8_best_vgaf.opt.param&bin  | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 15Mb |
|     *lite::ncnn::cv::face::attr::MobileEmotion7*      |   face-emotion-recognition-mobilenet_7.opt.param&bin   | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition)| 13Mb |


## Classification.

<div id="lite.ai.toolkit.hub.ncnn-classification"></div>


|                    Class                     |   Pretrained NCNN Files    |         Rename or Converted From (Repo)          | Size  |
| :------------------------------------------: | :------------------------: | :----------------------------------------------: | :---: |
|   *lite::ncnn::cv::classification::ShuffleNetV2*   |   shufflenet-v2-10.opt.param&bin    |  [onnx-models](https://github.com/onnx/models)   | 8.7Mb |
|   *lite::ncnn::cv::classification::DenseNet121*    |      densenet121.opt.param&bin      | [torchvision](https://github.com/pytorch/vision) | 30Mb  |
|     *lite::ncnn::cv::classification::GhostNet*     |       ghostnet.opt.param&bin        | [torchvision](https://github.com/pytorch/vision) | 20Mb  |
|     *lite::ncnn::cv::classification::HdrDNet*      |        hardnet.opt.param&bin        | [torchvision](https://github.com/pytorch/vision) | 13Mb  |
|      *lite::ncnn::cv::classification::IBNNet*      |       ibnnet18.opt.param&bin        | [torchvision](https://github.com/pytorch/vision) | 97Mb  |
|   *lite::ncnn::cv::classification::MobileNetV2*    |      mobilenetv2.opt.param&bin      | [torchvision](https://github.com/pytorch/vision) | 13Mb  |
|      *lite::ncnn::cv::classification::ResNet*      |       resnet18.opt.param&bin        | [torchvision](https://github.com/pytorch/vision) | 44Mb  |
|     *lite::ncnn::cv::classification::ResNeXt*      |        resnext.opt.param&bin        | [torchvision](https://github.com/pytorch/vision) | 95Mb  |
