# Lite.AI.ToolKit.Hub.TNN

You can download all the pretrained models files of TNN format from ([Baidu Drive](https://pan.baidu.com/s/1lvM2YKyUbEc5HKVtqITpcw) code: 6o6k)

## Object Detection.

<div id="lite.ai.toolkit.hub.tnn-object-detection"></div>

|                        Class                        |                 Pretrained TNN Files                 |               Rename or Converted From (Repo)                | Size  |
|:---------------------------------------------------:|:----------------------------------------------------:|:------------------------------------------------------------:|:-----:|
|         *lite::tnn::cv::detection::YoloV5*          |            yolov5l.opt.tnnproto&tnnmodel             |       [yolov5](https://github.com/ultralytics/yolov5)        | 188Mb |
|         *lite::tnn::cv::detection::YoloV5*          |            yolov5m.opt.tnnproto&tnnmodel             |       [yolov5](https://github.com/ultralytics/yolov5)        | 85Mb  |
|         *lite::tnn::cv::detection::YoloV5*          |            yolov5s.opt.tnnproto&tnnmodel             |       [yolov5](https://github.com/ultralytics/yolov5)        | 29Mb  |
|         *lite::tnn::cv::detection::YoloV5*          |            yolov5x.opt.tnnproto&tnnmodel             |       [yolov5](https://github.com/ultralytics/yolov5)        | 351Mb |  
|          *lite::tnn::cv::detection::YoloX*          |            yolox_x.opt.tnnproto&tnnmodel             |    [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)    | 378Mb |
|          *lite::tnn::cv::detection::YoloX*          |            yolox_l.opt.tnnproto&tnnmodel             |    [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)    | 207Mb |
|          *lite::tnn::cv::detection::YoloX*          |            yolox_m.opt.tnnproto&tnnmodel             |    [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)    | 97Mb  |
|          *lite::tnn::cv::detection::YoloX*          |            yolox_s.opt.tnnproto&tnnmodel             |    [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)    | 34Mb  |
|          *lite::tnn::cv::detection::YoloX*          |           yolox_tiny.opt.tnnproto&tnnmodel           |    [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)    | 19Mb  |
|          *lite::tnn::cv::detection::YoloX*          |           yolox_nano.opt.tnnproto&tnnmodel           |    [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)    | 3.5Mb |
|          *lite::tnn::cv::detection::YOLOP*          |         yolop-320-320.opt.tnnproto&tnnmodel          |           [YOLOP](https://github.com/hustvl/YOLOP)           | 30Mb  |
|          *lite::tnn::cv::detection::YOLOP*          |         yolop-640-640.opt.tnnproto&tnnmodel          |           [YOLOP](https://github.com/hustvl/YOLOP)           | 30Mb  |
|          *lite::tnn::cv::detection::YOLOP*          |        yolop-1280-1280.opt.tnnproto&tnnmodel         |           [YOLOP](https://github.com/hustvl/YOLOP)           | 30Mb  |
|         *lite::tnn::cv::detection::NanoDet*         |         nanodet_m_0.5x.opt.tnnproto&tnnmodel         |        [nanodet](https://github.com/RangiLyu/nanodet)        | 1.1Mb |
|         *lite::tnn::cv::detection::NanoDet*         |           nanodet_m.opt.tnnproto&tnnmodel            |        [nanodet](https://github.com/RangiLyu/nanodet)        | 3.6Mb |
|         *lite::tnn::cv::detection::NanoDet*         |         nanodet_m_1.5x.opt.tnnproto&tnnmodel         |        [nanodet](https://github.com/RangiLyu/nanodet)        | 7.9Mb |
|         *lite::tnn::cv::detection::NanoDet*         |       nanodet_m_1.5x_416.opt.tnnproto&tnnmodel       |        [nanodet](https://github.com/RangiLyu/nanodet)        | 7.9Mb |
|         *lite::tnn::cv::detection::NanoDet*         |         nanodet_m_416.opt.tnnproto&tnnmodel          |        [nanodet](https://github.com/RangiLyu/nanodet)        | 3.6Mb |
|         *lite::tnn::cv::detection::NanoDet*         |           nanodet_g.opt.tnnproto&tnnmodel            |        [nanodet](https://github.com/RangiLyu/nanodet)        | 14Mb  |
|         *lite::tnn::cv::detection::NanoDet*         |           nanodet_t.opt.tnnproto&tnnmodel            |        [nanodet](https://github.com/RangiLyu/nanodet)        | 5.1Mb |
|         *lite::tnn::cv::detection::NanoDet*         |     nanodet-RepVGG-A0_416.opt.tnnproto&tnnmodel      |        [nanodet](https://github.com/RangiLyu/nanodet)        | 26Mb  |
| *lite::tnn::cv::detection::NanoDetEfficientNetLite* | nanodet-EfficientNet-Lite0_320.opt.tnnproto&tnnmodel |        [nanodet](https://github.com/RangiLyu/nanodet)        | 12Mb  |
| *lite::tnn::cv::detection::NanoDetEfficientNetLite* | nanodet-EfficientNet-Lite1_416.opt.tnnproto&tnnmodel |        [nanodet](https://github.com/RangiLyu/nanodet)        | 15Mb  |
| *lite::tnn::cv::detection::NanoDetEfficientNetLite* | nanodet-EfficientNet-Lite2_512.opt.tnnproto&tnnmodel |        [nanodet](https://github.com/RangiLyu/nanodet)        | 18Mb  |
|      *lite::tnn::cv::detection::YoloX_V_0_1_1*      |         yolox_x_v0.1.1.opt.tnnproto&tnnmodel         |    [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)    | 378Mb |
|      *lite::tnn::cv::detection::YoloX_V_0_1_1*      |         yolox_l_v0.1.1.opt.tnnproto&tnnmodel         |    [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)    | 207Mb |
|      *lite::tnn::cv::detection::YoloX_V_0_1_1*      |         yolox_m_v0.1.1.opt.tnnproto&tnnmodel         |    [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)    | 97Mb  |
|      *lite::tnn::cv::detection::YoloX_V_0_1_1*      |         yolox_s_v0.1.1.opt.tnnproto&tnnmodel         |    [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)    | 34Mb  |
|      *lite::tnn::cv::detection::YoloX_V_0_1_1*      |       yolox_tiny_v0.1.1.opt.tnnproto&tnnmodel        |    [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)    | 19Mb  |
|      *lite::tnn::cv::detection::YoloX_V_0_1_1*      |       yolox_nano_v0.1.1.opt.tnnproto&tnnmodel        |    [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)    | 3.5Mb |
|          *lite::tnn::cv::detection::YoloR*          |        yolor-p6-320-320.opt.tnnproto&tnnmodel        |         [yolor](https://github.com/WongKinYiu/yolor)         | 157Mb |
|          *lite::tnn::cv::detection::YoloR*          |        yolor-p6-640-640.opt.tnnproto&tnnmodel        |         [yolor](https://github.com/WongKinYiu/yolor)         | 157Mb |
|          *lite::tnn::cv::detection::YoloR*          |     yolor-ssss-s2d-640-640.opt.tnnproto&tnnmodel     |         [yolor](https://github.com/WongKinYiu/yolor)         | 50Mb  |
|      *lite::tnn::cv::detection::YoloV5_V_6_0*       |     yolov5m.640-640.v.6.0.opt.tnnproto&tnnmodel      |       [yolov5](https://github.com/ultralytics/yolov5)        | 81Mb  |
|      *lite::tnn::cv::detection::YoloV5_V_6_0*       |     yolov5s.640-640.v.6.0.opt.tnnproto&tnnmodel      |       [yolov5](https://github.com/ultralytics/yolov5)        | 28Mb  |
|      *lite::tnn::cv::detection::YoloV5_V_6_0*       |     yolov5n.640-640.v.6.0.opt.tnnproto&tnnmodel      |       [yolov5](https://github.com/ultralytics/yolov5)        | 7.5Mb |
|      *lite::tnn::cv::detection::YoloV5_V_6_0*       |     yolov5m6.640-640.v.6.0.opt.tnnproto&tnnmodel     |       [yolov5](https://github.com/ultralytics/yolov5)        | 128Mb |
|      *lite::tnn::cv::detection::YoloV5_V_6_0*       |     yolov5s6.640-640.v.6.0.opt.tnnproto&tnnmodel     |       [yolov5](https://github.com/ultralytics/yolov5)        | 50Mb  |
|      *lite::tnn::cv::detection::YoloV5_V_6_0*       |     yolov5n6.640-640.v.6.0.opt.tnnproto&tnnmodel     |       [yolov5](https://github.com/ultralytics/yolov5)        | 14Mb  |
|      *lite::tnn::cv::detection::YoloV5_V_6_0*       |    yolov5m6.1280-1280.v.6.0.opt.tnnproto&tnnmodel    |       [yolov5](https://github.com/ultralytics/yolov5)        | 128Mb |
|      *lite::tnn::cv::detection::YoloV5_V_6_0*       |    yolov5s6.1280-1280.v.6.0.opt.tnnproto&tnnmodel    |       [yolov5](https://github.com/ultralytics/yolov5)        | 50Mb  |
|      *lite::tnn::cv::detection::YoloV5_V_6_0*       |    yolov5n6.1280-1280.v.6.0.opt.tnnproto&tnnmodel    |       [yolov5](https://github.com/ultralytics/yolov5)        | 14Mb  |
|       *lite::tnn::cv::detection::NanoDetPlus*       |       nanodet-plus-m_320.opt.tnnproto&tnnmodel       |        [nanodet](https://github.com/RangiLyu/nanodet)        | 4.5Mb |
|       *lite::tnn::cv::detection::NanoDetPlus*       |       nanodet-plus-m_416.opt.tnnproto&tnnmodel       |        [nanodet](https://github.com/RangiLyu/nanodet)        | 4.5Mb |
|       *lite::tnn::cv::detection::NanoDetPlus*       |    nanodet-plus-m-1.5x_320.opt.tnnproto&tnnmodel     |        [nanodet](https://github.com/RangiLyu/nanodet)        | 9.4Mb |
|       *lite::tnn::cv::detection::NanoDetPlus*       |    nanodet-plus-m-1.5x_416.opt.tnnproto&tnnmodel     |        [nanodet](https://github.com/RangiLyu/nanodet)        | 9.4Mb |
|        *lite::tnn::cv::detection::InsectDet*        |   quarrying_insect_detector.opt.tnnproto&tnnmodel    | [InsectID](https://github.com/quarrying/quarrying-insect-id) | 22Mb  |


## Matting.

<div id="lite.ai.toolkit.hub.tnn-matting"></div>

|                     Class                     |                       Pretrained TNN Files                       |                    Rename or Converted From (Repo)                     | Size  |
|:---------------------------------------------:|:----------------------------------------------------------------:|:----------------------------------------------------------------------:|:-----:|
| *lite::tnn::cv::matting::RobustVideoMatting*  |        rvm_mobilenetv3_fp32-480-480-sim.tnnproto&tnnmodel        |  [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting)  | 14Mb  |
| *lite::tnn::cv::matting::RobustVideoMatting*  |      rvm_mobilenetv3_fp32-480-480-sim.opt.tnnproto&tnnmodel      |  [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting)  | 14Mb  |
| *lite::tnn::cv::matting::RobustVideoMatting*  |      rvm_mobilenetv3_fp32-480-640-sim.opt.tnnproto&tnnmodel      |  [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting)  | 14Mb  |
| *lite::tnn::cv::matting::RobustVideoMatting*  |      rvm_mobilenetv3_fp32-640-480-sim.opt.tnnproto&tnnmodel      |  [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting)  | 14Mb  |
| *lite::tnn::cv::matting::RobustVideoMatting*  |     rvm_mobilenetv3_fp32-1080-1920-sim.opt.tnnproto&tnnmodel     |  [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting)  | 14Mb  |
| *lite::tnn::cv::matting::RobustVideoMatting*  |       rvm_resnet50_fp32-480-480-sim.opt.tnnproto&tnnmodel        |  [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting)  | 50Mb  |
| *lite::tnn::cv::matting::RobustVideoMatting*  |       rvm_resnet50_fp32-480-640-sim.opt.tnnproto&tnnmodel        |  [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting)  | 50Mb  |
| *lite::tnn::cv::matting::RobustVideoMatting*  |       rvm_resnet50_fp32-640-480-sim.opt.tnnproto&tnnmodel        |  [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting)  | 50Mb  |
| *lite::tnn::cv::matting::RobustVideoMatting*  |      rvm_resnet50_fp32-1080-1920-sim.opt.tnnproto&tnnmodel       |  [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting)  | 50Mb  |
|      *lite::tnn::cv::matting::MGMatting*      |             MGMatting-DIM-100k.opt.tnnproto&tnnmodel             |          [MGMatting](https://github.com/yucornetto/MGMatting)          | 113Mb |
|      *lite::tnn::cv::matting::MGMatting*      |             MGMatting-RWP-100k.opt.tnnproto&tnnmodel             |          [MGMatting](https://github.com/yucornetto/MGMatting)          | 113Mb |
|       *lite::tnn::cv::matting::MODNet*        | modnet_photographic_portrait_matting-1024x1024.tnnproto&tnnmodel |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|       *lite::tnn::cv::matting::MODNet*        | modnet_photographic_portrait_matting-1024x512.tnnproto&tnnmodel  |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|       *lite::tnn::cv::matting::MODNet*        |  modnet_photographic_portrait_matting-256x256.tnnproto&tnnmodel  |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|       *lite::tnn::cv::matting::MODNet*        |  modnet_photographic_portrait_matting-256x512.tnnproto&tnnmodel  |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|       *lite::tnn::cv::matting::MODNet*        | modnet_photographic_portrait_matting-512x1024.tnnproto&tnnmodel  |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|       *lite::tnn::cv::matting::MODNet*        |  modnet_photographic_portrait_matting-512x256.tnnproto&tnnmodel  |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|       *lite::tnn::cv::matting::MODNet*        |  modnet_photographic_portrait_matting-512x512.tnnproto&tnnmodel  |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|       *lite::tnn::cv::matting::MODNet*        |    modnet_webcam_portrait_matting-1024x1024.tnnproto&tnnmodel    |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|       *lite::tnn::cv::matting::MODNet*        |    modnet_webcam_portrait_matting-1024x512.tnnproto&tnnmodel     |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|       *lite::tnn::cv::matting::MODNet*        |     modnet_webcam_portrait_matting-256x256.tnnproto&tnnmodel     |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|       *lite::tnn::cv::matting::MODNet*        |     modnet_webcam_portrait_matting-256x512.tnnproto&tnnmodel     |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|       *lite::tnn::cv::matting::MODNet*        |    modnet_webcam_portrait_matting-512x1024.tnnproto&tnnmodel     |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|       *lite::tnn::cv::matting::MODNet*        |     modnet_webcam_portrait_matting-512x256.tnnproto&tnnmodel     |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
|       *lite::tnn::cv::matting::MODNet*        |     modnet_webcam_portrait_matting-512x512.tnnproto&tnnmodel     |               [MODNet](https://github.com/ZHKKKe/MODNet)               | 24Mb  |
| *lite::tnn::cv::matting::BackgroundMattingV2* |       BGMv2_mobilenetv2-256x256-full.opt.tnnproto&tnnmodel       | [BackgroundMattingV2](https://github.com/PeterL1n/BackgroundMattingV2) | 20Mb  |
| *lite::tnn::cv::matting::BackgroundMattingV2* |       BGMv2_mobilenetv2-512x512-full.opt.tnnproto&tnnmodel       | [BackgroundMattingV2](https://github.com/PeterL1n/BackgroundMattingV2) | 20Mb  |
| *lite::tnn::cv::matting::BackgroundMattingV2* |      BGMv2_mobilenetv2-1080x1920-full.opt.tnnproto&tnnmodel      | [BackgroundMattingV2](https://github.com/PeterL1n/BackgroundMattingV2) | 20Mb  |
| *lite::tnn::cv::matting::BackgroundMattingV2* |      BGMv2_mobilenetv2-2160x3840-full.opt.tnnproto&tnnmodel      | [BackgroundMattingV2](https://github.com/PeterL1n/BackgroundMattingV2) | 20Mb  |
| *lite::tnn::cv::matting::BackgroundMattingV2* |       BGMv2_resnet50-1080x1920-full.opt.tnnproto&tnnmodel        | [BackgroundMattingV2](https://github.com/PeterL1n/BackgroundMattingV2) | 20Mb  |
| *lite::tnn::cv::matting::BackgroundMattingV2* |       BGMv2_resnet50-2160x3840-full.opt.tnnproto&tnnmodel        | [BackgroundMattingV2](https://github.com/PeterL1n/BackgroundMattingV2) | 20Mb  |
| *lite::tnn::cv::matting::BackgroundMattingV2* |       BGMv2_resnet101-2160x3840-full.opt.tnnproto&tnnmodel       | [BackgroundMattingV2](https://github.com/PeterL1n/BackgroundMattingV2) | 154Mb |


## Face Recognition.

<div id="lite.ai.toolkit.hub.tnn-face-recognition"></div>  


|                     Class                      |                           Pretrained TNN Files                           |                    Rename or Converted From (Repo)                     | Size  |
|:----------------------------------------------:|:------------------------------------------------------------------------:|:----------------------------------------------------------------------:|:-----:|
|     *lite::tnn::cv::faceid::GlintArcFace*      |                ms1mv3_arcface_r100.opt.tnnproto&tnnmodel                 |       [insightface](https://github.com/deepinsight/insightface)        | 248Mb |
|     *lite::tnn::cv::faceid::GlintArcFace*      |                 ms1mv3_arcface_r50.opt.tnnproto&tnnmodel                 |       [insightface](https://github.com/deepinsight/insightface)        | 166Mb |
|     *lite::tnn::cv::faceid::GlintArcFace*      |                 ms1mv3_arcface_r34.opt.tnnproto&tnnmodel                 |       [insightface](https://github.com/deepinsight/insightface)        | 130Mb |
|     *lite::tnn::cv::faceid::GlintArcFace*      |                 ms1mv3_arcface_r18.opt.tnnproto&tnnmodel                 |       [insightface](https://github.com/deepinsight/insightface)        | 91Mb  |
|     *lite::tnn::cv::faceid::GlintCosFace*      |               glint360k_cosface_r100.opt.tnnproto&tnnmodel               |       [insightface](https://github.com/deepinsight/insightface)        | 248Mb |
|     *lite::tnn::cv::faceid::GlintCosFace*      |               glint360k_cosface_r50.opt.tnnproto&tnnmodel                |       [insightface](https://github.com/deepinsight/insightface)        | 166Mb |
|     *lite::tnn::cv::faceid::GlintCosFace*      |               glint360k_cosface_r34.opt.tnnproto&tnnmodel                |       [insightface](https://github.com/deepinsight/insightface)        | 130Mb |
|     *lite::tnn::cv::faceid::GlintCosFace*      |               glint360k_cosface_r18.opt.tnnproto&tnnmodel                |       [insightface](https://github.com/deepinsight/insightface)        | 91Mb  |
|    *lite::tnn::cv::faceid::GlintPartialFC*     |             partial_fc_glint360k_r100.opt.tnnproto&tnnmodel              |       [insightface](https://github.com/deepinsight/insightface)        | 248Mb |
|    *lite::tnn::cv::faceid::GlintPartialFC*     |              partial_fc_glint360k_r50.opt.tnnproto&tnnmodel              |       [insightface](https://github.com/deepinsight/insightface)        | 91Mb  |
|        *lite::tnn::cv::faceid::FaceNet*        |              facenet_vggface2_resnet.opt.tnnproto&tnnmodel               |       [facenet...](https://github.com/timesler/facenet-pytorch)        | 89Mb  |
|        *lite::tnn::cv::faceid::FaceNet*        |            facenet_casia-webface_resnet.opt.tnnproto&tnnmodel            |       [facenet...](https://github.com/timesler/facenet-pytorch)        | 89Mb  |
|     *lite::tnn::cv::faceid::FocalArcFace*      |              focal-arcface-ms1m-ir152.opt.tnnproto&tnnmodel              |   [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch)   | 269Mb |
|     *lite::tnn::cv::faceid::FocalArcFace*      |          focal-arcface-ms1m-ir50-epoch120.opt.tnnproto&tnnmodel          |   [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch)   | 166Mb |
|     *lite::tnn::cv::faceid::FocalArcFace*      |          focal-arcface-ms1m-ir50-epoch63.opt.tnnproto&tnnmodel           |   [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch)   | 166Mb |
|   *lite::tnn::cv::faceid::FocalAsiaArcFace*    |             focal-arcface-bh-ir50-asia.opt.tnnproto&tnnmodel             |   [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch)   | 166Mb |
| *lite::tnn::cv::faceid::TencentCurricularFace* |          Tencent_CurricularFace_Backbone.opt.tnnproto&tnnmodel           |               [TFace](https://github.com/Tencent/TFace)                | 249Mb |
|    *lite::tnn::cv::faceid::TencentCifpFace*    |        Tencent_Cifp_BUPT_Balancedface_IR_34.opt.tnnproto&tnnmodel        |               [TFace](https://github.com/Tencent/TFace)                | 130Mb |
|    *lite::tnn::cv::faceid::CenterLossFace*     |              CenterLossFace_epoch_100.opt.tnnproto&tnnmodel              |   [center-loss...](https://github.com/louis-she/center-loss.pytorch)   | 280Mb |
|      *lite::tnn::cv::faceid::SphereFace*       |                 sphere20a_20171020.opt.tnnproto&tnnmodel                 |      [sphere...](https://github.com/clcarwin/sphereface_pytorch)       | 86Mb  |
|     *lite::tnn::cv::faceid:MobileFaceNet*      |             MobileFaceNet_Pytorch_068.opt.tnnproto&tnnmodel              |   [MobileFace...](https://github.com/Xiaoccer/MobileFaceNet_Pytorch)   | 3.8Mb |
|    *lite::tnn::cv::faceid:CavaGhostArcFace*    |      cavaface_GhostNet_x1.3_Arcface_Epoch_24.opt.tnnproto&tnnmodel       |     [cavaface...](https://github.com/cavalleria/cavaface.pytorch)      | 15Mb  |
|    *lite::tnn::cv::faceid:CavaCombinedFace*    |        cavaface_IR_SE_100_Combined_Epoch_24.opt.tnnproto&tnnmodel        |     [cavaface...](https://github.com/cavalleria/cavaface.pytorch)      | 250Mb |
|   *lite::tnn::cv::faceid:MobileSEFocalFace*    | face_recognition.pytorch_Mobilenet_se_focal_121000.opt.tnnproto&tnnmodel | [face_recog...](https://github.com/grib0ed0v/face_recognition.pytorch) | 4.5Mb |


## Face Detection.

<div id="lite.ai.toolkit.hub.tnn-face-detection"></div>  

|                   Class                    |                     Pretrained TNN Files                     |                             Rename or Converted From (Repo)                             |  Size  |
|:------------------------------------------:|:------------------------------------------------------------:|:---------------------------------------------------------------------------------------:|:------:|
|  *lite::tnn::cv::face::detect::UltraFace*  |           ultraface-rfb-320.opt.tnnproto&tnnmodel            | [Ultra-Light...](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) | 1.5Mb  |
|  *lite::tnn::cv::face::detect::UltraFace*  |           ultraface-rfb-640.opt.tnnproto&tnnmodel            | [Ultra-Light...](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) | 1.5Mb  |
|  *lite::tnn::cv::face::detect::UltraFace*  |           ultraface-slim-320.opt.tnnproto&tnnmodel           | [Ultra-Light...](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) | 1.2Mb  |
|  *lite::tnn::cv::face::detect::UltraFace*  |           ultraface-slim-640.opt.tnnproto&tnnmodel           | [Ultra-Light...](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) | 1.2Mb  |
| *lite::tnn::cv::face::detect::RetinaFace*  |     Pytorch_RetinaFace_mobile0.25.opt.tnnproto&tnnmodel      |             [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface)              | 1.6Mb  |
| *lite::tnn::cv::face::detect::RetinaFace*  | Pytorch_RetinaFace_mobile0.25-640-640.opt.tnnproto&tnnmodel  |             [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface)              | 1.6Mb  |
| *lite::tnn::cv::face::detect::RetinaFace*  | Pytorch_RetinaFace_mobile0.25-320-320.opt.tnnproto&tnnmodel  |             [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface)              | 1.6Mb  |
| *lite::tnn::cv::face::detect::RetinaFace*  | Pytorch_RetinaFace_mobile0.25-720-1080.opt.tnnproto&tnnmodel |             [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface)              | 1.6Mb  |
|  *lite::tnn::cv::face::detect::FaceBoxes*  |               FaceBoxes.opt.tnnproto&tnnmodel                |                [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)                | 3.8Mb  |
|  *lite::tnn::cv::face::detect::FaceBoxes*  |           FaceBoxes-640-640.opt.tnnproto&tnnmodel            |                [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)                | 3.8Mb  |
|  *lite::tnn::cv::face::detect::FaceBoxes*  |           FaceBoxes-320-320.opt.tnnproto&tnnmodel            |                [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)                | 3.8Mb  |
|  *lite::tnn::cv::face::detect::FaceBoxes*  |           FaceBoxes-720-1080.opt.tnnproto&tnnmodel           |                [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)                | 3.8Mb  |
|    *lite::tnn::cv::face::detect::SCRFD*    |        scrfd_500m_shape160x160.opt.tnnproto&tnnmodel         |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.5Mb  |
|    *lite::tnn::cv::face::detect::SCRFD*    |        scrfd_500m_shape320x320.opt.tnnproto&tnnmodel         |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.5Mb  |
|    *lite::tnn::cv::face::detect::SCRFD*    |        scrfd_500m_shape640x640.opt.tnnproto&tnnmodel         |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.5Mb  |
|    *lite::tnn::cv::face::detect::SCRFD*    |     scrfd_500m_bnkps_shape160x160.opt.tnnproto&tnnmodel      |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.5Mb  |  
|    *lite::tnn::cv::face::detect::SCRFD*    |     scrfd_500m_bnkps_shape320x320.opt.tnnproto&tnnmodel      |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.5Mb  |  
|    *lite::tnn::cv::face::detect::SCRFD*    |     scrfd_500m_bnkps_shape640x640.opt.tnnproto&tnnmodel      |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.5Mb  |  
|    *lite::tnn::cv::face::detect::SCRFD*    |         scrfd_1g_shape160x160.opt.tnnproto&tnnmodel          |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.7Mb  |
|    *lite::tnn::cv::face::detect::SCRFD*    |         scrfd_1g_shape320x320.opt.tnnproto&tnnmodel          |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.7Mb  |
|    *lite::tnn::cv::face::detect::SCRFD*    |         scrfd_1g_shape640x640.opt.tnnproto&tnnmodel          |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.7Mb  |
|    *lite::tnn::cv::face::detect::SCRFD*    |        scrfd_2.5g_shape160x160.opt.tnnproto&tnnmodel         |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 3.3Mb  |
|    *lite::tnn::cv::face::detect::SCRFD*    |        scrfd_2.5g_shape320x320.opt.tnnproto&tnnmodel         |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 3.3Mb  |
|    *lite::tnn::cv::face::detect::SCRFD*    |        scrfd_2.5g_shape640x640.opt.tnnproto&tnnmodel         |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 3.3Mb  |
|    *lite::tnn::cv::face::detect::SCRFD*    |     scrfd_2.5g_bnkps_shape160x160.opt.tnnproto&tnnmodel      |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 3.3Mb  |  
|    *lite::tnn::cv::face::detect::SCRFD*    |     scrfd_2.5g_bnkps_shape320x320.opt.tnnproto&tnnmodel      |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 3.3Mb  |  
|    *lite::tnn::cv::face::detect::SCRFD*    |     scrfd_2.5g_bnkps_shape640x640.opt.tnnproto&tnnmodel      |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 3.3Mb  |  
|    *lite::tnn::cv::face::detect::SCRFD*    |         scrfd_10g_shape640x640.opt.tnnproto&tnnmodel         |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 16.9Mb |
|    *lite::tnn::cv::face::detect::SCRFD*    |        scrfd_10g_shape1280x1280.opt.tnnproto&tnnmodel        |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 16.9Mb |
|    *lite::tnn::cv::face::detect::SCRFD*    |      scrfd_10g_bnkps_shape640x640.opt.tnnproto&tnnmodel      |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 16.9Mb |  
|    *lite::tnn::cv::face::detect::SCRFD*    |     scrfd_10g_bnkps_shape1280x1280.opt.tnnproto&tnnmodel     |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 16.9Mb |  
|  *lite::tnn::cv::face::detect::YOLO5Face*  |      yolov5face-blazeface-640x640.opt.tnnproto&tnnmodel      |                 [YOLO5Face](https://github.com/deepcam-cn/yolov5-face)                  | 3.4Mb  |
|  *lite::tnn::cv::face::detect::YOLO5Face*  |          yolov5face-l-640x640.opt.tnnproto&tnnmodel          |                 [YOLO5Face](https://github.com/deepcam-cn/yolov5-face)                  | 181Mb  |
|  *lite::tnn::cv::face::detect::YOLO5Face*  |          yolov5face-m-640x640.opt.tnnproto&tnnmodel          |                 [YOLO5Face](https://github.com/deepcam-cn/yolov5-face)                  |  83Mb  |
|  *lite::tnn::cv::face::detect::YOLO5Face*  |        yolov5face-n-0.5-320x320.opt.tnnproto&tnnmodel        |                 [YOLO5Face](https://github.com/deepcam-cn/yolov5-face)                  | 2.5Mb  |
|  *lite::tnn::cv::face::detect::YOLO5Face*  |        yolov5face-n-0.5-640x640.opt.tnnproto&tnnmodel        |                 [YOLO5Face](https://github.com/deepcam-cn/yolov5-face)                  | 4.6Mb  |
|  *lite::tnn::cv::face::detect::YOLO5Face*  |          yolov5face-n-640x640.opt.tnnproto&tnnmodel          |                 [YOLO5Face](https://github.com/deepcam-cn/yolov5-face)                  | 9.5Mb  |
|  *lite::tnn::cv::face::detect::YOLO5Face*  |          yolov5face-s-640x640.opt.tnnproto&tnnmodel          |                 [YOLO5Face](https://github.com/deepcam-cn/yolov5-face)                  |  30Mb  |
| *lite::tnn::cv::face::detect::FaceBoxesV2* |          faceboxesv2-640x640.opt.tnnproto&tnnmodel           |                [FaceBoxesV2](https://github.com/jhb86253817/FaceBoxesV2)                | 4.0Mb  |


## Face Alignment.

<div id="lite.ai.toolkit.hub.tnn-face-alignment"></div>  


|                      Class                      |                              Pretrained TNN Files                              |                  Rename or Converted From (Repo)                   |  Size   |
|:-----------------------------------------------:|:------------------------------------------------------------------------------:|:------------------------------------------------------------------:|:-------:|
|       *lite::tnn::cv::face::align::PFLD*        |                      pfld-106-lite.opt.tnnproto&tnnmodel                       | [pfld_106_...](https://github.com/Hsintao/pfld_106_face_landmarks) |  1.0Mb  |
|       *lite::tnn::cv::face::align::PFLD*        |                       pfld-106-v3.opt.tnnproto&tnnmodel                        | [pfld_106_...](https://github.com/Hsintao/pfld_106_face_landmarks) |  5.5Mb  |
|       *lite::tnn::cv::face::align::PFLD*        |                       pfld-106-v2.opt.tnnproto&tnnmodel                        | [pfld_106_...](https://github.com/Hsintao/pfld_106_face_landmarks) |  5.0Mb  |
|      *lite::tnn::cv::face::align::PFLD98*       |                    PFLD-pytorch-pfld.opt.tnnproto&tnnmodel                     |       [PFLD...](https://github.com/polarisZhao/PFLD-pytorch)       |  4.8Mb  |
|   *lite::tnn::cv::face::align::MobileNetV268*   |       pytorch_face_landmarks_landmark_detection_56.opt.tnnproto&tnnmodel       |  [...landmark](https://github.com/cunjian/pytorch_face_landmark)   |  9.4Mb  |
|  *lite::tnn::cv::face::align::MobileNetV2SE68*  | pytorch_face_landmarks_landmark_detection_56_se_external.opt.tnnproto&tnnmodel |  [...landmark](https://github.com/cunjian/pytorch_face_landmark)   |  11Mb   |
|      *lite::tnn::cv::face::align::PFLD68*       |               pytorch_face_landmarks_pfld.opt.tnnproto&tnnmodel                |  [...landmark](https://github.com/cunjian/pytorch_face_landmark)   |  2.8Mb  |
| *lite::tnn::cv::face::align::FaceLandmarks1000* |                     FaceLandmark1000.opt.tnnproto&tnnmodel                     |   [FaceLandm...](https://github.com/Single430/FaceLandmark1000)    |  2.0Mb  |
|     *lite::tnn::cv::face::align::PIPNet98*      |            pipnet_resnet18_10x98x32x256_wflw.opt.tnnproto&tnnmodel             |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 44.0Mb  |
|     *lite::tnn::cv::face::align::PIPNet68*      |            pipnet_resnet18_10x68x32x256_300w.opt.tnnproto&tnnmodel             |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 44.0Mb  |
|     *lite::tnn::cv::face::align::PIPNet29*      |            pipnet_resnet18_10x29x32x256_cofw.opt.tnnproto&tnnmodel             |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 44.0Mb  |
|     *lite::tnn::cv::face::align::PIPNet19*      |            pipnet_resnet18_10x19x32x256_aflw.opt.tnnproto&tnnmodel             |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 44.0Mb  |
|     *lite::tnn::cv::face::align::PIPNet98*      |            pipnet_resnet101_10x98x32x256_wflw.opt.tnnproto&tnnmodel            |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 150.0Mb |
|     *lite::tnn::cv::face::align::PIPNet68*      |            pipnet_resnet101_10x68x32x256_300w.opt.tnnproto&tnnmodel            |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 150.0Mb |
|     *lite::tnn::cv::face::align::PIPNet29*      |            pipnet_resnet101_10x29x32x256_cofw.opt.tnnproto&tnnmodel            |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 150.0Mb |
|     *lite::tnn::cv::face::align::PIPNet19*      |            pipnet_resnet101_10x19x32x256_aflw.opt.tnnproto&tnnmodel            |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 150.0Mb |


## Head Pose Estimation.

<div id="lite.ai.toolkit.hub.tnn-head-pose-estimation"></div>  


|                Class                |       Pretrained TNN Files       |                  Rename or Converted From (Repo)                   | Size  |
|:-----------------------------------:|:--------------------------------:|:------------------------------------------------------------------:|:-----:|
| *lite::tnn::cv::face::pose::FSANet* | fsanet-var.opt.tnnproto&tnnmodel | [...fsanet...](https://github.com/omasaht/headpose-fsanet-pytorch) | 1.2Mb |
| *lite::tnn::cv::face::pose::FSANet* | fsanet-1x1.opt.tnnproto&tnnmodel | [...fsanet...](https://github.com/omasaht/headpose-fsanet-pytorch) | 1.2Mb |

## Face Attributes.

<div id="lite.ai.toolkit.hub.tnn-face-attributes"></div>  


|                     Class                      |                             Pretrained TNN Files                              |                      Rename or Converted From (Repo)                      | Size  |
|:----------------------------------------------:|:-----------------------------------------------------------------------------:|:-------------------------------------------------------------------------:|:-----:|
|   *lite::tnn::cv::face::attr::AgeGoogleNet*    |                      age_googlenet.opt.tnnproto&tnnmodel                      |               [onnx-models](https://github.com/onnx/models)               | 23Mb  |
|  *lite::tnn::cv::face::attr::GenderGoogleNet*  |                    gender_googlenet.opt.tnnproto&tnnmodel                     |               [onnx-models](https://github.com/onnx/models)               | 23Mb  |
|  *lite::tnn::cv::face::attr::EmotionFerPlus*   |                    emotion-ferplus-7.opt.tnnproto&tnnmodel                    |               [onnx-models](https://github.com/onnx/models)               | 33Mb  |
|  *lite::tnn::cv::face::attr::EmotionFerPlus*   |                    emotion-ferplus-8.opt.tnnproto&tnnmodel                    |               [onnx-models](https://github.com/onnx/models)               | 33Mb  |
|      *lite::tnn::cv::face::attr::SSRNet*       |                         ssrnet.opt.tnnproto&tnnmodel                          |         [SSR_Net...](https://github.com/oukohou/SSR_Net_Pytorch)          | 190Kb |
| *lite::tnn::cv::face::attr::EfficientEmotion7* |           face-emotion-recognition-enet_b0_7.opt.tnnproto&tnnmodel            | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 15Mb  |
| *lite::tnn::cv::face::attr::EfficientEmotion8* |      face-emotion-recognition-enet_b0_8_best_afew.opt.tnnproto&tnnmodel       | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 15Mb  |
| *lite::tnn::cv::face::attr::EfficientEmotion8* |      face-emotion-recognition-enet_b0_8_best_vgaf.opt.tnnproto&tnnmodel       | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 15Mb  |
|  *lite::tnn::cv::face::attr::MobileEmotion7*   |          face-emotion-recognition-mobilenet_7.opt.tnnproto&tnnmodel           | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 13Mb  |
|  *lite::tnn::cv::face::attr::ReXNetEmotion7*   | face-emotion-recognition-affectnet_7_vggface2_rexnet150.opt.tnnproto&tnnmodel | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 30Mb  |

## Classification.

<div id="lite.ai.toolkit.hub.tnn-classification"></div>


|                       Class                       |               Pretrained TNN Files                |               Rename or Converted From (Repo)                | Size  |
|:-------------------------------------------------:|:-------------------------------------------------:|:------------------------------------------------------------:|:-----:|
| *lite::tnn::cv::classification:EfficientNetLite4* |    efficientnet-lite4-11.opt.tnnproto&tnnmodel    |        [onnx-models](https://github.com/onnx/models)         | 49Mb  |
|   *lite::tnn::cv::classification::ShuffleNetV2*   |      shufflenet-v2-10.opt.tnnproto&tnnmodel       |        [onnx-models](https://github.com/onnx/models)         | 8.7Mb |
|   *lite::tnn::cv::classification::DenseNet121*    |         densenet121.opt.tnnproto&tnnmodel         |       [torchvision](https://github.com/pytorch/vision)       | 30Mb  |
|     *lite::tnn::cv::classification::GhostNet*     |          ghostnet.opt.tnnproto&tnnmodel           |       [torchvision](https://github.com/pytorch/vision)       | 20Mb  |
|     *lite::tnn::cv::classification::HdrDNet*      |           hardnet.opt.tnnproto&tnnmodel           |       [torchvision](https://github.com/pytorch/vision)       | 13Mb  |
|      *lite::tnn::cv::classification::IBNNet*      |          ibnnet18.opt.tnnproto&tnnmodel           |       [torchvision](https://github.com/pytorch/vision)       | 97Mb  |
|   *lite::tnn::cv::classification::MobileNetV2*    |         mobilenetv2.opt.tnnproto&tnnmodel         |       [torchvision](https://github.com/pytorch/vision)       | 13Mb  |
|      *lite::tnn::cv::classification::ResNet*      |          resnet18.opt.tnnproto&tnnmodel           |       [torchvision](https://github.com/pytorch/vision)       | 44Mb  |
|     *lite::tnn::cv::classification::ResNeXt*      |           resnext.opt.tnnproto&tnnmodel           |       [torchvision](https://github.com/pytorch/vision)       | 95Mb  |
|     *lite::tnn::cv::classification::InsectID*     | quarrying_insect_identifier.opt.tnnproto&tnnmodel | [InsectID](https://github.com/quarrying/quarrying-insect-id) | 27Mb  |
|      *lite::tnn::cv::classification:PlantID*      |   quarrying_planted_model.opt.tnnproto&tnnmodel   |  [PlantID](https://github.com/quarrying/quarrying-plant-id)  | 30Mb  |


## Segmentation.

<div id="lite.ai.toolkit.hub.onnx-segmentation"></div>  


|                    Class                     |              Pretrained TNN Files              |         Rename or Converted From (Repo)          | Size  |
|:--------------------------------------------:|:----------------------------------------------:|:------------------------------------------------:|:-----:|
| *lite::cv::segmentation::DeepLabV3ResNet101* | deeplabv3_resnet101_coco.opt.tnnproto&tnnmodel | [torchvision](https://github.com/pytorch/vision) | 232Mb |
|    *lite::cv::segmentation::FCNResNet101*    |      fcn_resnet101.opt.tnnproto&tnnmodel       | [torchvision](https://github.com/pytorch/vision) | 207Mb |


## Style Transfer.

<div id="lite.ai.toolkit.hub.tnn-style-transfer"></div>  

|                   Class                   |            Pretrained TNN Files             |        Rename or Converted From (Repo)        | Size  |
|:-----------------------------------------:|:-------------------------------------------:|:---------------------------------------------:|:-----:|
| *lite::tnn::cv::style::FastStyleTransfer* |    style-mosaic-8.opt.tnnproto&tnnmodel     | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::tnn::cv::style::FastStyleTransfer* |     style-candy-9.opt.tnnproto&tnnmodel     | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::tnn::cv::style::FastStyleTransfer* |     style-udnie-8.opt.tnnproto&tnnmodel     | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::tnn::cv::style::FastStyleTransfer* |     style-udnie-9.opt.tnnproto&tnnmodel     | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::tnn::cv::style::FastStyleTransfer* |  style-pointilism-8.opt.tnnproto&tnnmodel   | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::tnn::cv::style::FastStyleTransfer* |  style-pointilism-9.opt.tnnproto&tnnmodel   | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::tnn::cv::style::FastStyleTransfer* | style-rain-princess-9.opt.tnnproto&tnnmodel | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::tnn::cv::style::FastStyleTransfer* | style-rain-princess-8.opt.tnnproto&tnnmodel | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::tnn::cv::style::FastStyleTransfer* |     style-candy-8.opt.tnnproto&tnnmodel     | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::tnn::cv::style::FastStyleTransfer* |    style-mosaic-9.opt.tnnproto&tnnmodel     | [onnx-models](https://github.com/onnx/models) | 6.4Mb |


## Colorization.

<div id="lite.ai.toolkit.hub.tnn-colorization"></div>

|                  Class                   |            Pretrained TNN Files            |              Rename or Converted From (Repo)              | Size  |
|:----------------------------------------:|:------------------------------------------:|:---------------------------------------------------------:|:-----:|
| *lite::tnn::cv::colorization::Colorizer* |   eccv16-colorizer.opt.tnnproto&tnnmodel   | [colorization](https://github.com/richzhang/colorization) | 123Mb |
| *lite::tnn::cv::colorization::Colorizer* | siggraph17-colorizer.opt.tnnproto&tnnmodel | [colorization](https://github.com/richzhang/colorization) | 129Mb |


## Super Resolution.

<div id="lite.ai.toolkit.hub.tnn-super-resolution"></div>

|                  Class                   |        Pretrained TNN Files        |              Rename or Converted From (Repo)              | Size  |
|:----------------------------------------:|:----------------------------------:|:---------------------------------------------------------:|:-----:|
| *lite::tnn::cv::resolution::SubPixelCNN* | subpixel-cnn.opt.tnnproto&tnnmodel | [...PIXEL...](https://github.com/niazwazir/SUB_PIXEL_CNN) | 234Kb |


#